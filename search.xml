<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2018/09/10/0_hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Battlegrounds</title>
    <url>/2023/07/09/100_Battlegrounds/</url>
    <content><![CDATA[<p>Set up a flag: the completion progress reaches 100%</p>
<p>Now at <strong>99%</strong></p>
<span id="more"></span>
<table>
<tr>
<td>
<b>Pretzel Mustard</b>
</td>
<td>
Without losing a round<br>(Completed 2024/06/03)
</td>
<td>
<b>Spicy Pretzel Mustard</b>
</td>
<td>
Winning every round<br>(Completed 2024/06/03)
</td>
</tr>
<tr>
<td>
<b>Finish in 1st place</b>
</td>
<td>
250/250<br>(Completed 2024/06/01)
</td>
<td>
Finish in top 4
</td>
<td>
1000/1000<br>(Completed 2024/10/04)
</td>
</tr>
<tr>
<td>
<b>Funnel Cakes</b>
</td>
<td>
Without losing a round in Duos<br>(Completed 2024/06/03)
</td>
<td>
<b>Cinnamon Funnel Cakes</b>
</td>
<td>
Winning every round in Duos<br>(Completed 2024/06/03)
</td>
</tr>
<tr>
<td>
Finish in 1st place in Duos
</td>
<td>
180/250
</td>
<td>
Finish in top 2 in Duos
</td>
<td>
261/1000
</td>
</tr>
<tr>
<td>
<b>Beasts</b>
</td>
<td>
Completed 2024/01/31
</td>
<td>
<b>Golden Beasts</b>
</td>
<td>
Completed 2023/11/22
</td>
</tr>
<tr>
<td>
<b>Demons</b>
</td>
<td>
Completed 2024/05/30
</td>
<td>
<b>Golden Demons</b>
</td>
<td>
Completed 2024/05/16
</td>
</tr>
<tr>
<td>
<b>Dragons</b>
</td>
<td>
Completed 2024/06/15
</td>
<td>
<b>Golden Dragons</b>
</td>
<td>
Completed 2024/03/02
</td>
</tr>
<tr>
<td>
<b>Elementals</b>
</td>
<td>
Completed 2024/06/01
</td>
<td>
<b>Golden Elementals<b>
</td>
<td>
Completed 2024/05/16
</td>
</tr>
<tr>
<td>
<b>Mechs</b>
</td>
<td>
Completed 2024/03/17
</td>
<td>
<b>Golden Mechs</b>
</td>
<td>
Completed 2024/01/20
</td>
</tr>
<tr>
<td>
<b>Murlocs</b>
</td>
<td>
Completed 2024/06/06
</td>
<td>
<b>Golden Murlocs</b>
</td>
<td>
Completed 2024/05/21
</td>
</tr>
<tr>
<td>
<b>Naga</b>
</td>
<td>
Completed 2024/08/18
</td>
<td>
<b>Golden Naga</b>
</td>
<td>
Completed 2024/05/25
</td>
</tr>
<tr>
<td>
<b>Priates</b>
</td>
<td>
Completed 2024/3/11
</td>
<td>
<b>Golden Priates</b>
</td>
<td>
Completed 2023/12/24
</td>
</tr>
<tr>
<td>
Quilboar
</td>
<td>
3970/4000
</td>
<td>
<b>Golden Quilboar</b>
</td>
<td>
Completed 2024/07/14
</td>
</tr>
<tr>
<td>
<b>Undead<b>
</td>
<td>
Completed 2023/12/10
</td>
<td>
<b>Golden Undead</b>
</td>
<td>
Completed 2023/10/23
</td>
</tr>
</table>
<table>
<tr>
<td align="center" style="width: 100px;">
Hero
</td>
<td style="width: 100px;">
</td>
<td style="width: 200px;">
Achievement
</td>
<td>
Description
</td>
</tr>
<tr>
<td rowspan="2">
<b>A.F.Kay</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/8f9f341289ba4279f8eb37f97f56abb4306ec379b16837830a2ac1b688558820.png">
</td>
<td>
Idle Idol<br>(Completed 2023/10/30)
</td>
<td>
Passive Skip your first two turns. Start with two minions from Tavern
Tier 3.<br>Finish in 1st place as A.F.Kay
</td>
</tr>
<tr>
<td>
brb<br>(Completed 2023/10/30)
</td>
<td>
Finish in 1st place after AFKing your third turn as A.F.Kay
</td>
</tr>
<tr>
<td>
<b>AI’Akir</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/208f628c6ae4340ca6889ff5ba883a919c021429b75f73c4903a4c647e4e6bcd.png">
</td>
<td>
Wrath of the Windlord<br>(Completed 2023/11/23)
</td>
<td>
Passive Start of Combat: Give your left-most minion Windfury, Divine
Shield, and Taunt.<br>Finish in 1st place as AI’Akir
</td>
</tr>
<tr>
<td>
<b>Alextrasza</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d7a061e5ab1c9c132725aaabe271d9fd60e0ce39d7831d7706e80171bce8e5f3.png">
</td>
<td>
Life and Hope<br>(Completed 2023/11/11)
</td>
<td>
Passive. After you upgrade Bob's Tavern to Tavern Tier 5, Discover two
Dragons of any Tier.<br>Finish in 1st place as Alextrasza
</td>
</tr>
<tr>
<td>
<b>Ambassador Faelin</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/812794e11850309243ae674e2523d8492a85d922a2dfd796ae317826ac090c32.png">
</td>
<td>
All According To Plan<br>(Completed 2023/05/30)
</td>
<td>
Passive. Skip your first turn. Discover a Tier 2, 4, and 6 minion to get
at those Tiers.<br>Finish in 1st place as Ambassador Farlin
</td>
</tr>
<tr>
<td>
<b>Aranna Starseeker</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/432b5f6dee120335d8c37258e03083cf559659af8196d32dc54a3581c209774e.png">
</td>
<td>
Outlandish<br>(Completed 2023/06/04)
</td>
<td>
Passive. After 16 friendly minions attack, the first minion you buy each
turn is free. ( left!)<br>Finish in 1st place as Aranna Starseeker
</td>
</tr>
<tr>
<td>
<b>Arch-Villain Rafaam</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d7f38259e52d3f372d1a5bdf212478e0b8b837aa62015ea41d92598102671dc3.png">
</td>
<td>
Supreme Tavernologist<br>(Completed 2023/07/20)
</td>
<td>
Next combat, add a plain copy of the first minion you kill to your
hand.<br>Finish in 1st place as Arch-Villain Rafaam
</td>
</tr>
<tr>
<td>
<b>Brann</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/58a3df4cc1ff17d37afad9957bd1b0c862e218d537425a0e56aa2c780dc4850d.png">
</td>
<td>
Where's the Action?!<br>(Completed 2023/10/05)
</td>
<td>
Passive. After you buy 5 Battlecry minions, add Brann Bronzebeard to
your hand. (Once per game.)<br>Finish in 1st place as Brann
</td>
</tr>
<tr>
<td>
<b>Bru’kan</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/831548757f8f792b550828859e1d286ff2ae56896006008c6d131a7559cb5a04.png">
</td>
<td>
Elements of Style<br>(Completed 2023/07/30)
</td>
<td>
Choose an Element. Start of Combat: Call upon that Element.<br>Finish in
1st place as Bru’kan
</td>
</tr>
<tr>
<td>
<b>Cap’n Hoggarr</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/47acd041253ef3a0bec521b75a8021d5cae454e05cabfa29d6ea15ada1695311.png">
</td>
<td>
Sailing to Victory<br>(Completed 2023/09/03)
</td>
<td>
Passive After you buy a Pirate, gain 1 Gold.<br>Finish in 1st place as
Cap’n Hoggarr
</td>
</tr>
<tr>
<td>
<b>Captain Eudora</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5cb89601f8020957f30607a4a161ea1a8d05e2324b7062886b72774944a5dbb3.png">
</td>
<td>
Can You Dig It?<br>(Completed 2023/10/01)
</td>
<td>
Dig for a Golden minion! ( (Dig, Digs) left.)<br>Finish in 1st place as
Captain Eudora
</td>
</tr>
<tr>
<td>
<b>Captain Hooktusk</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/171fd3a249bbd535c99d99eed2631c865cdb158d84c28f8beb7315992e7695fb.png">
</td>
<td>
Never Fight Fair<br>(Completed 2023/11/10)
</td>
<td>
Remove a friendly minion. Choose one of two from a Tavern Tier lower to
keep.<br>Finish in 1st place as Captain Hooktusk
</td>
</tr>
<tr>
<td>
<b>Cariel Roame</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d78fba8c2d05f214f5613e2d2406fc6b5860eaf2e73dc033ec479efdeb103190.png">
</td>
<td>
Courageous Conviction<br>(Completed 2024/06/01)
</td>
<td>
Give friendly minions + /+ . (After you upgrade the Tavern, choose an
improvement!) Give friendly minions + /+ .<br>Finish in 1st place as
Cariel Roame
</td>
</tr>
<tr>
<td>
<b>Chenvaala</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2a88458e43b359eef32c9cfc194bd48801ab65272b95a8e125b394a61a542e6d.png">
</td>
<td>
Is It Cold In Here?<br>(Completed 2023/09/09)
</td>
<td>
Passive After you play 3 Elementals, reduce the cost of upgrading Bob's
Tavern by (3).<br>Finish in 1st place as Chenvaala
</td>
</tr>
<tr>
<td>
<b>Cho</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ad4ea03a821830fdab6b763970f1dbcb10c2bd9b95a1e6c22d6ade14c969057d.png">
</td>
<td>
I won!<br>(Completed 2024/04/28)
</td>
<td>
Passive. Whenever you play a Golden minion, both you and Gall get a
Triple Reward. <br>Finish in 1st place as Cho
</td>
</tr>
<tr>
<td>
<b>Cookie the Cook</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/fbffd5745f86f2a5171831e01be2e8b7b421fb5ceb273b89d4d080bc74e8bc18.png">
</td>
<td>
3-Star Chef<br>(Completed 2023/08/20)
</td>
<td>
Throw a minion in your pot. When you've gathered 3, Discover from their
minion types. ( left!)<br>Finish in 1st place as Cookie the Cook
</td>
</tr>
<tr>
<td>
<b>C’Thun</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/56c5d1797655207f4606dbecaaf78ccf04c3db0b5e50146d8d41eae64723227d.png">
</td>
<td>
Mathematically Impossible<br>(Completed 2023/05/30)
</td>
<td>
At end of turn, give a friendly minion +1/+1. Repeat (time, times).
(Upgrades each turn!)<br>Finish in 1st place as C’Thun
</td>
</tr>
<tr>
<td rowspan="2">
<b>Dancing’ Deryl</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/11ad255ca614b077f1dfb7697b208c113d6009214ae7b11ccece5ddb97269c8c.png">
</td>
<td>
Show Me Your Moves<br>(Completed 2024/04/06)
</td>
<td>
Finish in 1st place as Dancing’ Deryl
</td>
</tr>
<tr>
<td>
Hats on Hats on Hats<br>(Completed 2023/10/16)
</td>
<td>
Place 30 hats on one minion as Dancing’ Deryl
</td>
</tr>
<tr>
<td>
<b>Death Speaker Blackthorn</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/38f3bf86dd5f0a7145a06078048d4d9aa887462aa6c486fc05e15ac0769f5046.png">
</td>
<td>
Killer Speech<br>(Completed 2024/05/18)
</td>
<td>
Get 2 Blood Gems. (Twice per turn.)<br>Finish in 1st place as Death
Speaker Blackthorn
</td>
</tr>
<tr>
<td>
<b>Deathwing</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5c475eb99fec4aeeb88fb1525061f969d472e3811e448050e59ca08d142a55fa.png">
</td>
<td>
POWER INCARNATE!!!<br>(Completed 2023/11/16)
</td>
<td>
Passive Start of Combat: Give ALL minions +2 Attack
permanently.<br>Finish in 1st place as Deathwing
</td>
</tr>
<tr>
<td>
<b>Doctor Holli'dae</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/65c6e0ef3c77e4e2e6b284300abb1d7c0c70805d3b797fd654c332e2ea4ea94a.png">
</td>
<td>
Happy Holli'daes!<br>(Completed 2023/12/09)
</td>
<td>
Get a random Tavern spell.<br>Finish in 1st place as Deathwing
</td>
</tr>
<tr>
<td>
<b>Drek’Thar</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6dbd7d61c752154fb08a9e6759f4e9f7bdf64b18e28cc61e7993319cc7c84e1c.png">
</td>
<td>
Frostwolf Fame<br>(Completed 2023/05/29)
</td>
<td>
Passive Avenge (2): Give your minions +1 Attack permanently.<br>Finish
in 1st place as Drek’Thar
</td>
</tr>
<tr>
<td rowspan="2">
<b>E.T.C Band Manager</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5b7bcf32835bca15197f609ec5a66e9690e61e620fa979db5363bc58b8e8c6cb.png">
</td>
<td>
Chart Topper<br>(Completed 2023/06/12)
</td>
<td>
Discover a Buddy. (Unlocks at Tier 2.)<br>Finish in 1st place as E.T.C
Band Manager
</td>
</tr>
<tr>
<td>
Friend of a Friend<br>(Completed 2023/06/29)
</td>
<td>
Play 16 Buddies
</td>
</tr>
<tr>
<td>
<b>Edwin VanCleef</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/e41d31d85a4a02afb1c8c2e1efae16f7d252187dd7ed2d32eabe0fd4be1fdb6b.png">
</td>
<td>
For the Brotherhood<br>(Completed 2023/06/16)
</td>
<td>
Give a minion + /+ . Upgrades after you buy 4 minions. (
left!)<br>Finish in 1st place as Edwin VanCleef
</td>
</tr>
<tr>
<td>
<b>Elise Starseeker</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/11eb07c5477fbdd522f626c7cf2df9c19e1cd0811c0a2692f1aef0c3a5d69fa9.png">
</td>
<td>
Always Bring a Map<br>(Completed 2023/11/21)
</td>
<td>
Discover a minion from your Tavern tier. Costs (1) more after each
use.<br>Finish in 1st place as Elise Starseeker
</td>
</tr>
<tr>
<td>
<b>Enhance-o Mechano</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6db8abb382f57b1e43719765426f009ca540f8f3416ec7ed6084b170ef5e3f92.png">
</td>
<td>
The Little Bot That Could<br>(Completed 2023/07/08)
</td>
<td>
Passive. After each Refresh, give a minion in Bob's Tavern Taunt,
Windfury, Reborn, or Divine Shield.<br>Finish in 1st place as Enhance-o
Mechano
</td>
</tr>
<tr>
<td>
<b>Flobbidinous Floop</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/7a4862481039beee31f60cb1abcf344afc48fa974647737d50dae194a4eb6d44.png">
</td>
<td>
Floop De Loop<br>(Completed 2024/04/27)
</td>
<td>
Choose a friendly minion. Start of Combat: Transform it into your
teammate's highest Tier minion.<br>Finish in 1st place as Flobbidinous
Floop
</td>
</tr>
<tr>
<td>
<b>Forest Warden Omu</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/03d19a23896ea91148a8793e7d90ea876750985e365183b5c69a6ff7b40c711c.png">
</td>
<td>
Nowhere Left To Grow<br>(Completed 2023/11/05)
</td>
<td>
Passive After you upgrade Bob's Tavern, gain 2 Gold.<br>Finish in 1st
place as Forest Warden Omu
</td>
</tr>
<tr>
<td>
<b>Fungalmancer Flurgl</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/37e634262bb5411afe01d26ed3b624e2b7bf49bac38d3a5bbc23dfc1e58cf9e3.png">
</td>
<td>
Wishies for Fishies<br>(Completed 2023/06/10)
</td>
<td>
Passive. After you sell 5 minions, get a random Murloc. (
left.)<br>Finish in 1st place as Fungalmancer Flurgl
</td>
</tr>
<tr>
<td rowspan="2">
<b>Galakrond</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bbe94b8cdad0b6130578d29278fc48ba4fdcaf1229e8337557cfb7d1743e0df9.png">
</td>
<td>
The Progenitor<br>(Completed 2023/08/21)
</td>
<td>
Choose a minion in Bob's Tavern. Discover a higher Tier minion to
replace it.<br>Finish in 1st place as Galakrond
</td>
</tr>
<tr>
<td>
Long Term Investment<br>(Completed 2023/05/30)
</td>
<td>
Upgrade a Tier 1 minion to a Tier 6 minion as Galakrond.
</td>
</tr>
<tr>
<td>
<b>Galewing</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/a19edf90bd53e346d1593f27b17d8b994ad97c00b8ed5c5deef46666719e6115.png">
</td>
<td>
Take to the Skies<br>(Completed 2023/05/31)
</td>
<td>
Choose a new flightpath. Complete it to get a bonus!<br>Finish in 1st
place as Galewing
</td>
</tr>
<tr>
<td>
<b>George the Fallen</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/fea24fd3049da7c75a46526a2784303099c6341c3bfe4a4673a2547728e1846f.png">
</td>
<td>
For Karl!<br>(Completed 2023/11/19)
</td>
<td>
Give a minion Divine Shield.<br>Finish in 1st place as George the Fallen
</td>
</tr>
<tr>
<td>
<b>Greybough</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3bb12d29a29fd44c9c6eeab5635ec9283a1843e00980c2787ccd54c7424db6e7.png">
</td>
<td>
Sprouting Your Roots<br>(Completed 2023/07/13)
</td>
<td>
Passive Give +1/+2 and Taunt to minions you summon during
combat.<br>Finish in 1st place as Greybough
</td>
</tr>
<tr>
<td>
<b>Guff Runetotem</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/aceadc9bb5f5cf6f282b60d9dd726053f5334b226044c65c04606ce275928df0.png">
</td>
<td>
Perfectly Balanced<br>(Completed 2023/08/27)
</td>
<td>
Give a friendly minion of each Tavern Tier +2/+2.<br>Finish in 1st place
as Guff Runetotem
</td>
</tr>
<tr>
<td>
<b>Heistaron Togaggle</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/33e7ebcd9c43edc2489e3c26b350852dd295085aa9d534729dd49cc8a5887df6.png">
</td>
<td>
Who The King? I THE KING!<br>(Completed 2023/11/05)
</td>
<td>
Steal all minions in Bob's Tavern. Each turn, your next Hero Power costs
(1) less.<br>Finish in 1st place as Heistaron Togaggle
</td>
</tr>
<tr>
<td>
<b>Illidan Stormrage</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/1b0741094f9630a6997e13f7fc16efd05d99daae42de6dc598b082bff4b47e1d.png">
</td>
<td>
You ARE Prepared<br>(Completed 2023/12/05)
</td>
<td>
Passive. Start of Combat: Your left and right-most minions gain +2/+1
and attack immediately.<br>Finish in 1st place as Illidan Stormrage
</td>
</tr>
<tr>
<td>
<b>Infinite Toki</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/df407a1a99c5cdb82bfcc51a9a8df3a04d62e34b5eb1ed39873cf1a8e3f9919a.png">
</td>
<td>
The Ideal Timeline<br>(Completed 2023/09/06)
</td>
<td>
Refresh Bob's Tavern. Include two minions from a Tavern Tier
higher.<br>Finish in 1st place as Infinite Toki
</td>
</tr>
<tr>
<td>
<b>Inge, the Iron Hymn</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ad3fe7b6b636db6fdbfcd969753535cb1abfe376bbc40078497386a43b5ee2ae.png">
</td>
<td>
Ingenious! <br>(Completed 2023/06/14)
</td>
<td>
Give a minion Attack equal to your Tavern Tier. (Swaps to Health next
turn!)<br>Finish in 1st place as Inge. The Iron Hymn
</td>
</tr>
<tr>
<td>
<b>Ini Stormcoil</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/28d3ec677705631e612383acee49a36bdfe7ee0060d82bfd13ee3d5261d0b571.png">
</td>
<td>
The Stormcoil Override<br>(Completed 2023/05/29)
</td>
<td>
Passive After 10 friendly minions die, get a random Mech. Passive After
10 friendly minions die, get a random Mech. ( left.)<br>Finish in 1st
place as Ini Stormcoil
</td>
</tr>
<tr>
<td>
<b>Jandice Barov</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/86e594de784417d255d4f67da713cf1f748f581984a764d35200d5efef647c24.png">
</td>
<td>
Swap Meet<br>(Completed 2023/11/01)
</td>
<td>
Swap a friendly non-golden minion with a random one in Bob's
Tavern.<br>Finish in 1st place as Jandice Barov
</td>
</tr>
<tr>
<td>
<b>Kael’thas Sunstride</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f721e0cd097010c2db5640f8f995d2e7e58efa1e444b7ca71e61d2119a161b06.png">
</td>
<td>
No More Setbacks<br>(Completed 2023/09/24)
</td>
<td>
Passive Every third minion you play gains +2/+2.<br>Finish in 1st place
as Kael’thas Sunstride
</td>
</tr>
<tr>
<td>
<b>Kurtrus Ashfallen</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bb57bb887b9f6d85bbb399464e2dfe886c4d1a5fb670039e8e8c0b0628d7959b.png">
</td>
<td>
Ashfallen in Love With You<br>(Completed 2023/07/19)
</td>
<td>
Passive. Once per turn, after you buy 4 minions, Discover a plain copy
of one of them. ( left!) Passive. Once per turn, after you buy 4
minions, Discover a plain copy of one of them. (Done!)<br>Finish in 1st
place as Kurtrus Ashfallen
</td>
</tr>
<tr>
<td>
<b>Lady Vashj</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3860d62f42e43e75687a666f6da549e0f3ff805316ee98e306d3715d432d7d63.png">
</td>
<td>
Handmaiden's Tail<br>(Completed 2023/05/27)
</td>
<td>
Discover a Spellcraft spell of your Tier or lower. Its effect is
permanent.<br>Finish in 1st place as Lady Vashj
</td>
</tr>
<tr>
<td>
<b>Lich Baz’hial</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5984e14a91e46f5578fab2a184821029479837408c790d36cce226a369ade097.png">
</td>
<td>
Phylactery Mastery<br>(Completed 2023/08/22)
</td>
<td>
Steal a minion in Bob's Tavern. Take damage equal to its Tavern
Tier.<br>Finish in 1st place as Lich Baz’hial
</td>
</tr>
<tr>
<td rowspan="2">
<b>Lord Barov</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/b148b639c90aad9b3c2028ed482799e83aab3f69961aae3ca4ff8a0693fa4666.png">
</td>
<td>
Lord of the Manor(Completed 2024/08/20)
</td>
<td>
Guess which player will win their next combat. If they win, get 3
Coins.<br>Finish in 1st place as Lord Barov
</td>
</tr>
<tr>
<td>
Salty Bets<br>(Completed 2024/01/23)
</td>
<td>
Guess the correct outcome of Friendly Wagers 3 turns in a row.
</td>
</tr>
<tr>
<td>
<b>Lord Jaraxxus</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3956c3fd0b3df2c6cfecb46b0f60772c85f1291d535aeffd3a0b0bbfb076ab2d.png">
</td>
<td>
INFERNAL OVERDRIVE<br>(Completed 2024/07/12)
</td>
<td>
Choose a friendly Demon. It consumes a minion in Bob's Tavern to gain
its stats.<br>Finish in 1st place as Lord Jaraxxus
</td>
</tr>
<tr>
<td>
<b>Madam Goya</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d7e2dfe73e09dd4b3c6e0f6d8e74b6bac586de215f9fbdeecc17d191f6410aaf.png">
</td>
<td>
Passed With Flying Colors<br>(Completed 2024/06/10)
</td>
<td>
Pass a non-Golden minion.<br>Finish in 1st place as Madam Goya
</td>
</tr>
<tr>
<td>
<b>Maiev</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6918af890e98422bcf2bb5dbd3a2a57301a5a8398db2acf5c1c1f02bd849b9fd.png">
</td>
<td>
Iron Hand of Justice<br>(Completed 2023/12/21)
</td>
<td>
Make a minion in Bob's Tavern Dormant. After 3 turns, get it with
+2/+2.<br>Finish in 1st place as Maiev
</td>
</tr>
<tr>
<td>
<b>Malygos</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/eda55e0eba4bd97eb441a7cc44d56c990a847e32efd1663f5bbeb2d03775d6ee.png">
</td>
<td>
The Essence of Magic<br>(Completed 2023/10/22)
</td>
<td>
Replace a minion with a random one of the same Tavern Tier. (Twice per
turn.)<br>Finish in 1st place as Malygos
</td>
</tr>
<tr>
<td>
<b>Marin the Manager</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/51faa5dd4bafb288b5520f6d833e433a14107b04971b161ac72b158f4c24644d.png">
</td>
<td>
Under New Management<br>(Completed 2024/08/22)
</td>
<td>
Passive On Turn 5, choose a Lesser Trinket to buy.<br>Finish in 1st
place as Marin the Manager
</td>
</tr>
<tr>
<td>
<b>Master Nguyen</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dbd2f8d971d508bfb163854bcc8808c58e67b09bb77cf1aa24d2be01097937f0.png">
</td>
<td>
A Master's Tribute<br>(Completed 2023/08/31)
</td>
<td>
Passive At the start of every turn, choose from 2 new Hero
Powers.<br>Finish in 1st place as Master Nguyen
</td>
</tr>
<tr>
<td>
<b>Millhouse Manastorm</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d43b3fc7f7ec955b34824f1fdefaedee44e550949f276cac64f594be7544113d.png">
</td>
<td>
Light You Up. Sweetcheeks<br>(Completed 2023/09/10)
</td>
<td>
Passive Minions cost 2 Gold. Refresh costs 2 Gold. Tavern Tiers cost (1)
more.<br>Finish in 1st place as Millhouse Manastorm
</td>
</tr>
<tr>
<td>
<b>Millificent Manastorm</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/aa6f2e1fd9dcf285be12131c4e109939cc70eadcfd07d870e387deb6af776b22.png">
</td>
<td>
Meching It happen<br>(Completed 2023/11/05)
</td>
<td>
Passive Whenever you summon a Mech, give it +2 Attack.<br>Finish in 1st
place as Millificent Manastorm
</td>
</tr>
<tr>
<td>
<b>Mr. Bigglesworth</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ff85b0df5bf26537d5d98fc8eb4c7a2e44ecedebe2e110d072b1a54fe566cb76.png">
</td>
<td>
Whose Cat Is This?<br>(Completed 2023/05/30)
</td>
<td>
Passive When a player dies, Discover a minion from their warband. It
keeps any enchantments.<br>Finish in 1st place as Mr. Bigglesworth
</td>
</tr>
<tr>
<td>
<b>Mukla</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/28dcc046ea739dc8a036438539938e469cf06a1fd3b230cd1472c69c2726bd8e.png">
</td>
<td>
Banana Slamma'<br>(Completed 2023/09/29)
</td>
<td>
Passive At the start of your turn, get 2 Bananas and give everyone else
one.<br>Finish in 1st place as Mukla
</td>
</tr>
<tr>
<td>
<b>Murloc Holmes</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ee6f4c3a335863eee684b0d98de8fc44530ade7acee18de6a22581612c8a6676.png">
</td>
<td>
Amazing detective/Genius<br>(Completed 2023/06/15)
</td>
<td>
Look at 2 minions. Guess which one your next opponent had last combat
for a Coin.<br>Finish in 1st place as Murloc Holmes
</td>
</tr>
<tr>
<td>
<b>Mutanas the Devourer</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2dd5a9136566b4896ca735317b5450594197bc609fc79906eb010556652be500.png">
</td>
<td>
Insatiable<br>(Completed 2023/07/27)
</td>
<td>
Remove a friendly minion. Spit its stats onto another. Get 1
Gold.<br>Finish in 1st place as Mutanas the Devourer
</td>
</tr>
<tr>
<td>
<b>Nozdormu</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bba4e1ff4d9cefce0fc760bcd5f9ff152284cdfccb64d01926e53afc3fac8c54.png">
</td>
<td>
Just a Matter of Time<br>(Completed 2023/09/17)
</td>
<td>
Passive Your first Refresh each turn costs (0).<br>Finish in 1st place
as Nozdormu
</td>
</tr>
<tr>
<td>
<b>N’Zoth</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6f0671e5ca8eaf154dc205e04e0e1fbdf00b4e01be0841d7d3e9209ad72bf168.png">
</td>
<td>
Gone Fishing<br>(Completed 2023/06/21)
</td>
<td>
Passive Start the game with a 2/2 Fish that gains all your Deathrattles
in combat.<br>Finish in 1st place as N’Zoth
</td>
</tr>
<tr>
<td>
<b>Onyxia</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/c1d1d6ec0720af1a9e244eb6914b5c0bf91137ac59f8f9d0257037d90b6909da.png">
</td>
<td>
More Dots!<br>(Completed 2023/06/23)
</td>
<td>
Passive Avenge (3): Summon a 3/1 Whelp. It attacks
immediately.<br>Finish in 1st place as Onyxia
</td>
</tr>
<tr>
<td>
<b>Overlord Saurfang</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f242e3b547fb5ce53ea1eb1db3167b92fa15c831e799b5de392bbe857bc08fee.png">
</td>
<td>
Might of the Horde<br>(Completed 2023/12/02)
</td>
<td>
Give a minion in the Tavern + /+ . (Doubles every turns!) Give a minion
in the Tavern + /+ . (Doubles in (turn, turns)!)<br>Finish in 1st place
as Overlord Saurfang
</td>
</tr>
<tr>
<td>
<b>Ozumat</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dd50f028a96656f724020b52592ef9f4de6a8144713af427aa605c21c8a685a2.png">
</td>
<td>
Throne of the Tides<br>(Completed 2023/08/27)
</td>
<td>
Passive. Start of Combat: Summon a / Tentacle with Taunt. (Gains +1/+1
after you sell a minion!)<br>Finish in 1st place as Ozumat
</td>
</tr>
<tr>
<td>
<b>Patches the Pirate</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/8ff3c758832f1ba6473046d775b1e9c2f4e159194cdbdee3c0151ea1b0685e29.png">
</td>
<td>
Who's In Charrrrge?<br>(Completed 2024/03/03)
</td>
<td>
Get a Pirate. After you buy a Pirate, your next Hero Power costs (1)
less.<br>Finish in 1st place as Patches the Pirate
</td>
</tr>
<tr>
<td>
<b>Patchwerk</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/35aca69a6618c085bf4bf8a3f6ff8b41154a30f377c256a50634053e72cf02a2.png">
</td>
<td>
Werkin' It<br>(Completed 2023/06/10)
</td>
<td>
Passive Start with 60 Health instead of 30.<br>Finish in 1st place as
Patchwerk
</td>
</tr>
<tr>
<td>
<b>Professor Putricide</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6b13a07802d9632a024835b0687c01dd51cedc309ff9c3fc4ad49b1dd10d7bbb.png">
</td>
<td>
Plague Perfected<br>(Completed 2023/08/07)
</td>
<td>
Craft a custom Undead. ( Creations left!)<br>Finish in 1st place as
Professor Putricide
</td>
</tr>
<tr>
<td>
<b>Pyramad</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/487af9778a9118e33f66671889fdbc755f1e95754cf8b33193d70d5bd071259e.png">
</td>
<td>
Brick House<br>(Completed 2023/09/15)
</td>
<td>
Give a minion + Health. (Gains +1 Health each turn you don't use
this!)<br>Finish in 1st place as Pyramad
</td>
</tr>
<tr>
<td>
<b>Queen Azshara</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/351a78bb20006f04421dc0da1f2bd4273dbc2b66e7365bd883efd751d87f8d2c.png">
</td>
<td>
The Water's Fine<br>(Completed 2023/07/29)
</td>
<td>
Passive. When your warband reaches 25 total Attack, begin your Naga
Conquest. Passive. When your warband reaches 25 total Attack, begin your
Naga Conquest. ( left!)<br>Finish in 1st place as Queen Azshara
</td>
</tr>
<tr>
<td>
<b>Queen Wagtoggle</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/0b0fbdc1cc12d3e681b6c536418f36b4d54418aed245d0bc6bff60a2c565c4f3.png">
</td>
<td>
Me Win Candle!<br>(Completed 2023/11/05)
</td>
<td>
Passive. Start of Combat: Give a friendly minion of each type stats
equal to their Tavern Tiers.<br>Finish in 1st place as Queen Wagtoggle
</td>
</tr>
<tr>
<td>
<b>Ragnaros</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d2eb043afaec5a9543a9012820668e927684cb2bbb908291dd7f692f2c64148c.png">
</td>
<td>
By fire be Purged!<br>(Completed 2023/06/12)
</td>
<td>
Passive After you kill 25 enemy minions, get Sulfuras. (
left!)<br>Finish in 1st place as Ragnaros
</td>
</tr>
<tr>
<td>
<b>Rakanishu</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2bdb3634ac6e7554e01f7750afebbc5401640b3ee9df44531d2522ac1da67d74.png">
</td>
<td>
Hot! Hot! Hot!<br>(Completed 2023/08/13)
</td>
<td>
Give a minion stats equal to your Tavern Tier.<br>Finish in 1st place as
Rakanishu
</td>
</tr>
<tr>
<td>
<b>Reno Jackson</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ef2c29f1dacc4df898394cc80fd7ded5d0c2f3c3bf13cb9c7fff556bba5485dc.png">
</td>
<td>
Aren't I Amazing?<br>(Completed 2023/09/08)
</td>
<td>
Make a friendly minion Golden. (Once per game.)<br>Finish in 1st place
as Reno Jackson
</td>
</tr>
<tr>
<td>
<b>Rock Master Voone</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/036db476361284d598f486f0a6f070c543f08b803955d1bc97efa1175acecb50.png">
</td>
<td>
Ready to Rock!<br>(Completed 2023/06/16)
</td>
<td>
Passive. At the end of every 3 turns, get a plain copy of the left-most
card in your hand. ( (turn, turns) left!) <br>Finish in 1st place as
Rock Master Voone
</td>
</tr>
<tr>
<td>
<b>Rokara</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f390ceaa1a0434817f9262aa0e573509a53798fe5bf721b062f01c85a1fa3a02.png">
</td>
<td>
Not Too Young To Fight<br>(Completed 2023/07/01)
</td>
<td>
Passive. After a friendly minion kills an enemy, give it +1 Attack
permanently.<br>Finish in 1st place as Rokara
</td>
</tr>
<tr>
<td>
<b>Scabbs Cutterbutter</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/42b6ce697b70a7174441ab6f8e954c74e65262e197f2657647b982f7f896a3cd.png">
</td>
<td>
You made this? I made this!<br>(Completed 2023/05/30)
</td>
<td>
Discover a plain copy of a minion from your next opponent's
warband.<br>Finish in 1st place as Scabbs Cutterbutter
</td>
</tr>
<tr>
<td>
<b>Shudderwock</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bfc5bab0926fcbbdf2f1d167e8022f7edc8047993064edf1ecadc8aa09e5aad4.png">
</td>
<td>
O Frabjous Day!<br>(Completed 2023/11/04)
</td>
<td>
Trigger a friendly minion's Battlecry.<br>Finish in 1st place as
Shudderwock
</td>
</tr>
<tr>
<td>
<b>Silas Darkmoon</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f6c8a988042d2838d33c981b3ad8c52068bfee24b5bad28d0fdbdb614202a9c6.png">
</td>
<td>
Greatest Show On Azeroth<br>(Completed 2023/06/25)
</td>
<td>
Passive. Darkmoon Tickets are in the Tavern! Get 3 to Discover a minion
from your Tavern Tier.<br>Finish in 1st place as Silas Darkmoon
</td>
</tr>
<tr>
<td>
<b>Sindragosa</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ce6fca97b7ee6fbfd31aeee774f15010ed4ad27dbae46833c2397acd184bd014.png">
</td>
<td>
Icy Wings of Northrend<br>(Completed 2023/11/05)
</td>
<td>
Freeze a minion in Bob's Tavern. Passive: At the end of your turn,
Frozen minions get +2/+1.<br>Finish in 1st place as Sindragosa
</td>
</tr>
<tr>
<td>
<b>Sir Finley Mrrgglton</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3017d5bc0d7918641ceaf1a0f98286170dd76f51640cdc0e46ea2d1d60ee81af.png">
</td>
<td>
How About a New Adventure?<br>(Completed 2023/08/31)
</td>
<td>
Passive At the start of the game, Discover a Hero Power.<br>Finish in
1st place as Sir Finley Mrrgglton
</td>
</tr>
<tr>
<td rowspan="3">
<b>Sire Denatureius</b>
</td>
<td rowspan="3">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/12c1f9737729cc2f32d414bbfd7ece06833cfa96ff030406bd1db1a78be9d7fc.png">
</td>
<td>
I AM Revendreth<br>(Completed 2023/05/28)
</td>
<td>
Passive. At the start of the game, choose one of two Quests.<br>Finish
in 1st place as Sire Denatureius
</td>
</tr>
<tr>
<td>
Case Closed<br>(Completed 2023/07/14)
</td>
<td>
Complete 10 different Quests
</td>
</tr>
<tr>
<td>
Handsomely Rewarded<br>(Completed 2023/07/14)
</td>
<td>
Earn 15 different Quest Rewards.
</td>
</tr>
<tr>
<td rowspan="2">
<b>Skycap’n Kragg</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/257b9cf7c78d018570ab0fcf4501719bc58019afd4f0d531f050a903e4f8ec61.png">
</td>
<td>
I'm the Cap'n Now<br>(Completed 2023/07/11)
</td>
<td>
Gain Gold. Increases each turn. (Once per game.)<br>Finish in 1st place
as Skycap’n Kragg
</td>
</tr>
<tr>
<td>
Rainy Day Fund<br>(Completed 2023/06/08)
</td>
<td>
Upgrade your Tavern Tier twice in one turn as Skycap’n Kragg
</td>
</tr>
<tr>
<td>
<b>Snake Eyes</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ab242e1c6ed6993963d7362ceefa0ec34e183a9241653f73f6b5b43525172f9a.png">
</td>
<td>
This is how I roll<br>(Completed 2023/12/09)
</td>
<td>
Roll a 6-sided die. Gain that much Gold. (Cannot be used again for that
many turns!)<br>Finish in 1st place as Snake Eyes
</td>
</tr>
<tr>
<td>
<b>Sneed</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/feb36c75696b071a11ba92b88b707cc257e78000e34d79ffb0fd6408af212984.png">
</td>
<td>
Mind if I Roll Sneed?<br>(Completed 2023/0622)
</td>
<td>
Give a minion "Deathrattle: Summon a random minion from a Tavern Tier
lower."<br>Finish in 1st place as Sneed
</td>
</tr>
<tr>
<td>
<b>Sylvanas Windrunner</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dbe5dc9a94396a2e96ceca5d9ef7db743f269a1563538ed9541ff83a370de47d.png">
</td>
<td>
Let None Survive!<br>(Completed 2023/09/24)
</td>
<td>
Give +2/+1 to your minions that died last combat.<br>Finish in 1st place
as Sylvanas Windrunner
</td>
</tr>
<tr>
<td>
<b>Tae'thelan Bloodwatcher</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/172f7600ab278bde0612c1e675a2d9fd81b80a5799e6b141acfbb4c407a67af7.png">
</td>
<td>
Rewarding Research<br>(Completed 2023/12/20)
</td>
<td>
Passive Every third Tavern spell you buy costs (0). <br>Finish in 1st
place as Tae'thelan Bloodwatcher
</td>
</tr>
<tr>
<td>
<b>Tamsin Roame</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/24a33af73786549cfc465e34350bacaf03bcfef217e59460ba667d3ac8a0f4c7.png">
</td>
<td>
A-Roame-atherapy<br>(Completed 2023/10/22)
</td>
<td>
Start of Combat: Destroy your lowest Health minion. Give its stats to
your other minions.<br>Finish in 1st place as Tamsin Roame
</td>
</tr>
<tr>
<td>
<b>Tavish Stormpike</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d82e87fb69bd11aec529281ccc903b4ea9e87f29e3ffdf7a54a4a57e972954d0.png">
</td>
<td>
360 No Scope!<br>(Completed 2023/05/28)
</td>
<td>
Take aim! Start of Combat: Deal damage to your target. (Doubles every
turns!) Take aim! Start of Combat: Deal damage to your target. (Doubles
in (turn, turns)!)<br>Finish in 1st place as Tavish Stormpike
</td>
</tr>
<tr>
<td>
<b>Teron Gorefiend</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ef5a9e031e2cdcdda9fa55124294247b3502d97e888f3c752d8cddf548f46d18.png">
</td>
<td>
That was Goreious!<br>(Completed 2023/11/17)
</td>
<td>
Choose a friendly minion. Start of Combat: Destroy it. Once you have
space, resummon an exact copy.<br>Finish in 1st place as Teron Gorefiend
</td>
</tr>
<tr>
<td>
<b>Tess Greymane</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/17a12a8a7335de3807f74c8eb00fdb30e12c518b5f916eaa7507072a86087580.png">
</td>
<td>
Couldn't Do It Without You<br>(Completed 2023/11/18)
</td>
<td>
Refresh Bob's Tavern with your last opponent's warband.<br>Finish in 1st
place as Tess Greymane
</td>
</tr>
<tr>
<td rowspan="2">
<b>The Curator</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2198929a67a341765cd3b60141f40522c6e51e1ef47d6e29c56c8ed7abad7a2b.png">
</td>
<td>
Menagerie Manager<br>(Completed 2023/09/24)
</td>
<td>
Passive. Start the game with a 1/1 Amalgam with Venomous and all minion
types.<br>Finish in 1st place as The Curator
</td>
</tr>
<tr>
<td>
LOST "DOG". CALL CURATOR<br>(Completed 2023/09/21)
</td>
<td>
End your turn with the Amalgam in play while not playing as The Curator
</td>
</tr>
<tr>
<td rowspan="2">
<b>The Great Akazamzarak</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/19e004e0d5733af2e9e6a698f6e7606d465fb8917c9f20b55b4e4318f1b2eb7c.png">
</td>
<td>
The Magician's Code<br>(Completed 2023/09/22)
</td>
<td>
Discover a Secret. Put it into the battlefield.<br>Finish in 1st place
as The Great Akazamzarak
</td>
</tr>
<tr>
<td>
Comeback King<br>(Completed 2023/09/22)
</td>
<td>
Finish in 1st place after having your Ice Block triggered as The Great
Akazamzarak
</td>
</tr>
<tr>
<td>
<b>The Jailer</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dd00d910a25ecacf576f4a9eef0aabef472ebb063d99af7771ac5355c6a4cfa4.png">
</td>
<td>
Master of Maw<br>(Completed 2023/11/13)
</td>
<td>
Give a minion + /+ . Upgrades after nine friendly minions die. ( left!)
Give a minion + /+ . Upgrades after nine friendly minions die. (
left!)<br>Finish in 1st place as The Jailer
</td>
</tr>
<tr>
<td>
<b>The Lich King</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d5ecee3ec0546d517e6ac31803f76126787e4d7a4c64ab0160db9d0b4f64e61e.png">
</td>
<td>
There Must Always Be a Lich King<br>(Completed 2023/07/16)
</td>
<td>
Give a minion Reborn until next turn.<br>Finish in 1st place as The Lich
King
</td>
</tr>
<tr>
<td>
<b>The Nameless One</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/b1c0549d7a441bc3753353748eb06cdb93be99ccfda77be36741b635885f41e0.png">
</td>
<td>
No Ones a Winner<br>(Completed 2024/04/27)
</td>
<td>
Passive. Start of Game: Copy your teammate's Hero Power.<br>Finish in
1st place as The Nameless One
</td>
</tr>
<tr>
<td>
<b>The Rat King</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/13de6035422fd8200c3ec5c095a1a75d92767b395e6be340633d641c84e85fc1.png">
</td>
<td>
There Must Always Be a Rat King<br>(Completed 2023/11/09)
</td>
<td>
Discover a minion of a specific minion type. Swaps type each
turn.<br>Finish in 1st place as The Rat King
</td>
</tr>
<tr>
<td>
<b>Thorim, Stormlord<b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/7ade2fe78e8170fe89a0329f3989f7797702fd5b046f6a9c1f8a6d9d00fa349b.png">
</td>
<td>
A Tier above the rest<br>(Completed 2023/01/29)
</td>
<td>
Meetings always reach quorum when they're led by Thorim. Passive. At the
start of the game, Discover a Tier 7 minion to get after you spend 65
Gold.<br>Finish in 1st place as Thorim, Stormlord
</td>
</tr>
<tr>
<td rowspan="2">
<b>Tickatus</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f9de22070f0cf2698497bfed225dc408d2dfa18d4a56f390497cbdb561fc0b2a.png">
</td>
<td>
Ticketmaster<br>(Completed 2023/07/21)
</td>
<td>
Passive Every 4 turns, Discover a Darkmoon Prize. ( (turn, turns)
left!)<br>Finish in 1st place as Tickatus
</td>
</tr>
<tr>
<td>
And YOU get Prize Cards!<br>(Completed 2023/08/23)
</td>
<td>
Play 16 different Darkmoon Prizes.
</td>
</tr>
<tr>
<td>
<b>Trade Prince Gallywix</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/1dc20a3d033232bea3a95e1b53659be58fc32a34cb6973da45c45e2288fba644.png">
</td>
<td>
Profit Prophet<br>(Completed 2023/11/26)
</td>
<td>
Passive After you sell a minion, get 1 extra Gold next turn.<br>Finish
in 1st place as Trade Prince Gallywix
</td>
</tr>
<tr>
<td>
<b>Vanndar Stormpike</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f8065b689b8dfde3b8c25f19f4cb6619241e00b78c3b8ce9f549318c55ad0390.png">
</td>
<td>
Stormpike Stardom<br>(Completed 2023/11/11)
</td>
<td>
Passive Avenge (2): Give your minions +1 Health permanently.<br>Finish
in 1st place as Vanndar Stormpike
</td>
</tr>
<tr>
<td>
<b>Varden Dawngrasp</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dd3acdbb568b6a9ad80181048b2151d287e8b751d30435bb3f301b96d08aa4db.png">
</td>
<td>
Chill Out<br>(Completed 2023/08/27)
</td>
<td>
Passive. After Bob's Tavern is Refreshed, copy his highest Tier minion
and Freeze them both.<br>Finish in 1st place as Varden Dawngrasp
</td>
</tr>
<tr>
<td>
<b>Vol’jin</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/684eb855c8eb91247a4e4e968ea63f94d320d0563ac64bac76d357abbae7080e.png">
</td>
<td>
Da Voodoo Shuffle<br>(Completed 2024/02/03)
</td>
<td>
Choose two minions. Swap their stats.<br>Finish in 1st place as Vol’jin
</td>
</tr>
<tr>
<td>
<b>Xyrella</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/427f8059ebf91ccd337c4e9b92ea8fda0059c8e87f9e431bbb6b8987671f7c1d.png">
</td>
<td>
Brightest Kid Around<br>(Completed 2023/07/09)
</td>
<td>
Choose a minion in Bob's Tavern to add to your hand. Set its stats to
2.<br>Finish in 1st place as Xyrella
</td>
</tr>
<tr>
<td>
<b>Yogg-Saron, Hope’s End</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f1dc1cda3b54e88565c0e68cf7b6daede62e526b13ac92e52259bba6be961818.png">
</td>
<td>
Madness in the Tavern<br>(Completed 2023/11/05)
</td>
<td>
Add a random minion in Bob's Tavern to your hand. Give it
+1/+1.<br>Finish in 1st place as Yogg-Saron. Hope’s End
</td>
</tr>
<tr>
<td>
<b>Ysera</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3958796b08f99af07900ad211c849806c3e3303885f5834ab0029ad8a02fed32.png">
</td>
<td>
A Dream Come True<br>(Completed 2024/05/01)
</td>
<td>
Passive Bob always offers an extra Dragon whenever the Tavern is
Refreshed.<br>Finish in 1st place as Ysera
</td>
</tr>
<tr>
<td>
<b>Y'Shaarj</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/9662e21c32847be6f13d7d54010ed8e33d84dfe70aefc7bda6ddd2d3b2d09bbd.png">
</td>
<td>
Rage Your Dream<br>(Completed 2023/07/16)
</td>
<td>
Start of Combat: Summon and get a random minion from your Tavern
Tier.<br>Finish in 1st place as Y'Shaarj
</td>
</tr>
<tr>
<td>
<b>Zephrys the Great</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/8a3ce8409e37fa1f637fbe1889b4fd47bb99b989c905af6660b2b2671a6fed1a.png">
</td>
<td>
Phenomental Cosmic Powers<br>(Completed 2023/05/27)
</td>
<td>
If you have two copies of a minion, find the third. ( (Wish, Wishes)
left!)<br>Finish in 1st place as Zephrys the Great
</td>
</tr>
</table>
]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>Hearthstone</tag>
        <tag>Battlegrounds</tag>
        <tag>game</tag>
      </tags>
  </entry>
  <entry>
    <title>在riscv平台优化SGEMM</title>
    <url>/2024/02/26/102_sgemm_riscv_cn/</url>
    <content><![CDATA[<p>本项目记录了在riscv平台上优化SGEMM（单精度浮点通用矩阵乘法）的过程。</p>
<span id="more"></span>
<p><img src=/images/sgemm/data/result_all.png width=70% /></p>
<p>本项目记录了在riscv平台上优化SGEMM（单精度浮点通用矩阵乘法）的过程。</p>
<p>通用矩阵乘（<code>General Matrix Multiplication</code>，简称<code>gemm</code>）是深度学习框架的核心计算单元之一，广泛用于<code>Convolution</code>、<code>Full connection</code>、<code>Matmul</code>等算子实现。</p>
<p>我是在 <strong>全志 Nezha D1</strong>
开发板上进行的实验与探索，<code>version 0 - version 5</code> 是使用的 C
语言，<code>version 6 - version 9</code> 则部分使用了汇编，涉及到
<code>RISC-V V</code> 扩展指令。</p>
<p>注意：不同于其他的某些gemm优化工程，在本工程中，全部使用
<strong>行主序</strong> 的矩阵。因为我喜欢 <strong>行主序</strong>！</p>
<h1 id="预备知识">预备知识</h1>
<p><img src=/images/sgemm/pics/riscv.gif width=50% /></p>
<p><a href="https://riscv.org/">RISC-V</a> 是一种开放标准指令集架构
(ISA)，通过开放协作开启处理器创新的新时代。</p>
<hr />
<p><img src=/images/sgemm/pics/GEMM.png width=70% /></p>
<p><a
href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">GEMM</a>
通用矩阵乘法，基本线性代数子程序之一。</p>
<hr />
<p><a
href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a>每秒浮点运算次数，亦称每秒峰值速度，（英语：<code>Floating-point operations per second</code>；缩写：<code>FLOPS</code>），即每秒所执行的浮点运算次数。一个
<code>GFLOPS(gigaFLOPS)</code> 等于每秒十亿（10^9）次的浮点运算；</p>
<p>矩阵乘的计算量是
<code>2 * M * N * K</code>，计算量除以耗时即为当前gemm版本的gflops。 -
乘以 2 是因为每次操作包含一次乘法和一次加法</p>
<h1 id="准备工作">准备工作</h1>
<p>相关代码位于 <code>./prepare/</code>。</p>
<h2 id="测试交叉编译">测试交叉编译</h2>
<p>我使用的 <strong>全志 Nezha D1</strong> 开发板，在<a
href="https://xuantie.t-head.cn/community/download?id=4090445921563774976">这里</a>下载的交叉编译链接。</p>
<p>详细教程见<a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/blob/main/prepare/README.md">readme</a></p>
<h2 id="内存带宽测试">内存带宽测试</h2>
<p>分别通过以下几个小工程对开发板内存带宽进行测试</p>
<ul>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/2.memcpy_bandwidth_test">2.memcpy_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/3.flw_bandwidth_test">3.flw_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/4.vlw_bandwidth_test">4.vlw_bandwidth_test</a></li>
</ul>
<p><img src=/images/sgemm/prepare/imgs/memory_bandwidth_test.png width=70% /></p>
<h2 id="roofline模型">roofline模型</h2>
<p><a href="https://en.wikipedia.org/wiki/Roofline_model">Roofline</a>
提出了使用
<code>Operational Intensity</code>（计算强度）进行定量分析的方法，并给出了模型在计算平台上所能达到理论计算性能上限公式。</p>
<p>根据<a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a>所说 - <code>D1</code>的算力可达
<code>4 GFlops(@1GHz)</code> - Memory : <code>2.727 GB/s</code> (DDR3
792 MHz)。 -
虽然我测出来最高是<code>2.592 GB/s</code>，可能哪里出了问题？ -
商汤还是要相信一下的，暂时以他的为准吧</p>
<p><img src=/images/sgemm/pics/roofline.png width=70% /></p>
<h1 id="sgemm优化">sgemm优化</h1>
<p>相关代码位于 <code>./sgemm/</code>。</p>
<h2 id="使用说明">使用说明</h2>
<p>以 <code>step0</code> 为例，你需要先编辑 Makefile
，配置自己的交叉编译链。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> sgemm/step0/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb push test_bl_sgemm_step0.x ./.</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb shell <span class="string">&#x27;./test_bl_sgemm_step0.x&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h2 id="version-0-naive版本">Version 0: naive版本</h2>
<p>这个版本思路在我看来是<strong>最直观</strong>的，毕竟我当时就是这么<strong>学习</strong>、<strong>理解</strong>和<strong>计算</strong>矩阵乘法的:
&gt; A 的一行乘以 B 的一列得到 C 的一个元素。</p>
<p><img src=/images/sgemm/data/result_0.png width=70% /></p>
<p><img src=/images/sgemm/pics/step0.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;          <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;      <span class="comment">// Start 0-st loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>我认为<code>version 0</code>非常契合的解释了 <span
class="math inline">\(C_{mn} = \sum_{k=1}^{K} A_{mk}B_{kn}\)</span>
这个公式。</p>
<p>但是这个版本缺点比较明显：在理论算力为 <code>4 GFLOPS</code>
的平台仅发挥出最大 <code>0.03 GFLOPS</code>
的计算性能。这是因为<strong>对于矩阵B的访问，cache命中率极低，即「空间局部性很差」</strong>。整个计算下来，相当于访问矩阵B的次数多了很多很多次。</p>
<p>对于多维数组的元素尽量能按照顺序访问。这样可以改善内存访问的空间局部性，对缓存更加友好。</p>
<p>另外，观察到随着尺寸的增加，性能有较大的抖动。分析数据可以知道：当<code>m=n=k</code>为
128 164 192 228 256 288 320 352 384 时性能都很差。这几个数相差32，32 *
4(<code>sizeof(float)</code>) = 128 B。</p>
<p>猜测性能抖动与<code>cacheline</code>以及<code>硬件预取</code>有关——
cacheline = 64B，cache miss
后，硬件预取即<code>HWPrefetcher</code>，多读 1 个 cacheline。</p>
<h2 id="version-1-循环交换版本">Version 1: 循环交换版本</h2>
<p>重复使用Cache中的数据是最基本的高效使用Cache方法。对于多层嵌套循环，可以通过交换两个嵌套的循环（<code>loop interchange</code>）、逆转循环迭代执行的顺序（<code>loop reversal</code>）、将两个循环体合并成一个循环体（<code>loop fusion</code>）、循环拆分（<code>loop distribution</code>）、循环分块（<code>loop tiling</code>）、<code>loop unroll and jam</code>等循环变换操作。</p>
<p>选择适当的循环变换方式，既能保持程序的语义，又能改善程序性能。</p>
<p><img src=/images/sgemm/data/result_1.png width=70% /></p>
<p><img src=/images/sgemm/pics/step1.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;          <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;      <span class="comment">// Start 0-nd loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>相较于<code>version 0</code>，<code>version 1</code>对于矩阵B的操作，空间局部性较好，性能有较大提升（尤其是对于尺寸较大的情况，而在
m = n = k &lt;= 68 时，版本0的效率更高）。</p>
<p>调整m,n,k的顺序，对结果没有影响(即保持程序的语义)，但是可以影响性能。
测试不同循环顺序的性能（全志Nezha D1平台，以m=n=k=512为例）</p>
<table>
<thead>
<tr class="header">
<th>循环顺序</th>
<th>GFLOPS</th>
<th>分析</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNK</td>
<td>0.012</td>
<td>访问B的cache miss高</td>
</tr>
<tr class="even">
<td>MKN</td>
<td>0.180</td>
<td></td>
</tr>
<tr class="odd">
<td>NMK</td>
<td>0.012</td>
<td>访问B的cache miss高</td>
</tr>
<tr class="even">
<td>NKM</td>
<td>0.009</td>
<td>访问A的cache miss高</td>
</tr>
<tr class="odd">
<td>KMN</td>
<td>0.165</td>
<td></td>
</tr>
<tr class="even">
<td>KNM</td>
<td>0.009</td>
<td>访问A的cache miss高</td>
</tr>
</tbody>
</table>
<p>但是，<code>version 1</code>
的硬件使用率仍然很低，继续想办法优化。</p>
<h2 id="version-2-分块版本">Version 2: 分块版本</h2>
<p><img src=/images/sgemm/data/result_2.png width=70% /></p>
<p><img src=/images/sgemm/pics/step2.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;          <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;        <span class="comment">// Start 1-st loop</span></span><br><span class="line">        AddDot_4x4_opt( k, &amp;A( i, <span class="number">0</span> ), lda, &amp;B( <span class="number">0</span>, j ), ldb, &amp;C( i, j ), ldc );</span><br><span class="line">    &#125;                                          <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                              <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p><img src=/images/sgemm/pics/gemm_block.png width=70% /></p>
<p>为了避免多余的 cache 换入换出，于是进行分块处理。<a
href="https://zhuanlan.zhihu.com/p/342923482">浅谈分块矩阵优化方法为什么有用</a>一文讲的挺好，建议阅读学习。</p>
<p>版本2进行了分块操作后，性能仍然不理想，这是因为，这个版本只是表面逻辑上实现了分块，块内的计算还有一些小技巧没有应用。</p>
<h2 id="version-3-分块优化版本">Version 3: 分块优化版本</h2>
<p><img src=/images/sgemm/data/result_3.png width=70% /></p>
<p><img src=/images/sgemm/pics/step3.gif width=70% /></p>
<p>增加了AddDot_4x4_opt</p>
<p>在<a
href="https://github.com/flame/blislab/blob/master/tutorial.pdf">BLISlab-tutorial</a>中提到了几个小技巧：</p>
<ul>
<li>循环展开 2.4.2 Loop unrolling
<ul>
<li>Updating loop index i and the pointer cp every time through the
inner loop creates considerable overhead. For this reason, a compiler
will perform loop unrolling.</li>
</ul></li>
<li>寄存器缓存 2.4.3 Register variables
<ul>
<li>Notice that computation can only happen if data is stored in
registers. A compiler will automatically transform code so that the
intermediate steps that place certain data in registers is
inserted.</li>
</ul></li>
</ul>
<p>使用了这个技巧后，这个版本性能有了大幅度的提升！</p>
<p>然而，这个版本对于尺寸较大的矩阵，性能仍然比较低。查找原因，比如访问B[0,0],
B[0,1], B[0,2],
B[0,3]后，访问B[1,0]的时候，当尺寸较大的时候必定<code>cache miss</code>。因此，如果能提前把数据重新排列就好了。</p>
<h2 id="version-4-b-prepack版本">Version 4: B prepack版本</h2>
<p><img src=/images/sgemm/data/result_4.png width=70% /></p>
<p><img src=/images/sgemm/pics/step4.gif width=70% /></p>
<p>我默认矩阵B是<strong>参数</strong>，所以可以提前进行<code>pack</code>操作。版本4对矩阵B进行了<code>prepack</code>，性能更进一步！</p>
<p>性能提升的原因很明显：访问矩阵 B 的 <code>cache miss</code>
明显减少了。这也是我第一次深刻意识到模型推理前对神经网络权重做<code>prepack</code>的重要性。</p>
<p>可以看到，当尺寸比较大的时候，性能还是有所下降的。这应该是对矩阵A访问的
<code>cache miss</code> 比较多。要对A做 <code>pack</code> 吗？</p>
<p>我默认矩阵A是<strong>输入</strong>，因此对A做 <code>pack</code>
的话是不能提前做的，是要算在整体时间内的。那么，有必要吗？</p>
<h2 id="version-5-a-pack-b-prepack版本">Version 5: A pack &amp; B
prepack版本</h2>
<p><img src=/images/sgemm/data/result_5.png width=70% /></p>
<p><img src=/images/sgemm/pics/step5.gif width=70% /></p>
<p>在版本4的基础上，版本5对矩阵A进行了<code>pack</code></p>
<p>这里默认矩阵A是<strong>输入</strong>，所以需要在计算过程中进行<code>pack</code>操作，这个耗时是要放在计时里面的。</p>
<p>结果还是让人欣喜的，尤其是在大尺寸矩阵上，取得了进一步的性能提升。</p>
<p>我本来是试一试的心态做了这个尝试，毕竟这个操作会多读一遍<code>A</code>以及写一遍<code>packA</code>。看来接下来主要就是与<code>cache miss</code>作斗争了。</p>
<p>目前思路方面的优化基本到头了，在计算过程中做一些<code>preload</code>是值得尝试一下的。</p>
<p>接下来上汇编，搞向量计算，以及在汇编里面做<code>preload</code>。</p>
<h2 id="version-6-汇编版本">Version 6: 汇编版本</h2>
<p><img src=/images/sgemm/data/result_6.png width=70% /></p>
<p>简要说明：A没有<code>pack</code>，B进行了16个数的<code>prepack</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;       <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">    <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;   <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">        <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>关于 <code>rvv</code> 指令的使用，我认为<code>vsetvli</code>
是灵魂，<code>vfmacc.vf</code> 是主力。</p>
<p>关于这些内容，我从<a
href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 | RISC-V
技术解析</a>学到了很多，他们可真专业啊！建议理论指导和知识点学习去他们那里学，向
<code>OpenPPL</code> 致敬！</p>
<p>至于汇编算子，汇编里面的小细节就多了，强烈吐槽：<strong>写汇编真烦人啊！尤其是
debug 过程，很折磨人。</strong>
我上次写汇编还是本科上课的时候，重新拾起还是有一些新奇和兴奋的，而且能够非常细粒度的控制算子执行，成就感还是很大的。</p>
<p>关于汇编文件具体怎么实现的，我认为最快的方式就是去看汇编代码。这里就不多做解释了</p>
<p>需要注意的是，这个版本效果很差，这是为什么呢？又是
<strong>循环顺序</strong> 的问题。</p>
<h2 id="version-7-汇编版本-调换循环顺序">Version 7: 汇编版本
调换循环顺序</h2>
<p><img src=/images/sgemm/data/result_7.png width=70% /></p>
<p>简要说明：A没有<code>pack</code>，B进行了16个数的<code>prepack</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;       <span class="comment">// Start 2-st loop</span></span><br><span class="line">    <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">    <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">    <span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;   <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">        <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>调换了循环的次序，先n方向后m方向，性能大大提升。</p>
<p>但是，大尺寸矩阵的性能还不是很好。究其原因，还是在访存上。大尺寸矩阵的计算在<code>roofline</code>模型里属于是<strong>计算密集型</strong>，理想情况是<strong>计算的时间</strong>和<strong>访存的时间</strong>尽可能重叠，而现在基本是花很多时间在访存了（又是因为<code>cache miss</code>！）。</p>
<h2 id="version-8-汇编版本-加入preload">Version 8: 汇编版本
加入preload</h2>
<p><img src=/images/sgemm/data/result_8.png width=70% /></p>
<p>简要说明：A没有<code>pack</code>，B进行了16个数的<code>prepack</code>，做了
<code>preload</code> 操作。</p>
<p>性能相对爆炸！最高达到了 <code>2.212 GFLOPS</code>。</p>
<p>核心操作： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfmacc.vf v16,  ft0, v0</span><br><span class="line">vlw.v v4, (bp0)         # b0&#x27;-&gt;v4</span><br><span class="line">flw fs4, 384(bp0)       # pre-load B</span><br><span class="line">addi bp0,bp0,64</span><br><span class="line">vfmacc.vf v20,  ft1, v0</span><br></pre></td></tr></table></figure> 在 <code>vfmacc.vf</code> 之间加入一些
<code>load</code>
操作，把之后要用到的数据提前<code>load</code>到<code>cache</code>中，可以大大降低
<code>cache miss</code>。</p>
<p>我最开始很疑惑——这样看代码明明也是顺序计算，怎么做到<strong>计算的时间</strong>和<strong>访存的时间</strong>重叠的呢？直到后来了解到<code>cacheline</code>的原理才明白这里的精髓，果然基础知识很重要啊。</p>
<h2 id="version-9-汇编版本-a做pack">Version 9: 汇编版本 A做pack</h2>
<p><img src=/images/sgemm/data/result_9.png width=70% /></p>
<p>按照之前的经验，也尝试对 矩阵A 做了一下
<code>pack</code>，出乎意料的是结果不是很好。稍微分析了一下，应该是这个版本的汇编对
矩阵A 的<code>preload</code>写得不是很好。</p>
<p>上个版本虽然对A没有pack，但是对4排的A都有<code>preload</code>，所以也算是解决了矩阵A的
<code>cache miss</code>的痛点吧。</p>
<h1 id="总结">总结</h1>
<p>要想继续优化这个算子，后续要做的事情还有很多，比如在汇编里面重排流水线。</p>
<p>在<a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a>中也提到，如果使用 vf 指令，能够达到它理论峰值的
80%，即
4*80%，3.2GFLOPs。我现在只有<code>2.121 GFLOPS</code>，理论上还是有很大优化空间的。</p>
<p>另外，
RVV目前用的是<code>0.7.1</code>版本，感觉RVV的指令优化还是有很多工作要做，比如，目前遇到最严重的<code>vlw</code>效率低的问题。</p>
<p>总之，做这些工作让我从很多大佬那里学到了很多知识，非常感谢。同时也希望本文能帮助到更多的人。</p>
<h1 id="致谢">致谢</h1>
<ul>
<li><p><a href="https://github.com/flame/blislab">BLISlab: A Sandbox for
Optimizing GEMM</a></p>
<p>这个项目让我了解了如何优化GEMM。</p></li>
<li><p><a
href="https://github.com/surez-ok/blislab_riscv">riscv平台优化矩阵乘(基于blislab优化实践)</a></p>
<p>我基于这个项目进行了实验和探索。</p></li>
<li><p>感谢丁大佬的指导。</p></li>
</ul>
<h1 id="参考文章">参考文章</h1>
<p><a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a></p>
<p><a href="http://riscvbook.com/">RISC-V-Reader</a></p>
<p><a
href="https://github.com/riscv/riscv-v-spec/releases">riscv-v-spec-0.7.1</a></p>
<p><a
href="https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf">Anatomy
of High-Performance Matrix Multiplication</a></p>
<p><a
href="https://developer.huawei.com/consumer/cn/forum/topic/0201105374686101528">编译器优化丨Cache优化</a></p>
<p><a
href="https://www.leiphone.com/category/yanxishe/Puevv3ZWxn0heoEv.html">OpenBLAS项目与矩阵乘法优化
| AI 研习社</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34204282">Roofline
Model与深度学习模型的性能分析</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/342923482">浅谈分块矩阵优化方法为什么有用</a></p>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>RISCV</tag>
        <tag>GEMM</tag>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTeX学习</title>
    <url>/2023/05/30/69_LaTeX/</url>
    <content><![CDATA[<p>今天看的论文的排版真的太好看了！开始学习<code>LaTeX</code>，整个过程记录一下。</p>
<span id="more"></span>
<p><code>mactex</code>下载地址：<a
href="https://www.tug.org/mactex/">https://www.tug.org/mactex/</a></p>
<pre><code>MacTeX.pkg.
The Size is 5.2G - 15 March 2023
The MD5 sum is 64416e6da409609c48970e23906e7642
The SHA512 sum is 0ff569be6af3e658b4a60e2b89bf10c41b9bae38bdaee60021c69aea238d2436886c0883954ab0a64394d9a6e080958b2fc82f91ac5c4dfe9e1de32df46efd49</code></pre>
<p><a
href="https://www.tug.org/mactex/READ_ME_FIRST.pdf">READ_ME_FIRST</a>给了我很大帮助。</p>
<ul>
<li>2023.05.31</li>
</ul>
<p>看了<a href="https://www.doc88.com/p-9773781953864.html">LaTeX Notes
v1.20</a>，<a
href="https://www.dralpha.com/">作者</a>很有意思，也给我打开了<code>LaTeX</code>大门。</p>
<p>新的版本位于<a
href="https://github.com/huangxg/lnotes">https://github.com/huangxg/lnotes</a>,<a
href="https://github.com/huangxg/lnotes/blob/master/lnotes2.pdf">PDF文档</a>。</p>
<p>在<a
href="https://zhuanlan.zhihu.com/p/521649367">知乎</a>看到一篇很好的文章</p>
<p>【更新中】</p>
]]></content>
      <tags>
        <tag>LaTeX</tag>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title>WeightonlyGEMM:dequantize_s4_to_fp16x2代码解析</title>
    <url>/2024/08/12/101_WeightonlyGEMM:%20dequantize_s4_to_fp16x2%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>本文聚焦于AWQ的W4A16 (4-bit weight, 16-bit activation) CUDA
kernel的反量化。</p>
<span id="more"></span>
<h1 id="背景">背景</h1>
<p>在 ZZK大佬的文章 <a
href="https://zhuanlan.zhihu.com/p/675427125">可能是讲的最清楚的WeightonlyGEMM博客</a>
中学到很多知识</p>
<hr />
<p>最近在学AWQ的量化，<a
href="https://arxiv.org/pdf/2306.00978">AWQ论文</a></p>
<p>其中我比较关注GPU的部分，其中数据排布这块很有意思：</p>
<p><img src="/images/101/1.jpeg" width="60%"></p>
<p>于是到<a href="https://arxiv.org/pdf/2211.10017">Who Says Elephants
Can’t Run</a> 这篇论文看了一下，<strong>WeightOnly GEMM</strong>
技术就是在这里提出的，其中权重反量化是这么操作的：</p>
<p><img src="/images/101/2.jpeg" width="60%"></p>
<p>这个论文提出了一种 `IntToFloat
的巧妙的方式，可以大大提速反量化速度。</p>
<p><img src="/images/101/3.jpeg" width="60%"></p>
<p>背景介绍结束，更详细的讲解可以直接看ZZK大佬的文章，接下来解释
<code>dequantize_s4_to_fp16x2</code> 代码。</p>
<h1 id="代码">代码</h1>
<p>首先放出完整代码（80行）： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__device__ uint4 <span class="title function_">dequantize_s4_to_fp16x2</span><span class="params">(<span class="type">uint32_t</span> <span class="type">const</span>&amp; source)</span> &#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__CUDA_ARCH__) &amp;&amp; __CUDA_ARCH__ &lt; 750</span></span><br><span class="line">  assert(<span class="literal">false</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  uint4 result;</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span>* h = reinterpret_cast&lt;<span class="type">uint32_t</span>*&gt;(&amp;result);</span><br><span class="line">  <span class="type">uint32_t</span> <span class="type">const</span> i4s = reinterpret_cast&lt;<span class="type">uint32_t</span> <span class="type">const</span>&amp;&gt;(source);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First, we extract the i4s and construct an intermediate fp16 number.</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> immLut = (<span class="number">0xf0</span> &amp; <span class="number">0xcc</span>) | <span class="number">0xaa</span>;</span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> BOTTOM_MASK = <span class="number">0x000f000f</span>;</span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> TOP_MASK = <span class="number">0x00f000f0</span>;</span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> I4s_TO_F16s_MAGIC_NUM = <span class="number">0x64006400</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Note that the entire sequence only requires 1 shift instruction. This is</span></span><br><span class="line">  <span class="comment">// thanks to the register packing format and the fact that we force our</span></span><br><span class="line">  <span class="comment">// integers to be unsigned, and account for this in the fp16 subtractions. In</span></span><br><span class="line">  <span class="comment">// addition, I exploit the fact that sub and fma have the same throughput in</span></span><br><span class="line">  <span class="comment">// order to convert elt_23 and elt_67 to fp16 without having to shift them to</span></span><br><span class="line">  <span class="comment">// the bottom bits before hand.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW</span></span><br><span class="line">  <span class="comment">// dependency if we issue immediately before required.</span></span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> top_i4s = i4s &gt;&gt; <span class="number">8</span>;</span><br><span class="line">  <span class="comment">// Extract elt_01 - (i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line">  <span class="comment">// Extract elt_23 (i4s &amp; 0x00f000f0) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(TOP_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line">  <span class="comment">// Extract elt_45 (top_i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">2</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(top_i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line">  <span class="comment">// Extract elt_67 (top_i4s &amp; 0x00f000f0) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">3</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(top_i4s), <span class="string">&quot;n&quot;</span>(TOP_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// I use inline PTX below because I am not sure if the compiler will emit</span></span><br><span class="line">  <span class="comment">// float2half instructions if I use the half2 ctor. In this case, I chose</span></span><br><span class="line">  <span class="comment">// performance reliability over code readability.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// This is the half2 &#123;1032, 1032&#125; represented as an integer.</span></span><br><span class="line">  <span class="comment">// static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64086408;</span></span><br><span class="line">  <span class="comment">// Haotian: subtract &#123;1024, 1024&#125; instead, we do not need to map to [-8, 7]</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> FP16_TOP_MAGIC_NUM = <span class="number">0x64006400</span>;</span><br><span class="line">  <span class="comment">// This is the half2 &#123;1 / 16, 1 / 16&#125; represented as an integer.</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> ONE_SIXTEENTH = <span class="number">0x2c002c00</span>;</span><br><span class="line">  <span class="comment">// This is the half2 &#123;-72, -72&#125; represented as an integer.</span></span><br><span class="line">  <span class="comment">// static constexpr uint32_t NEG_72 = 0xd480d480;</span></span><br><span class="line">  <span class="comment">// Haotian: Let&#x27;s use &#123;-64, -64&#125;.</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> NEG_64 = <span class="number">0xd400d400</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Finally, we construct the output numbers.</span></span><br><span class="line">  <span class="comment">// Convert elt_01</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;sub.f16x2 %0, %1, %2;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">0</span>]), <span class="string">&quot;r&quot;</span>(FP16_TOP_MAGIC_NUM))</span>;</span><br><span class="line">  <span class="comment">// Convert elt_23</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;fma.rn.f16x2 %0, %1, %2, %3;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">1</span>]), <span class="string">&quot;r&quot;</span>(ONE_SIXTEENTH), <span class="string">&quot;r&quot;</span>(NEG_64))</span>;</span><br><span class="line">  <span class="comment">// Convert elt_45</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;sub.f16x2 %0, %1, %2;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">2</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">2</span>]), <span class="string">&quot;r&quot;</span>(FP16_TOP_MAGIC_NUM))</span>;</span><br><span class="line">  <span class="comment">// Convert elt_67</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;fma.rn.f16x2 %0, %1, %2, %3;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">3</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">3</span>]), <span class="string">&quot;r&quot;</span>(ONE_SIXTEENTH), <span class="string">&quot;r&quot;</span>(NEG_64))</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="代码解析">代码解析</h1>
<p>接下来逐句分析： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__device__ uint4 <span class="title function_">dequantize_s4_to_fp16x2</span><span class="params">(<span class="type">uint32_t</span> <span class="type">const</span>&amp; source)</span></span><br></pre></td></tr></table></figure></p>
<p>这行代码定义了一个名为 <code>dequantize_s4_to_fp16x2</code>
的函数，这个函数是一个 CUDA 设备函数
(<strong>device</strong>)，它的输入是一个常量引用 <code>uint32_t</code>
类型的 <code>source</code>，返回值是 <code>uint4</code> 类型。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__CUDA_ARCH__) &amp;&amp; __CUDA_ARCH__ &lt; 750</span></span><br><span class="line">  assert(<span class="literal">false</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br></pre></td></tr></table></figure>
<p>这段预处理指令检查 CUDA 架构版本是否小于
750。如果是，则触发断言错误，表明该代码不适用于 CUDA 750
以下的架构。否则，继续执行后面的代码。 比如这个函数里面用到了
<code>lop3.b32</code> 函数，可以在 <a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/">这里</a>
查到</p>
<pre><code>Requires sm_50 or higher.
Qualifier .BoolOp requires sm_70 or higher.</code></pre>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">uint4 result;</span><br><span class="line"></span><br><span class="line"><span class="type">uint32_t</span>* h = reinterpret_cast&lt;<span class="type">uint32_t</span>*&gt;(&amp;result);</span><br><span class="line"><span class="type">uint32_t</span> <span class="type">const</span> i4s = reinterpret_cast&lt;<span class="type">uint32_t</span> <span class="type">const</span>&amp;&gt;(source);</span><br></pre></td></tr></table></figure>
<p>这里定义了一个 <code>uint4</code> 类型的变量
<code>result</code>。</p>
<p>接着将 <code>result</code> 的地址转换为 <code>uint32_t*</code>
类型的指针 <code>h</code>，以便后续的位操作。</p>
<p>然后将 <code>source</code> 重新解释为 <code>uint32_t</code>
类型并赋值给 <code>i4s</code>。</p>
<p>通过类型转换和指针操作，使得对这些整数的访问和操作更加便捷。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> immLut = (<span class="number">0xf0</span> &amp; <span class="number">0xcc</span>) | <span class="number">0xaa</span>;</span><br><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> BOTTOM_MASK = <span class="number">0x000f000f</span>;</span><br><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> TOP_MASK = <span class="number">0x00f000f0</span>;</span><br><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> I4s_TO_F16s_MAGIC_NUM = <span class="number">0x64006400</span>;</span><br></pre></td></tr></table></figure>
<p>这里定义了一些用于位操作的常量：</p>
<ul>
<li><p><code>immLut</code> 是一个查找表的立即数。</p></li>
<li><p><code>BOTTOM_MASK</code> 是用于提取较低位的掩码。</p></li>
<li><p><code>TOP_MASK</code> 是用于提取较高位的掩码。</p></li>
<li><p><code>I4s_TO_F16s_MAGIC_NUM</code>
是用于整数转换为半精度浮点数的魔数。</p></li>
</ul>
<p><img src="/images/101/4.jpeg" width="95%"></p>
<p>这几个数在内存中是这么表示的，后面用到的时候会仔细说，那时候会发现这几个数可真妙啊。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">uint32_t</span> top_i4s = i4s &gt;&gt; <span class="number">8</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/101/5.jpeg" width="95%"></p>
<p>这里是将 <code>i4s</code> 向右移位8位，以便处理 <code>elt_45</code>
和 <code>elt_67</code>。暂时先不要管为什么这么做。
到这里，预备工作都做好了，接下来开始重头戏。</p>
<h2 id="处理e0-e1元素">处理e0 e1元素</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Extract elt_01 - (i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br></pre></td></tr></table></figure>
<p>使用内联 PTX 指令 <code>lop3.b32</code> 执行逻辑操作，将
<code>i4s</code> 的低位（<code>BOTTOM_MASK</code> 掩码）与
<code>I4s_TO_F16s_MAGIC_NUM</code> 结合并存储在 <code>h[0]</code>
中。</p>
<p>首先看一下 <code>lop3.b32</code> 执行了什么操作，这个
<code>lop3.b32</code> 是一种CUDA汇编指令，用于执行
三元逻辑运算。它利用查找表（immLut）来简化逻辑操作。这个函数可以在<a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/">这里</a>查到.</p>
<pre><code>For a logical operation F(a, b, c) the value of immLut can be computed by applying the same operation to three predefined constant values as follows:

ta = 0xF0;
tb = 0xCC;
tc = 0xAA;

immLut = F(ta, tb, tc);</code></pre>
<p>比如： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">If F = (a &amp; b &amp; c);</span><br><span class="line">immLut = <span class="number">0xF0</span> &amp; <span class="number">0xCC</span> &amp; <span class="number">0xAA</span> = <span class="number">0x80</span></span><br><span class="line"></span><br><span class="line">If F = (a | b | c);</span><br><span class="line">immLut = <span class="number">0xF0</span> | <span class="number">0xCC</span> | <span class="number">0xAA</span> = <span class="number">0xFE</span></span><br><span class="line"></span><br><span class="line">If F = (a &amp; b &amp; ~c);</span><br><span class="line">immLut = <span class="number">0xF0</span> &amp; <span class="number">0xCC</span> &amp; (~<span class="number">0xAA</span>) = <span class="number">0x40</span></span><br><span class="line"></span><br><span class="line">If F = ((a &amp; b | c) ^ a);</span><br><span class="line">immLut = (<span class="number">0xF0</span> &amp; <span class="number">0xCC</span> | <span class="number">0xAA</span>) ^ <span class="number">0xF0</span> = <span class="number">0x1A</span></span><br></pre></td></tr></table></figure></p>
<p>现在我们可以看一下刚才定义的
<code>immLut = (0xf0 &amp; 0xcc) | 0xaa;</code> 那我们得到的操作就是
<code>(a &amp; b) | c</code>，也就是<code>( i4s &amp; BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM</code>.</p>
<p>这里先插一句，这个<code>immLut</code>设置的实在是太妙了！首先，abc这三个数可以形成一个
3 bit 的编码，可以表示 8 种状态，</p>
<p><img src="/images/101/6.jpeg" width="80%"></p>
<p>然后无论abc进行什么样的三元逻辑操作，得到的结果都是唯一的，这也就形成了immLut的表。实在是妙啊。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Extract elt_01 - (i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br></pre></td></tr></table></figure>
<p>然后结合具体数值看一下是怎么操作的：</p>
<p><img src="/images/101/7.jpeg" width="95%"></p>
<p>这一步操作是提取元素0和元素1的，这里设置元素0是<code>0000</code>，即
0，元素1是<code>0001</code>，即 1。</p>
<p><code>i4s</code>与<code>BOTTOM_MASK</code>进行位与操作后，实际上就是把<code>e0</code>和<code>e1</code>筛选了出来，和<code>I4s_TO_F16s_MAGIC_NUM</code>进行位或操作后，是给指数位进行了赋值。</p>
<p>现在看这个 <code>I4s_TO_F16s_MAGIC_NUM</code>
，<code>0x64006400</code>，只看这个指数位部分是
<code>11001</code>，对应数字 25，fp16指数位的偏移量为
<code>2**4−1=15</code>, 那么偏移后指数为 <code>25 - 15 = 10</code>。</p>
<p>以e1为例，</p>
<p>尾数位是<code>0.0000000001</code>，加上1，那就是<code>1.0000000001</code>。</p>
<p>指数位是10，所以结果是<code>1.00000001*2^10 = 10000000001</code>。也就是小数点向后移动10位。</p>
<p>这个结果的十进制是 1025。</p>
<p>这时候就可以理解这个 <code>I4s_TO_F16s_MAGIC_NUM</code>
为什么这么设置了——原来的数值类型是<code>int4</code>，肯定是整数，那么问题的关键就是要把尾数放大到一定程度，使新的数值是整数。那么，显而易见，要把小数点向右移动10位（因为尾数位有10位），也就反推出了<code>magic number</code>的具体数值，妙啊。</p>
<p>显然，此时的结果并不正确，因为加了1并且右移了10位，也就是加了1024，所以我们要把结果减去1024才可以，所以还需要进行这一步操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Convert elt_01</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;sub.f16x2 %0, %1, %2;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">0</span>]), <span class="string">&quot;r&quot;</span>(FP16_TOP_MAGIC_NUM))</span>;</span><br></pre></td></tr></table></figure>
<p>其中，<code>FP16_TOP_MAGIC_NUM = 0x64006400;</code></p>
<p>这句代码实现了减1024这个操作，于是最终得到fp16格式的<code>e1=1</code>，
<code>e0=0</code>，数据无误。</p>
<p>需要注意的是，<code>FP16_TOP_MAGIC_NUM = 0x64006400;</code>这个数代表的其实是fp16格式的
1024。</p>
<p>我最开始在这里感到疑惑，心想为什么要减这么一个数，这也不是1024啊，直到后来意识到现在是在做fp16的减法，那么当然也要是fp16的格式。</p>
<p>ok，到现在为止，最难的部分就理解完了。接下来看一下别的情况：</p>
<h2 id="处理e2-e3元素">处理e2 e3元素</h2>
<p><img src="/images/101/8.jpeg" width="95%"></p>
<p>对于e2、e3而言，这时候就不能用 <code>BOTTOM_MASK</code>
来进行位与操作了，应该改为 <code>TOP_MASK</code></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Extract elt_23 (i4s &amp; 0x00f000f0) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(TOP_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/101/9.jpeg" width="95%"></p>
<p>如果减去1024，呢么得到<code>e3=48</code>，<code>e2=32</code>，这和刚才不太一样，事情变得不太简单。</p>
<p>问题在哪？以e3为例，这时候小数点右移10位意味着把e3乘了<code>2^4=16</code>，所以当前的结果是<code>1024 + X * 16</code>。</p>
<p>所以针对e2、e3的下一步操作稍有不同：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Convert elt_23</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;fma.rn.f16x2 %0, %1, %2, %3;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">1</span>]), <span class="string">&quot;r&quot;</span>(ONE_SIXTEENTH), <span class="string">&quot;r&quot;</span>(NEG_64))</span>;</span><br></pre></td></tr></table></figure>
<p>其中，<code>ONE_SIXTEENTH = 0x2c002c00; NEG_64 = 0xd400d400;</code></p>
<p>这是做了什么操作？可以理解为</p>
<p><code>(1024 + X * 16) * (1/16) - 64 = X</code></p>
<p>ok，到现在为止，e0e1e2e3都搞定了，最后看一下e4e5，</p>
<h2 id="处理e4-e5元素">处理e4 e5元素</h2>
<p><img src="/images/101/10.jpeg" width="95%"></p>
<p>e4 e5元素怎么提取呢？可以先认真思考一下。</p>
<p>我最开始的想法是，针对e4
e5的位置，再写一个mask进行位与操作不就好了，然后和前面做一样的操作。</p>
<p>但是这样是不可行的，以e5为例，这时候e5在16位的高8位，与fp16的指数位有冲突。</p>
<p>那么，处理方式就很简单了，文章最开始的时候提到过这个操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">uint32_t</span> top_i4s = i4s &gt;&gt; <span class="number">8</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/101/11.jpeg" width="95%"></p>
<p>这里是将 i4s 向右移位8位，以便处理 <code>elt_45</code> 和
<code>elt_67</code>。</p>
<p>这样一来，就可以按照e0 e1的方式处理e4 e5了。</p>
<p>ok，到现在为止，e0 e1 e2 e3 e4 e5都搞定了，有了前面的基础，e6
e7理解起来就很简单，这里不做过多赘述。</p>
<p>整个代码实现到这里也就讲完了。</p>
<hr />
<h2 id="写在最后">写在最后</h2>
<p>最后，补充几点</p>
<ul>
<li><p>对于e2e3而言，为什么不像e5e6那样，向右移动8位，然后做e0e1相同的操作？</p>
<p>这样的操作从逻辑上确实更好理解，也避免了后面的乘1/16的操作，看上去似乎更简单。</p>
<p>但是要明白一点，e5e6那样，向右移动8位，是不得不这么做。</p>
<p>为了性能，e2e3没必要额外增加一个移位操作。所以关键点只剩一个：<code>fma.rn.f16x2</code>指令和<code>sub.f16x2</code>指令的区别，如果
移位+sub指令 快于
<code>fma.rn</code>指令，那就是赚的。代码中的注释标明了这一点：</p>
<pre><code>  In addition, I exploit the fact that sub and fma have the same throughput in order to convert elt_23 and elt_67 to fp16 without having to shift them to the bottom bits before hand.</code></pre>
<p>sub（减法）和
fma（乘法累加）具有相同的吞吐量，这意味着它们的性能相当。</p></li>
<li><p>为什么要做<code>interleave</code>？</p>
<p>本文缘起就是因为研究这个权重排布</p>
<p><img src="/images/101/12.jpeg" width="60%"></p>
<p>前文中一直没有提这件事——为什么权重是<code>interleave</code>排布？</p>
<p><img src="/images/101/13.png" width="95%"></p>
<p>按照zzk大佬的观点，对于<code>Int8</code>来说，本来就是不需要<code>Interleave</code>的，但是<code>Int4</code>快速转换不能使用PRMT指令，需要通过一些trick，进而需要这个交错权重的格式</p>
<p>也就是说，<code>int8</code>快速反量化中，使用了<code>PRMT</code>指令，这个指令的操作粒度是字节<code>byte</code>；而int4是<code>sub-byte</code>，不足一个字节；因此，原来在<code>INT8</code>快速反量化的那一套无法直接用在<code>int4</code>上。</p>
<p>所以针对<code>int4</code>，使用指令<code>LOP3</code>来替代<code>PRMT</code>，从而完成了<code>int4</code>快速反量化到<code>fp16</code>的核心逻辑。通过交错权重，可以减少<code>shift</code>指令，提高性能。关于这一点，我认为<a
href="https://zhuanlan.zhihu.com/p/657073857?utm_medium=social&amp;utm_oi=56635854684160&amp;utm_psn=1692002696943525888&amp;utm_source=wechat_session&amp;s_r=0">[LLM推理优化]🔥WINT8/4-(03):
LOP3指令详解及INT4转FP16/BF16分析</a> 讲解的比较详细。</p></li>
</ul>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>AWQ</tag>
        <tag>dequantize</tag>
      </tags>
  </entry>
  <entry>
    <title>flatc</title>
    <url>/2023/06/28/71_flatc/</url>
    <content><![CDATA[<p>记录一下flatbuffer的学习</p>
<span id="more"></span>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/google/flatbuffers.git</span><br><span class="line"></span><br><span class="line">cmake -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=Release</span><br><span class="line"></span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<p><a
href="https://stackoverflow.com/questions/24275803/how-to-build-flatbuffers-on-osx">reference</a></p>
<p>./flatc -t schema.fbs -- test.tflite</p>
<p>./flatc -t ~/Projects/tensorflow/tensorflow/lite/schema/schema.fbs --
~/Downloads/encoder.tflite</p>
<p>参考资料：</p>
<p><a
href="https://www.jianshu.com/p/523968b82e74?tdsourcetag=s_pcqq_aiomsg">iOS中FlatBuffers的使用</a></p>
]]></content>
  </entry>
  <entry>
    <title>万物皆可RSS</title>
    <url>/2023/06/29/67_RSS/</url>
    <content><![CDATA[<p>万物皆可RSS！</p>
<span id="more"></span>
<p><a href="https://sspai.com/post/60781">指路文章</a></p>
<p><a
href="https://www.bilibili.com/video/BV1oU4y1s7o8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=a706cf2a9a33f4c298a6ef4764cea661">入坑视频教程</a></p>
<p><a
href="https://www.runningcheese.com/rss-subscriptions">入坑文章指南</a></p>
<blockquote>
<p>RSS，英文全称Really Simple
Syndication，中文译作简易信息聚合（也叫聚合内容），是一种基于XML标准，在互联网上被广泛采用的内容包装盒投递协议。它能够描述和同步网站内容，而RSS订阅这种形式则可以更快地获取最新信息。</p>
</blockquote>
<ul>
<li><p>碎片化阅读</p>
<blockquote>
<p>碎片化阅读是指一种通过手机、电子书、计算机等进行的不完整的、断断续续的阅读模式。</p>
</blockquote></li>
<li><p>信息茧房 &gt;
信息茧房概念是由哈佛大学法学院教授、奥巴马总统的法律顾问凯斯·桑斯坦在其2006年出版的著作《信息乌托邦——众人如何生产知识》中提出的。通过对互联网的考察，桑斯坦指出，在信息传播中，因公众自身的信息需求并非全方位的，公众只注意自己选择的东西和使自己愉悦的通讯领域，久而久之，会将自身桎梏于像蚕茧一般的“茧房”中。</p></li>
</ul>
<p>众所周知，我们身处一个信息爆炸的网络大数据时代，再加上数据埋点、人群画像等精准营销手段，往往会产生这样的一个结果：<strong>我们经常会被迫接收到来自服务商提供的，经由大数据算法检验认为的“所谓匹配我们”的信息。</strong></p>
<p>而这些“精准匹配的个性化”信息会不断缩小我们的认知范围，久而久之，渐渐我们会失去对信息的敏感，处于一种“<strong>别人推荐什么，你就看什么</strong>”的状态中。这是很可怕的！！</p>
<p>我们每个人都是独立的个体，应当是“自由”的。这份自由中自然包含着“思想自由”：个人拥有或思考独立于他人之外的观点、事实、思想之自由。</p>
<p>--摘自<a href="https://sspai.com/post/60781">指路文章</a></p>
<h1 id="rsshub-feedly-reeder搭配">RSSHub + Feedly + Reeder搭配</h1>
<p>RSSHub是一个开源、简单易用、易于扩展的 RSS
生成器，可以给任何奇奇怪怪的内容生成 RSS
订阅源。目前借助于开源社区的力量快速发展中，已经适配数百家网站的上千项内容。</p>
<ul>
<li>RSSHub可以为几乎所有网站提供RSS订阅源，比如知乎、虎扑、博客、微信公众号、B站、起点等媒体站点。甚至即使该站点不提供RSS订阅服务，我们也可以基于它来制作一个RSS订阅源，可谓非常强大</li>
</ul>
<p>Feedly是一个RSS聚合器应用程序，支持各种网页浏览器和运行iOS或Android的移动设备，也是一个基于云端的服务。其从各种在线资源聚合用户自定的新闻订阅源，并可与他人分享。</p>
<ul>
<li>选择Feedly则是因为其提供了文件夹分类功能，且也拥有数量不菲的订阅源，在对RSS订阅源的管理上十分方便，同时恰好也在Reeder支持的第三方订阅服务列表中。</li>
</ul>
<p>Reeder是一款评价极高的iOS端RSS阅读应用。其支持与Feedbin、Feedly、Inoreader等RSS第三方服务云端同步，甚至也支持与FreshRSS、Reader、Fever等自建RSS服务同步数据。</p>
<ul>
<li>使用Reeder则仅仅是因为它是iOS上广受好评的RSS阅读端APP，而我购买后体验的效果也很满意。</li>
</ul>
<p>--摘自<a href="https://sspai.com/post/60781">指路文章</a></p>
<p>我绕了一大圈，看各种资料，最终的选择也恰巧是如此。</p>
<h1 id="我的一些操作">我的一些操作：</h1>
<h2 id="mac安装reeder4">1、mac安装Reeder4</h2>
<p>程序体积比我想象的要小</p>
<p>新建了个账户，icloud的feeds。</p>
<p>订阅了一下自己的博客，阅读体验还可以。</p>
<h2 id="iphone安装reeder5">2、iphone安装Reeder5</h2>
<p>重新注册了个美区账号，<a
href="https://mp.weixin.qq.com/s/s9fMsBzVu4nXBbBuYym9mA">步骤</a></p>
<p>使用礼品卡的方式下载了Reeder5</p>
<p>新建了个账户，icloud的feeds，和电脑的订阅同步上了～</p>
<h2 id="浏览器安装rss插件">3、浏览器安装<a
href="https://greasyfork.org/zh-CN/scripts/373252-rss-show-site-all-rss">RSS+插件</a></h2>
<p>安装后，在网页的右下角会有一个“蓝色圆点”，显示当前网页有多少个可用
RSS 源，如果没有，则不会显示。</p>
<p>有了它，我们就再也不需要到处找 RSS 订阅源。</p>
<p>只需要和平时上网一样，遇到喜欢的 RSS 源，一键添加即可。</p>
<h2 id="注册feedly">4、注册feedly</h2>
<p>https://feedly.com/</p>
<p>但是，<a href="https://www.v2ex.com/t/797931">Reeder 4 无法添加
feedly</a></p>
<p>决定买Reeder5，既然拥有国外信用卡是早晚的事，那就早点做吧。</p>
<h2 id="注册币安">5、注册币安</h2>
<p><a href="https://www.binance.com/zh-CN">币安</a></p>
<p><a
href="https://www.youtube.com/watch?v=sWm0RgSapp8">币安使用方式</a></p>
<p>欧易似乎更好用，但我还是先入为主了。</p>
<h2 id="注册onekey-visa卡">6、注册onekey visa卡</h2>
<p>在<a
href="https://card.onekey.so/?i=S1RXT8">这里</a>注册美国虚拟信用卡</p>
<p><a href="https://www.youtube.com/watch?v=nIfFC7hFZ90">教程</a></p>
<p>我是用币安向onekey提现USDT实现激活的，期间扣除了1USDT的手续费。以及onekey有1$的开卡费和一点手续费。</p>
<h2 id="添加feedly账户">7、添加feedly账户</h2>
<p>下载Reeder5之后，就可以绑定Feedly账户了。</p>
<hr />
<p>目前我是<code>RSS+插件</code>的重度使用者</p>
<hr />
<h1 id="rss真香"><strong>RSS真香！</strong></h1>
]]></content>
      <tags>
        <tag>RSS</tag>
      </tags>
  </entry>
  <entry>
    <title>git</title>
    <url>/2023/06/19/73_git/</url>
    <content><![CDATA[<p>日常的一些git操作其实已经非常熟悉了，比较欠缺的是团队协作遇到的一些问题。</p>
<p>在此记录一下需要经常查阅的情景。</p>
<span id="more"></span>
<h1
id="git将当前修改的内容提交到新的分支上">Git：将当前修改的内容提交到新的分支上</h1>
<h2 id="前言">前言：</h2>
<p>有时候在参加一个项目时，执行clone得到master分支，
一开始只是想看看源码或者忘记了自己没有新建分支，结果后面自己根据需求添加了代码<code>添加后没有执行commit</code>,
但是此时的修改都在master分支，
提交必然是不可以的，还是要新建分支<code>所有修改都要在新建分支上进行</code>，最后在分支执行通过后，才能合并到master分支。
那么，这时候如何力挽狂澜，如何在保存这些修改的前提下，新建分支并提交呢？</p>
<h2 id="操作思路">操作思路：</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//步骤1：在当前的master分支上的修改暂存起来</span></span><br><span class="line">git stash</span><br><span class="line"><span class="comment">//步骤2：暂存修改后，在本地新建分支（new_branch为新分支的名字）</span></span><br><span class="line">git checkout -b new_branch</span><br><span class="line"><span class="comment">//步骤3：将暂存的修改放到新建分支中</span></span><br><span class="line">git stash pop</span><br><span class="line"><span class="comment">//步骤4：使用TortoiseGit进行commit，比如add、modify、delete...</span></span><br><span class="line"><span class="comment">//步骤5：将提交的内容push到远程服务器</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure>
<p>————————————————</p>
<p>版权声明：本文为CSDN博主「Oruizn」的原创文章，遵循CC 4.0
BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Oruizn/article/details/111294375</p>
<h1
id="git创建本地分支并关联远程分支">git创建本地分支并关联远程分支</h1>
<h2 id="创建本地分支">1. 创建本地分支</h2>
<p><code>git branch 分支名</code></p>
<p>例如：<code>git branch dev</code>，这条命令是基于当前分支创建的本地分支，假设当前分支是master(远程分支)，则是基于master分支创建的本地分支dev。</p>
<h2 id="切换到本地分支">2. 切换到本地分支</h2>
<p><code>git checkout 分支名</code></p>
<p>例如：<code>git checkout dev</code>，这条命令表示从当前master分支切换到dev分支。
## 3. 创建本地分支并切换</p>
<p><code>git checkout -b 分支名</code></p>
<p>例如：<code>git checkout -b dev</code>，这条命令把创建本地分支和切换到该分支的功能结合起来了，即基于当前分支master创建本地分支dev并切换到该分支下。</p>
<h2 id="提交本地分支到远程仓库">4. 提交本地分支到远程仓库</h2>
<p><code>git push origin 本地分支名</code></p>
<p>例如：<code>git push origin dev</code>，这条命令表示把本地dev分支提交到远程仓库，即创建了远程分支dev。</p>
<blockquote>
<p>注：要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你创建的本地分支不会因为你的写入操作而被自动同步到你引入的远程服务器上，你需要明确地执行推送分支的操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。</p>
</blockquote>
<h2 id="新建本地分支与远程分支关联">5.新建本地分支与远程分支关联</h2>
<p><code>git branch –set-upstream 本地新建分支名 origin/远程分支名</code></p>
<p>或者</p>
<p><code>git branch –set-upstream-to=origin/远程分支名</code></p>
<p>例如：<code>git branch –set-upstream-to=origin/dev</code>，把本地dev分支和远程dev分支相关联。</p>
<blockquote>
<p>注：本地新建分支，
push到远程服务器上之后，使用<code>git pull</code>或者<code>git pull</code>
拉取或提交数据时会报错，必须使用命令：<code>git pull origin dev</code>（指定远程分支）；如果想直接使用<code>git pull</code>或<code>git push</code>拉去提交数据就必须创建本地分支与远程分支的关联。</p>
</blockquote>
<p>————————————————</p>
<p>版权声明：本文为CSDN博主「EntyIU」的原创文章，遵循CC 4.0
BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_59244784/article/details/129170582</p>
<hr />
<p>推git遇到了一个错误：</p>
<p><code>remote: fatal: pack exceeds maximum allowed size</code>。</p>
<p>估计以后也会再次遇到，记录一下解决步骤。</p>
<!-- more -->
<h2 id="解决方案">解决方案</h2>
<p>第一反应当然是google一下，顺利找到<a
href="https://stackoverflow.com/questions/67086183/git-push-remote-fatal-pack-exceeds-maximum-allowed-size">解决方案</a></p>
<p>First, you can use <code>git-sizer</code> to get an idea of what is
taking too much space in your current local repository (that you fail to
push)</p>
<ul>
<li><p>if it is because of a commit too big, you can:</p>
<ul>
<li><code>git reset @~</code> to cancel that commit</li>
<li>remake several smaller commits</li>
<li>try and push again</li>
</ul></li>
<li><p>if it is because of a file too big, you can try and activate
<code>Git LFS</code>, but that is limited by quota and going above might
include a non-free service.</p></li>
</ul>
<p>More generally, a "large project" might need to be split up into
several Git repositories.</p>
<h2 id="实践操作">实践操作</h2>
<p>1、安装git-sizer</p>
<p>https://formulae.brew.sh/formula/git-sizer</p>
<p><code>brew install git-sizer</code></p>
<p>2、分析文件大小</p>
<p>在git项目内执行<code>git-sizer</code></p>
<pre><code>Processing blobs: 6673                        
Processing trees: 1053                        
Processing commits: 2                        
Matching commits to trees: 2                        
Processing annotated tags: 0                        
Processing references: 3                        
| Name                         | Value     | Level of concern               |
| ---------------------------- | --------- | ------------------------------ |
| Biggest objects              |           |                                |
| * Blobs                      |           |                                |
|   * Maximum size         [1] |  60.9 MiB | ******                         |
|                              |           |                                |
| Biggest checkouts            |           |                                |
| * Maximum path depth     [2] |    10     | *                              |
| * Maximum path length    [2] |   146 B   | *                              |
| * Total size of files    [2] |  1.40 GiB | *                              |

[1]  ***********(delete)*********** (refs/heads/main:toolchain/***********(delete)***********)
[2]  ***********(delete)*********** (refs/heads/main^&#123;tree&#125;)</code></pre>
<p>好像也没有太大的文件，最大的也就是64MB。那应该是整个commit太大了。</p>
<p>于是我拆开多个分别push，解决了问题。</p>
<h2 id="lfs">LFS</h2>
<p>下次遇到不得不使用LFS的时候再来补充后续的博客</p>
<p><a href="https://www.jianshu.com/p/493b81544f80">Git
LFS的使用</a></p>
<p>brew install git-lfs</p>
<p>git lfs install</p>
]]></content>
      <tags>
        <tag>git</tag>
        <tag>branch</tag>
      </tags>
  </entry>
  <entry>
    <title>Final Cut study notes</title>
    <url>/2023/07/08/75_Final-cut-study-notes/</url>
    <content><![CDATA[<p>学习Final Cut Pro的笔记，提高生产力。</p>
<span id="more"></span>
<h1 id="配置">配置</h1>
<hr />
<h1 id="插件">插件</h1>
<h2 id="motion-vfx"><a href="https://www.motionvfx.com/">MOTION
VFX</a></h2>
<p>一个插件集合的网站，巨好用！</p>
<h3 id="mcamrig摄像机运动"><a
href="https://www.motionvfx.com/store,mcamrig,p2219.html?category=63&amp;irgwc=1&amp;irgwc=1">mCamRig</a>摄像机运动</h3>
<ul>
<li>对出教程的UP主特别有用</li>
<li>可以快速制作缩放、旋转，添加镜头模糊、镜头运动效果</li>
<li>用法：
<ul>
<li>字幕里找到插件，拉到对应的片段上</li>
</ul></li>
</ul>
<h3 id="mvertical手机视频背景虚化"><a
href="https://www.motionvfx.com/store,mvertical,p2176.html?irgwc=1">mVertical</a>手机视频背景虚化</h3>
<ul>
<li>横版视频插入竖版视频时，填充两边的黑色</li>
<li>实现背景重叠的背景虚化</li>
<li>小技巧
<ul>
<li>选择片段，按一下X，就自动选择这个片段长度</li>
<li>在字幕里找到mVertical，按Q，就可以整段附着到这个片段</li>
</ul></li>
</ul>
<h2 id="commandpost最强辅助"><a
href="https://commandpost.io">Commandpost</a>最强辅助</h2>
<h1 id="清理缓存">清理缓存</h1>
<p>资源库 右键 显示包内容</p>
<h1 id="高效技巧">高效技巧</h1>
<p>添加视频时，进行了部分选择，标记 -&gt; 清除所选范围（快捷键：Option +
X 取消选择）</p>
<p>添加字幕：按enter会换行，按commond+enter退出编辑状态</p>
<p>把当前片段拉长（某一帧拉长） - 编辑 -&gt; 添加静帧 - 快捷键：shift +
H，片段是完整的没有被切割</p>
<p>固定音乐位置 - 第一帧最前面加一个空隙 Edit -&gt; Insert Generator
-&gt; Gap(option+W) - 选择Gap，control+D，输入1（1帧时长） -
把音乐放最前面 - 删除的时候，Option + commond + Delete</p>
<p>调整变速，不影响整体时长 - 右键， Lift from storyline - Overwrite to
Primary Storyline</p>
<p>添加的时候只要要画面，不要声音</p>
<p>Final cut pro 1. 资源管理 - 关系： 资源库 -&gt; 事件 -&gt; 项目 -
导入素材 - 按住 command， 手动点击素材，批量选择</p>
<p>2.添加 素材到时间线，基本操作 - 1. 片段选择 - 选择入点： I - 选择出点
： O - 添加到时间线： E - 取消选择 - Option + X - 2. 预览素材 -
顺序播放： L - 加速： 连续按2,3次L - 暂停 ：K - 倒序播放 ：J -
3.添加字幕 - 添加到选定时间点： Q - 4. 在2个素材之间快速插入： W - 5.
进阶学习 - 个人收藏 - 关键词 - 3.剪辑 - 1. 切割工具 - B - 2. 延长 缩短
选择的视频 - 逗号 和 句号 - 3. 切割并选中 - command + B - 4.
范围选择工具 - 按住R不松 - 5. 删除 播放头开始 到 选中位置 - Option + [ -
6. 删除 选中位置 到 素材结束位置 - Option + ] - 7. 删除 主时间线视频，
同时不删除连接的视频 ， 且保留 B-roll - Option + command + delete - 8.
速度条<br />
- command + R - 9. 速度条 分段 - shift + B - 10. 完整 显示时间线 - shift
+ Z - 11. 吸附 开关 - N - 12. 打上标记 - M - 13. 预览 开关 - S - 14.
音频 - 音量调节 - Control + or Control - - 15. 字幕 - 添加字幕 到 浏览头
右边 - Q - 添加字幕 到 浏览头 左边 - shift + Q - 直接添加 基本字幕 -
control + T - 16. 转场 - 添加默认转场 - command + T - 17. 添加 静止帧 -
Shift + H - 18. 添加 缝隙 - Option + W - -&gt; Control + D （设置帧数）
- 19. 从故事情节中 提取 - 不影响 时间线 option + command + 向上</p>
]]></content>
      <tags>
        <tag>finalcut</tag>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>Roofline model学习</title>
    <url>/2023/06/05/77_Roofline/</url>
    <content><![CDATA[<blockquote>
<p>给定特定的计算平台，我们的算法实现是不是已经榨干硬件平台的最大能力，还有没有进一步改善的空间？这些问题寻寻觅觅答案，真像雾里看花，我们渴望有一双慧眼，帮我们穿透迷津。</p>
</blockquote>
<p>学习Roofline，有一种理论指导实践的感觉。</p>
<span id="more"></span>
<p>阅读论文：<a
href="https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf">Roofline:
An Insightful Visual Performance Model for Floating-Point Programs and
Multicore Architectures</a></p>
<p>博客：<a
href="https://blog.csdn.net/21cnbao/article/details/116213606?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168593229216800222880877%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=168593229216800222880877&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-116213606-null-null.142%5Ev88%5Einsert_down1,239%5Ev2%5Einsert_chatgpt&amp;utm_term=Roofline%3A%20An%20Insightful%20Visual%20Performance%20Model&amp;spm=1018.2226.3001.4187">Roofline模型初步</a></p>
<p>知乎：<a href="https://zhuanlan.zhihu.com/p/34204282">Roofline
Model与深度学习模型的性能分析</a></p>
<p>我的总结：</p>
<p>【更新中】</p>
]]></content>
      <tags>
        <tag>study</tag>
        <tag>Roofline</tag>
      </tags>
  </entry>
  <entry>
    <title>gdb</title>
    <url>/2023/06/19/79_gdb/</url>
    <content><![CDATA[<p>今天遇到段错误，旁观大佬一番操作定位解决。</p>
<p>大大提高开发效率的关键几步，值得记录一下。</p>
<span id="more"></span>
<p><code>gdb --args 运行命令</code></p>
<p><code>r</code></p>
<p><code>bt</code></p>
<p><code>p</code></p>
<h2 id="gdb调试传参args">gdb调试传参args</h2>
<p>假设我们的可执行程序是worker,gdb时需要传递参数arg1和arg2，有3种方式：</p>
<ul>
<li><p>方式1</p>
<p><code>gdb --args worker arg1 arg2</code></p></li>
<li><p>方式2: 进入gdb界面，然后设置参数</p>
<p><code>gdb worker</code></p>
<p><code>set args arg1 arg2</code></p>
<p><code>show args</code></p></li>
<li><p>方式3: 在run指令后面直接跟参数</p>
<p><code>gdb worker</code></p>
<p><code>r arg1 arg2</code></p></li>
</ul>
<h2 id="命令">命令</h2>
<p>run/<code>r</code> 运行</p>
<p>continue/c 继续运行</p>
<p>next/n 单步运行</p>
<p>quit/ctrl+d 退出GDB</p>
<p>print/<code>p</code> var 打印变量的值</p>
<p>print/p &amp;var 打印变量地址</p>
<p>printf/p *addr 打印地址的值</p>
<p><code>bt</code> 查看函数back trace</p>
]]></content>
      <tags>
        <tag>study</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title>vtype_register</title>
    <url>/2023/06/14/81_vtype-register/</url>
    <content><![CDATA[<p>最近搞了搞risc-v v，vtype寄存器可谓是灵魂的存在了。</p>
<p>在此记录一下关于vtype寄存器相关的知识和结论。</p>
<span id="more"></span>
<h2 id="background">Background</h2>
<blockquote>
<p>With a vector length specific (VLS) SIMD instruction set the main
problem is to pick the right vector register size. Of course there is a
trade-off between the amount of data-level parallelism and hardware
costs. Due to Moore's law, vector register sizes can be increased over
time without making the CPU chip more expensive. Also, some users are
interested in powerful CPUs with wider vector registers while the
average user is fine with averagely sized register. Thus, there is no
one right vector register size.</p>
</blockquote>
<p>对于向量长度特定 (VLS) SIMD
指令集，主要问题是选择正确的向量寄存器大小。
当然，在数据级并行性和硬件成本之间存在权衡。
由于<code>摩尔定律</code>，<code>矢量寄存器的大小</code>可以随着时间的推移而增加，而不会增加
CPU 芯片的成本。 此外，一些用户对具有<code>更宽</code>向量寄存器的强大
CPU 感兴趣，而普通用户则对<code>平均大小</code>的寄存器感兴趣。
因此，<code>没有一个正确的向量寄存器大小</code>。</p>
<blockquote>
<p>The solution to all this is to design a variable length vector
instruction set. In that way the instructions are then agnostic to the
vector register size of a concrete CPU implementation. Thus, the binary
code is portable between low, middle and high-end CPUs, and
automatically makes use of wider registers in newer CPUs.</p>
</blockquote>
<p>解决方案是<code>设计一个可变长度的向量指令集</code>。
这样，指令就与具体 CPU 实现的向量寄存器大小无关。
因此，二进制代码可在低端、中端和高端 CPU 之间移植，并自动在较新的 CPU
中使用更宽的寄存器。</p>
<p>个人感觉，RISC-V这个后起之秀就胜在不用顾及历史积累，可以很轻易的从头开始。</p>
<h2 id="design-challenges-opcode-space">Design Challenges: Opcode
Space</h2>
<p>Community wanted to stay with 32-bit instruction encoding</p>
<ul>
<li>Low-end embedded systems have 32-bit instruction fetch</li>
<li>Harder to support mixed-length instruction streams, 16,32,48, &amp;
64 bits</li>
<li>Static code size matters on embedded platforms</li>
</ul>
<p>But wanted vast array of datatypes and custom datatypes, and large
set of operations</p>
<h2 id="opcode-solution">Opcode Solution</h2>
<p>There are two registers used when operating vectors in RVV</p>
<h3 id="寄存器vtype-vector-type.">寄存器<strong>vtype: Vector
Type.</strong></h3>
<p>Added a control register to hold some information about current
setting of vector unit</p>
<p>vtype describes
<code>the type of vector we are going to operate</code> and includes</p>
<p>vtype fields (total additional 6-7 state bits) - vsew: standard
element width (SEW=8,16,32,…,1024) - Size in bits of the elements being
operated - <code>8 ≤ sew ≤ ELEN</code> - vlmul: vector length multiplier
(LMUL=1,2,4,8) - <span class="math inline">\(lmul = 2^k\)</span> where
-3 ≤ k ≤ 3 (i.e., lmul ∈ {1/8, 1/4, 1/2, 1, 2, 4, 8}) - 寄存器分组
Groups registers to form “longer vector” - Number of registers in each
group is LMUL - 举例：当 LMUL=2时，<code>vadd v2, v4, v6</code> 意味着
<code>(v2,v3) := (v4,v5) + (v6,v7)</code> - vediv: vector element
divider (EDIV=1,2,4,8)</p>
<p>Encoding only occupies 1.5 major opcodes</p>
<p>Full 64-bit instruction encoding also planned - Can view current
32-bit encoding as compressed form of full encoding</p>
<h3
id="寄存器vl-vector-length-not-to-be-confused-with-vlen">寄存器<strong>vl:
Vector Length</strong> (not to be confused with VLEN!)</h3>
<p>vl describes <code>how many elements of the vector</code> (starting
from the element zero) <code>we are going to operate</code></p>
<ul>
<li><p>$ 0 ≤ vl ≤ vlmax(sew, lmul) $</p></li>
<li><p>$ vlmax(sew, lmul) = (VLEN / sew) × lmul $</p></li>
</ul>
<p>Vector length <span class="math inline">\(vl\)</span> set to min(AVL,
VLMAX)</p>
<h2 id="rvv-defines-32-vector-registers-of-size-vlen-bits">RVV defines
32 vector registers of size VLEN bits</h2>
<ul>
<li>v0 to v31</li>
<li>VLEN is a <code>constant parameter</code> chosen by the implementor
and must be a power of two
<ul>
<li>Zv* standard extensions constraint VLEN to be at least 64 or
128</li>
</ul></li>
<li>E.g., VLEN=512 would be equivalent in size to Intel AVX-512</li>
<li>VLEN is not a great name so read it as
“<strong><code>vector register size (in bits)</code></strong>”</li>
</ul>
<h2 id="vectors-in-rvv-are-divided-in-elements.">Vectors in RVV are
divided in elements.</h2>
<ul>
<li>The size of elements in bits is at least 8 bits up to ELEN bits</li>
<li>ELEN is a constant parameter chosen by the implementor</li>
<li>Must be a power of two and 8 ≤ ELEN ≤ VLEN
<ul>
<li>Zv* standard extensions constrain ELEN to be at least 32 or 64</li>
</ul></li>
</ul>
<h2 id="vector-length-control">Vector Length control</h2>
<p>Current maximum vector length is register length in bits divided by
current element width setting:</p>
<p><span class="math display">\[VLMAX = VLEN/SEW\]</span></p>
<ul>
<li>E.g., <code>VLEN = 512b, SEW=32b</code> =&gt;
<code>VLMAX = 16</code></li>
</ul>
<p>Current active vector length set by vl register</p>
<p><span class="math display">\[0&lt;=vl&lt;=VLMAX\]</span></p>
<h2 id="vsetvlivsetvl-instructions">vsetvli/vsetvl instructions</h2>
<p><span class="math inline">\(vsetvli\)</span> instruction sets both
vector configuration and vector length:</p>
<p><code>vsetvli rd，rs1，vtypei(e8/e16/e32/e64)，multi-num(m1-m8)</code></p>
<p>rd - Returns setting of vector length in scalar register -
返回值，告诉硬件接下去运行函数所需要的操作元素个数的量</p>
<p>rs1 - Application vector length (AVL) - 设置操作元素</p>
<p>vtypei - 每个元素的位宽大小</p>
<p>multi-num - 寄存器的连用数量</p>
<hr />
<p>在 rv64 的编写中，如果 rd
的返回值不容，会导致硬件部分中心进行设置，此步骤极大的占用运行时间，所以<code>在编写的时候需要尽量可能的减少 rd 数值的变换</code>。</p>
<p>rv64 中每个 vector 寄存器总位宽是大于等于 128-bit,
以128为例子，对于如何对 rd
进行数值的变换，公式方面有一个很简单的换算：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rd == rs1 &lt;= (vtypei * multi-num ) ? rs1 : (vtypei * multi-num )</span><br><span class="line">举个例子：</span><br><span class="line"></span><br><span class="line">vsetvli rd, rs1 == 8， vtypei == e8， multi-num == m1</span><br><span class="line">返回值便是 rd == 8 (128 / e8 == 16 &gt;= rs1 =&gt; 8)</span><br><span class="line"></span><br><span class="line">vsetvli rd, rs1 == 32，vtypei == e16， multi-num == m1</span><br><span class="line">返回值便是 rd == 16 (128 / e16 = 16)</span><br></pre></td></tr></table></figure>
<p>如果一段代码中同时存在上述两条指令，那么便会出现硬件重设置情况，对代码整体的性能会产生负面影响。</p>
<hr />
<h2 id="simple-memcpy-example">Simple memcpy example</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># void*memcpy(void*dest,const void*src, size_t n)</span><br><span class="line"># a0=dest, a1=src, a2=n</span><br><span class="line">memcpy:</span><br><span class="line">mv a3, a0 # Copy destination</span><br><span class="line">loop:</span><br><span class="line">vsetvli t0, a2, e8 # Vectors of 8b</span><br><span class="line">vlb.v v0, (a1) # Load bytes</span><br><span class="line">add a1, a1, t0 # Bump pointer</span><br><span class="line">sub a2, a2, t0 # Decrement count</span><br><span class="line">vsb.v v0, (a3) # Store bytes</span><br><span class="line">add a3, a3, t0 # Bump pointer</span><br><span class="line">bnez a2, loop # Any more?</span><br><span class="line">ret # Return</span><br></pre></td></tr></table></figure>
<h2 id="usability-code">usability code</h2>
<p>通过intrinsic的方式，查询<code>vlenb</code>，进而可以兼容不同位宽的设备。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">csrr_vl</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;csrr %0, vl&quot;</span></span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;=r&quot;</span>(a)</span></span><br><span class="line"><span class="params">                 :</span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">csrr_vtype</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;csrr %0, vtype&quot;</span></span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;=r&quot;</span>(a)</span></span><br><span class="line"><span class="params">                 :</span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">csrr_vlenb</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;csrr %0, vlenb&quot;</span></span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;=r&quot;</span>(a)</span></span><br><span class="line"><span class="params">                 :</span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整理的很乱，暂时没时间仔细梳理了。</p>
<h2 id="参考文献">参考文献</h2>
<p><a href="https://gms.tf/riscv-vector.html">Programming with RISC-V
Vector Instructions</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/363901274">RISC-V
小白优化学习</a></p>
<p><a
href="https://riscv.org/wp-content/uploads/2019/06/17.40-Vector_RISCV-20190611-Vectors.pdf">Vector
Extension Update</a></p>
<p><a
href="https://llvm.org/devmtg/2019-04/slides/TechTalk-Kruppe-Espasa-RISC-V_Vectors_and_LLVM.pdf">Adventures
with RISC-V Vectors and LLVM</a></p>
]]></content>
      <tags>
        <tag>risc-v</tag>
        <tag>tech</tag>
      </tags>
  </entry>
  <entry>
    <title>FasterTransformer</title>
    <url>/2023/07/06/83_FasterTransformer/</url>
    <content><![CDATA[<p>记录一下部署大模型的经历</p>
<p>【更新中】</p>
<span id="more"></span>
<p><code>git clone https://github.com/NVIDIA/FasterTransformer.git</code></p>
<p>查看<code>docs/bert_guide.md</code></p>
<p>https://github.com/NVIDIA/nvidia-docker</p>
<p><a href="https://fanjunyu.com/posts/bb9b6d25/">NVIDIA Docker
的安装和使用</a> 好文！</p>
<p>期间需要注意这个问题，于是我最后一句改成了<code>docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi</code></p>
<pre><code>遇到问题
`permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get &quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version&quot;: dial unix /var/run/docker.sock: connect: permission denied`

解决方式：

docker守护进程启动的时候，会默认赋予名字为docker的用户组读写Unix socket的权限，因此只要创建docker用户组，并将当前用户加入到docker用户组中，那么当前用户就有权限访问Unix socket了，进而也就可以执行docker相关命令</code></pre>
<p>事实上，后来我跟着<a
href="https://blog.csdn.net/biejieyu1016/article/details/120874915">这篇教程</a>做的</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker #添加docker用户组</span><br><span class="line">sudo gpasswd -a $USER docker  #将登陆用户加入到docker用户组中</span><br><span class="line">newgrp docker #更新用户组</span><br><span class="line">docker images    #测试docker命令是否可以使用sudo正常使用</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version</span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           24.0.3</span><br><span class="line"> API version:       1.43</span><br><span class="line"> Go version:        go1.20.5</span><br><span class="line"> Git commit:        3713ee1</span><br><span class="line"> Built:             Wed Jul  5 20:44:55 2023</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Context:           default</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          24.0.3</span><br><span class="line">  API version:      1.43 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.20.5</span><br><span class="line">  Git commit:       1d9c861</span><br><span class="line">  Built:            Wed Jul  5 20:44:55 2023</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.6.21</span><br><span class="line">  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.1.7</span><br><span class="line">  GitCommit:        v1.1.7-0-g860f061</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.19.0</span><br><span class="line">  GitCommit:        de40ad0</span><br></pre></td></tr></table></figure>
<p>查看<code>nvidia-smi</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Fri Jul  7 14:04:28 2023       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Quadro P620                    Off | 00000000:01:00.0  On |                  N/A |</span><br><span class="line">| 34%   41C    P8              N/A /  N/A |    191MiB /  2048MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|    0   N/A  N/A      1833      G   /usr/lib/xorg/Xorg                           72MiB |</span><br><span class="line">|    0   N/A  N/A      1949    C+G   ...libexec/gnome-remote-desktop-daemon       38MiB |</span><br><span class="line">|    0   N/A  N/A      1990      G   /usr/bin/gnome-shell                         73MiB |</span><br><span class="line">|    0   N/A  N/A      4784      G   gnome-control-center                          1MiB |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p><a
href="https://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;name=22.04">查找nvidia/cuda镜像</a></p>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>notes</tag>
        <tag>tech</tag>
      </tags>
  </entry>
  <entry>
    <title>踏上ChatGPT旅程</title>
    <url>/2023/05/26/85_ChatGPT/</url>
    <content><![CDATA[<p>开始使用并学习ChatGPT以及相关的<code>LLM(Large Language Model)</code>，不然就被时代淘汰了～</p>
<span id="more"></span>
<h1 id="写在前面">0.写在前面</h1>
<p>从去年11月底ChatGPT发布，到后来爆火，改变互联网，这半年时间我对ChatGPT还是比较冷漠的。</p>
<p>期间我也体验了一下，以及体验了<code>cursor</code>（一个使用chatgpt-4的ai辅助编程工具），但自始至终还是充满了不信任感。所以也一直没注册账号，没想进一步探索。</p>
<p>直到今天，我改变了对它看法，或者说我改变了看它的角度。</p>
<p>以前潜意识里是觉得ChatGPT是智囊、军师、管家之类的身份，觉得人的主观能动性受到限制，所以会抵制。</p>
<p>现在我想把它当工具，来便利我的工作和生活，便没有了心理阻碍。</p>
<p>于是，ChatGPT旅程，出发～</p>
<h1 id="注册chatgpt">1. 注册ChatGPT</h1>
<p>我参考了这篇文章<a
href="https://www.xnbeast.com/create-openai-chatgpt-account/">ChatGPT注册教程攻略：解决手机无法接收验证码问题（亲测有效）</a>
，在<a
href="https://sms-activate.org/">sms-activate</a>购买了临时国外手机号——</p>
<ul>
<li><p>买了个英国的手机号，结果</p>
<p><code>Your account was flagged for potential abuse. If you feel this is an error, please contact us at help.openai.com</code></p></li>
<li><p>换了个美国的也不行</p></li>
<li><p>最后换了个印度尼西亚的手机号，顺利注册，但是进入ChatGPT又显示</p>
<p><code>Sorry, you have been blocked</code></p></li>
</ul>
<p>出师不利啊……突然想起来我买的VPN的维护者说过，这个vpn不搞chatGPT那破玩意儿。于是在tg上找了个<code>@chatgptzs_bot</code>，暂时先用着，也挺好用的。</p>
<p>但是还不甘心，之后我用<code>自由鲸</code>的欧洲节点，开启全局模式，能正常使用ChatGPT了。不过目前不是付费用户，所以没有联网功能和插件功能。</p>
<h1 id="chatgpt使用技巧">2. ChatGPT使用技巧</h1>
<p>昨晚在B站看到一个视频挺好的——</p>
<p><a
href="https://b23.tv/XCclvE1">学完这个视频，简历加一条：熟练掌握ChatGPT解决复杂问题｜ChatGPT使用教程-哔哩哔哩</a></p>
<p>说实话，我也是有一部分被这个视频驱动来搞ChatGPT的。</p>
<h2 id="乔哈里johari沟通视窗">乔哈里(Johari)沟通视窗</h2>
<table>
<thead>
<tr class="header">
<th></th>
<th>你知道</th>
<th>你不知道</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT知道</td>
<td>公开区</td>
<td>盲区</td>
</tr>
<tr class="even">
<td>GPT不知道</td>
<td>隐私区</td>
<td>未知区</td>
</tr>
</tbody>
</table>
<ul>
<li><p>盲区｜问法1:
<code>我想了解xxx，我应该向你问哪些问题？</code></p></li>
<li><p>盲区｜问法2:<code>请给我列出xxx领域/行业相关的，最常用的50个概念，并做简单解释。如果有英文缩写，请给出完整的英文解释。</code></p></li>
<li><p>盲区｜问法3:<code>请详细介绍一下xxx的主要生平事迹。</code>
<code>请详细介绍一下xxx公司的发展历程。</code></p></li>
<li><p>公开区｜检验认知:<code>对于xxx主题/技能，你认为哪些是我必须理解和掌握的核心要点？</code></p></li>
<li><p>公开区｜检验认知:<code>我理解的xxx是这样的，你觉得我的理解对吗？</code></p></li>
<li><p>公开区｜检验认知:<code>我对xxx有一些想法，你能帮我批判性地分析一下这些想法的优点和缺点吗？</code></p></li>
<li><p>公开区｜检验认知:<code>我正在考虑xxx的决定，你能帮我分析一下可能的结果和影响吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我知道xxx的概念，我想知道更多关于xxx的信息。</code></p></li>
<li><p>公开区｜扩充认知:<code>我在xxx问题上遇到困难，你能提供一些可能的解决方案或建议吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我想要深入学习xxx，你能推荐一些进阶的学习资源或学习路径吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我想要在xxx领域有所创新，你能提供一些启发或想法吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我想在xxx领域提升自己，你能根据最新的研究和趋势给我一些建议吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我正在考虑学习xxx，你能给我一些关于这个领域未来发展的观点吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>（背景信息）我要做关于xxx的研究，我认为原因是，还有其他可能的原因吗？给出一些可能的研究假设。</code></p></li>
<li><p>公开区｜扩充认知:<code>我是一个xx行业新手，马上要采访这个行业的资深大佬，我应该向他请教哪些有价值的问题？</code></p></li>
<li><p>盲区｜问法1: <code>你怎么看待这种现象？</code></p></li>
<li><p>盲区｜问法2: <code>可能的原因有哪些？</code></p></li>
<li><p>盲区｜问法3: <code>这会对xxx产生什么样的影响？</code></p></li>
<li><p>盲区｜问法4: <code>你觉得xxx应该怎么做？</code></p></li>
<li><p>未知区｜问法1:
<code>如果xxx，这对社会会产生什么影响？</code></p></li>
<li><p>未知区｜反向提问:
<code>为了测试我对xxx的了解程度，你会问我什么问题来检验我的水平，最少10个</code></p></li>
<li><p>未知区｜反向提问:
<code>我是xx领域的专家，你会问我哪些问题来检验我的专业水平？最少10个。</code></p></li>
<li><p>未知区｜反向提问:
<code>现在我们玩个“你问我答”的游戏，目的是为了测试我在xxxx方面的专业水平。你负责提问，我负责回答。你要根据我的答案进行反馈、评价、补充。如果我说不知道，你就直接输出正确答案，然后你继续提问。</code></p></li>
<li><p>未知区｜反向提问:
<code>我已经很精通xxx了，我想知道我是否还有需要学习的地方？</code></p></li>
</ul>
<p>1 知道自己要干什么，但是不知道ChatGPT能帮你什么。 -
<code>我要做xxx，你能帮我做什么？</code></p>
<p>2 知道要ChatGPT帮你做什么，但是你不知道给它输入什么具体的信息。 -
<code>我想要你xxx，我应该给你输入什么信息？</code></p>
<p>3 直接给ChatGPT指令。任务目标、背景信息、输出要求都非常清晰。 -
<code>请推荐（书名、电视剧、电影、网站、博主、专家、学习资源） 等等。请翻译.请总结.请润色。</code></p>
<h1 id="llm学习">3. LLM学习</h1>
<p>我：给我推荐一些学习大规模语言模型的资料吧</p>
<blockquote>
<p>当然！以下是一些学习大规模语言模型的资料推荐： 1.
论文和研究文章：深入了解大规模语言模型的原理和技术，可以阅读相关的论文和研究文章，例如《Transformers:
Attention Is All You Need》、《BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding》和《GPT (Generative Pre-trained
Transformer)》。 2.
博客文章和教程：有许多博客文章和教程可供学习，这些文章会解释大规模语言模型的概念、实现和应用。可以查找像Towards
Data Science、Medium和OpenAI Blog等平台上的相关文章。 3. GitHub
上的代码库：学习大规模语言模型的实现和代码细节，可以查看GitHub上的开源代码库，如Hugging
Face的transformers库、OpenAI的GPT代码库等。 4.
视频教程和在线课程：有一些在线课程和视频教程专门介绍大规模语言模型。例如，Coursera
上的 "Natural Language Processing with Sequence Models" 和 YouTube 上的
"The Transformer Explained" 等。 5.
社区和论坛：加入与自然语言处理和大规模语言模型相关的社区和论坛，与其他人交流讨论，获取实践经验和最新动态。</p>
<p>这些资源可以帮助你深入了解大规模语言模型的背后原理、应用和实现。希望对你的学习有所帮助！如需更具体的资源或问题，请随时告诉我。</p>
</blockquote>
<h2 id="学习视频">学习视频</h2>
<p>Google Cloud Tech</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=zizonToFXDs">Introduction
to Large Language Models</a></li>
</ul>
<p>ZOMI</p>
<ul>
<li><p><a
href="https://www.youtube.com/watch?v=XujuQB9AxGA">ChatGPT狂飙原理剖析：GPT系列详解！【ChatGPT】系列第01篇</a></p></li>
<li><p><a
href="https://www.youtube.com/watch?v=qf2zcev7mHQ">ChatGPT狂飙：强化学习RLHF与PPO！【ChatGPT】系列第02篇</a></p></li>
<li><p><a
href="https://www.youtube.com/watch?v=Oy-8BqwXck8&amp;t=1s">ChatGPT狂飙：InstructGPT解析！【ChatGPT】原理第03篇</a></p></li>
<li><p>【ChatGPT狂飙：InstructGPT解析！】思考：</p>
<ul>
<li>大模型通过计算的方式模拟人类的思考，类似于ChatGPT的RLHF技术是否会给世界带来新的技术产业革命？</li>
<li>ChatGPT 使用了 Ray 作为细粒度的并行计算和昇构计算，管理分配
RLIDL模型完成复杂训练任务，方便利用强化学习对环境和计算任务进行控制，这对A框架的分布式能力边界带来哪些新的冲击？</li>
<li>ChatGPT
非常重数据交互，存算一体技术会不会针对ChatGPT等应用的现专用芯片和新的架构?</li>
</ul></li>
</ul>
<p><a
href="https://imzhanghao.com/2023/02/24/chatgpt/">ChatGPT模型的三层理解</a></p>
<p><a
href="https://github.com/dalinvip/Awesome-ChatGPT">Awesome-ChatGPT</a></p>
<p>2023.6.1 学习<a
href="https://www.youtube.com/watch?v=nzqlFIcCSWQ">Transformer论文逐段精读</a></p>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>tech</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>Using Ubuntu from scratch</title>
    <url>/2023/06/30/87_Using-Ubuntu-from-scratch/</url>
    <content><![CDATA[<p>原来的Ubuntu系统的用户权限搞崩了，从头开始搞一个～</p>
<span id="more"></span>
<h1 id="系统安装">1.系统安装</h1>
<h2 id="下载">1.1 下载</h2>
<p><a href="http://mirrors.aliyun.com/ubuntu-releases/20.04/">Ubuntu
20.4镜像</a></p>
<h2 id="把iso镜像文件转换成dmg文件">1.2 把iso镜像文件转换成dmg文件</h2>
<p><code>cd Downloads</code></p>
<p><code>hdiutil convert -format UDRW -o ubuntu.iso ubuntu-20.04.6-desktop-amd64.iso</code></p>
<ul>
<li><p><code>-format</code>为生成文件的权限；</p></li>
<li><p><code>UDRW</code> :表示转换成有read/write的权限的镜像。</p></li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Reading Driver Descriptor Map (DDM : 0)…</span><br><span class="line">Reading Ubuntu 20.04.6 LTS amd64         (Apple_ISO : 1)…</span><br><span class="line">Reading Apple (Apple_partition_map : 2)…</span><br><span class="line">Reading Ubuntu 20.04.6 LTS amd64         (Apple_ISO : 3)…</span><br><span class="line">.....................................................</span><br><span class="line">Reading EFI (Apple_HFS : 4)…</span><br><span class="line">.....................................................</span><br><span class="line">Reading Ubuntu 20.04.6 LTS amd64         (Apple_ISO : 5)…</span><br><span class="line">...............................................................................</span><br><span class="line">Elapsed Time:  4.971s</span><br><span class="line">Speed: 834.8MB/s</span><br><span class="line">Savings: 0.0%</span><br><span class="line">created: /Users/zhaodongyu/Downloads/ubuntu.iso.dmg</span><br></pre></td></tr></table></figure>
<h2 id="烧录至优盘">1.3 烧录至优盘</h2>
<p>注意找到U盘的路径，使用<code>diskutil list</code>命令查询。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/dev/disk4 (external, physical):</span><br><span class="line"><span class="meta prompt_">   #</span><span class="language-bash">:                       TYPE NAME                    SIZE       IDENTIFIER</span></span><br><span class="line">   0:     FDisk_partition_scheme                        *16.0 GB    disk4</span><br><span class="line">   1:             Windows_FAT_32 NO NAME                 16.0 GB    disk4s1</span><br></pre></td></tr></table></figure>
<p>找到U盘的路径为/dev/disk4，并使用命令行卸载U盘：</p>
<p><code>diskutil unmountDisk /dev/disk4</code></p>
<p>显示信息：Unmount of all volumes on disk4 was successful</p>
<p>把iso文件写入U盘 <code>mv ubuntu.iso.dmg ubuntu.iso</code></p>
<p><code>sudo dd if=./ubuntu.iso of=/dev/disk4 bs=1m</code></p>
<p>等了几分钟后，显示 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">4149+1 records in</span><br><span class="line">4149+1 records out</span><br><span class="line">4351463424 bytes transferred in 369.833082 secs (11766020 bytes/sec)</span><br></pre></td></tr></table></figure></p>
<p>弹出优盘</p>
<p><code>sudo eject /dev/disk4</code></p>
<h1 id="系统设置">2.系统设置</h1>
<h2 id="开启ssh连接">2.1 开启SSH连接</h2>
<p><a
href="https://blog.csdn.net/jackghq/article/details/54974141">ubuntu开启SSH服务远程登录</a></p>
<p><code>sudo apt-get install openssh-server</code></p>
<p>可以ssh连接了</p>
<h2 id="更新vim">2.2 更新vim</h2>
<ul>
<li><p>卸载旧版的vi：sudo apt-get remove vim-common</p></li>
<li><p>安装新版的vim：sudo apt-get install vim</p></li>
</ul>
<h2 id="挂载硬盘">2.3 挂载硬盘</h2>
<p>查看磁盘信息命令: <code>sudo fdisk -l</code></p>
<p>其中有： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Disk /dev/sdb: 931.51 GiB, 1000204886016 bytes, 1953525168 sectors</span><br><span class="line">Disk model: ST1000DM003-1SB1</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br></pre></td></tr></table></figure></p>
<p>查看硬盘的UUID命令: <code>sudo blkid</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/dev/sdb: UUID=&quot;2b4d86ff-3fb8-40c4-b5ef-dd840f65f94f&quot; BLOCK_SIZE=&quot;4096&quot; TYPE=&quot;ext4&quot;</span><br></pre></td></tr></table></figure>
<p>mkdir创建挂载点data</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zhaodongyu@portal:/$ sudo mkdir data</span><br><span class="line">zhaodongyu@portal:/$ ls</span><br><span class="line">bin   cdrom  dev  home  lib32  libx32      media  opt   root  sbin  srv       sys  usr</span><br><span class="line">boot  data   etc  lib   lib64  lost+found  mnt    proc  run   snap  swapfile  tmp  var</span><br></pre></td></tr></table></figure>
<p>手动挂载</p>
<p><code>sudo mount /dev/sdb /data</code></p>
<p>永久性挂载分区，修改分区文件，输入命令<code>sudo vi /etc/fstab</code>，增加
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UUID=2b4d86ff-3fb8-40c4-b5ef-dd840f65f94f /data           ext4    defaults          0       0</span><br></pre></td></tr></table></figure></p>
<p>查看挂载好的硬盘信息 <code>df -hT</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/dev/sdb       ext4   916G  207G  663G  24% /data</span><br></pre></td></tr></table></figure>
<h2 id="安装-vs-code">2.4 安装 VS code</h2>
<p>打开 Ubuntu
软件中心，并且搜索<code>Visual Studio Code</code>,然后安装应用</p>
<h2 id="安装make">2.5 安装make</h2>
<p><code>sudo apt install make</code></p>
<h2 id="cmake升级">2.5 cmake升级</h2>
<p><a
href="https://blog.csdn.net/Boys_Wu/article/details/104940575">参考</a></p>
<p>我在<a
href="https://cmake.org/download/">cmake官网</a>下载了<code>cmake-3.27.0-rc4-linux-x86_64.tar.gz</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf cmake-3.27.0-rc4-linux-x86_64.tar.gz</span><br><span class="line"></span><br><span class="line">sudo mv cmake-3.27.0-rc4-linux-x86_64 /opt/cmake-3.27.0</span><br><span class="line"></span><br><span class="line">sudo ln -sf /opt/cmake-3.27.0/bin/* /usr/bin/</span><br><span class="line"></span><br><span class="line">cmake --version</span><br><span class="line">    cmake version 3.27.0-rc4</span><br><span class="line"></span><br><span class="line">sudo vim ~/.bashrc </span><br><span class="line">    export  PATH=$PATH:/opt/cmake-3.27.0/bin</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="gcc降级">2.6 gcc降级</h2>
<p>原来的gcc版本是9.4</p>
<p><code>sudo apt-get install gcc-9</code></p>
<p><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100</code></p>
<p>sudo apt-get install g++-9</p>
<p>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9
100</p>
]]></content>
      <tags>
        <tag>notes</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Crawler</title>
    <url>/2020/05/19/89_Crawler/</url>
    <content><![CDATA[<p>背景：国内表情识别的数据集太少了 &amp;
表情数据集基本都是外国人脸，训出来的模型实际使用效果不好。</p>
<p>于是，在百度上爬了一批数据下来补充到数据集里面。</p>
<p>[爬虫有风险，操作需谨慎]</p>
<span id="more"></span>
<h1 id="写在前面的话">写在前面的话</h1>
<p>一共两个文件</p>
<ul>
<li><p>crawler.py</p>
<p>主程序：实现爬虫功能，爬取的关键词在keywords.txt里面</p></li>
<li><p>keywords.txt</p>
<p>关键词：比如爬开心的图，就可以设置 happy 高兴 高兴表情 高兴的人
高兴脸 开心 开心脸 开心的人 开心表情 欢愉 愉悦 愉悦表情 欢愉表情
幸福表情 幸福的脸 幸福的人 愉悦表情 欢欣表情 欢喜表情 喜悦</p>
<p><strong>每个关键词用换行分隔</strong></p></li>
</ul>
<p>运行python文件后，需要等待一会（正在获取图片的<code>url</code>），然后就显示下载log了。</p>
<p>结果会保存到<code>image</code>文件夹内，后续需要进一步批处理文件以及进行人脸提取、对齐、清洗等工作。</p>
<p>清洗数据集还是相当繁琐的，建议不要浪费时间在数据清洗上，自己想办法找途径处理。</p>
<pre><code>[爬虫有风险，操作需谨慎]

希望你能把这个代码用在正途，造福社会:)

希望你能开源数据集，减少社会整体的成本:)</code></pre>
<h1 id="python代码">python代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">download_path = <span class="string">&#x27;./image/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_onepage_urls</span>(<span class="params">onepageurl</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取单个翻页的所有图片的urls+当前翻页的下一翻页的url&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> onepageurl:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;已到最后一页, 结束&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> [], <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        html = requests.get(onepageurl)</span><br><span class="line">        html.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">        html = html.text</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line">        pic_urls = []</span><br><span class="line">        fanye_url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> pic_urls, fanye_url</span><br><span class="line">    pic_urls = re.findall(<span class="string">&#x27;&quot;objURL&quot;:&quot;(.*?)&quot;,&#x27;</span>, html, re.S)</span><br><span class="line">    fanye_urls = re.findall(re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;a href=&quot;(.*)&quot; class=&quot;n&quot;&gt;下一页&lt;/a&gt;&#x27;</span>), html, flags=<span class="number">0</span>)</span><br><span class="line">    fanye_url = <span class="string">&#x27;http://image.baidu.com&#x27;</span> + fanye_urls[<span class="number">0</span>] <span class="keyword">if</span> fanye_urls <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> pic_urls, fanye_url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_pic</span>(<span class="params">pic_urls,label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;给出图片链接列表, 下载所有图片&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#for i, pic_url in tqdm(enumerate(pic_urls)):</span></span><br><span class="line">    <span class="keyword">for</span> i,pic_url <span class="keyword">in</span> <span class="built_in">enumerate</span>(pic_urls):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pic = requests.get(pic_url, timeout=<span class="number">15</span>)</span><br><span class="line">            string = download_path + label + <span class="string">&#x27;_baidu/&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(string, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(pic.content)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;成功下载第(%s/%s)张图片: %s&#x27;</span> % (<span class="built_in">str</span>(i + <span class="number">1</span>), <span class="built_in">str</span>(<span class="built_in">len</span>(pic_urls)), <span class="built_in">str</span>(pic_url)))</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;下载第%s张图片时失败: %s&#x27;</span> % (<span class="built_in">str</span>(i + <span class="number">1</span>), <span class="built_in">str</span>(pic_url)))</span><br><span class="line">            <span class="comment">#print(e)</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;keywords.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.read().splitlines()</span><br><span class="line">        <span class="built_in">print</span>(lines)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(lines):</span><br><span class="line">        file_path = os.path.join(download_path, line)</span><br><span class="line">        file_path = file_path + <span class="string">&#x27;_baidu&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">            os.makedirs(file_path, <span class="number">0o755</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shutil.rmtree(file_path)</span><br><span class="line">            os.makedirs(file_path, <span class="number">0o755</span>)</span><br><span class="line"></span><br><span class="line">        url_init_first = <span class="string">r&#x27;http://image.baidu.com/search/flip?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=result&amp;fr=&amp;sf=1&amp;fmq=1497491098685_R&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;ctd=1497491098685%5E00_1519X735&amp;word=&#x27;</span></span><br><span class="line">        url_init = url_init_first + urllib.parse.quote(line, safe=<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">        all_pic_urls = []</span><br><span class="line">        onepage_urls, fanye_url = get_onepage_urls(url_init)</span><br><span class="line">        all_pic_urls.extend(onepage_urls)</span><br><span class="line"></span><br><span class="line">        fanye_count = <span class="number">0</span>  <span class="comment"># 累计翻页数</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            onepage_urls, fanye_url = get_onepage_urls(fanye_url)</span><br><span class="line">            fanye_count += <span class="number">1</span></span><br><span class="line">            <span class="comment">#print(&#x27;第页&#x27; % str(fanye_count))</span></span><br><span class="line">            <span class="keyword">if</span> fanye_url == <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> onepage_urls == []:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            all_pic_urls.extend(onepage_urls)</span><br><span class="line"></span><br><span class="line">        down_pic(<span class="built_in">list</span>(<span class="built_in">set</span>(all_pic_urls)),line)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="附keywords.txt">附<code>keywords.txt</code></h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">鄙视脸</span><br><span class="line">鄙视表情</span><br><span class="line">嫌恶</span><br><span class="line">恶心表情</span><br><span class="line">腻烦表情</span><br><span class="line">厌烦表情</span><br><span class="line">憎恶表情</span><br><span class="line">反感表情</span><br><span class="line">不耐烦</span><br><span class="line">不耐烦表情</span><br><span class="line">嫌弃表情</span><br><span class="line">fear</span><br><span class="line">恐惧</span><br><span class="line">恐惧表情</span><br><span class="line">恐惧的人</span><br><span class="line">恐惧脸</span><br><span class="line">害怕</span><br><span class="line">害怕表情</span><br><span class="line">害怕脸</span><br><span class="line">恐慌</span><br><span class="line">恐慌表情</span><br><span class="line">恐慌脸</span><br><span class="line">惊吓</span><br><span class="line">惊怖表情</span><br><span class="line">可骇表情</span><br><span class="line">怯生生表情</span><br><span class="line">胆怯表情</span><br><span class="line">胆怯脸</span><br><span class="line">恐慌的人</span><br><span class="line">惧怕表情</span><br><span class="line">惧怕的人</span><br><span class="line">happy</span><br><span class="line">高兴</span><br><span class="line">高兴表情</span><br><span class="line">高兴的人</span><br><span class="line">高兴脸</span><br><span class="line">开心</span><br><span class="line">开心脸</span><br><span class="line">开心的人</span><br><span class="line">开心表情</span><br><span class="line">欢愉</span><br><span class="line">愉悦</span><br><span class="line">愉悦表情</span><br><span class="line">欢愉表情</span><br><span class="line">幸福表情</span><br><span class="line">幸福的脸</span><br><span class="line">幸福的人</span><br><span class="line">愉悦表情</span><br><span class="line">欢欣表情</span><br><span class="line">欢喜表情</span><br><span class="line">喜悦</span><br><span class="line">sad</span><br><span class="line">悲伤</span><br><span class="line">悲伤表情</span><br><span class="line">悲伤的人</span><br><span class="line">悲伤脸</span><br><span class="line">伤心</span><br><span class="line">伤心表情</span><br><span class="line">难过</span><br><span class="line">难过表情</span><br><span class="line">难过的人</span><br><span class="line">伤心的人</span><br><span class="line">伤感表情</span><br><span class="line">悲痛表情</span><br><span class="line">哀伤</span><br><span class="line">心酸表情</span><br><span class="line">悲戚表情</span><br><span class="line">哀痛的人</span><br><span class="line">悲痛的人</span><br><span class="line">哀伤脸</span><br><span class="line">心酸脸</span><br><span class="line">surprise</span><br><span class="line">惊讶</span><br><span class="line">惊讶表情</span><br><span class="line">惊讶的人</span><br><span class="line">惊讶脸</span><br><span class="line">诧异</span><br><span class="line">惊奇</span><br><span class="line">惊奇表情</span><br><span class="line">惊奇脸</span><br><span class="line">诧异脸</span><br><span class="line">诧异表情</span><br><span class="line">吃惊</span><br><span class="line">吃惊表情</span><br><span class="line">吃惊脸</span><br><span class="line">吃惊的人</span><br><span class="line">惊诧</span><br><span class="line">惊愕</span><br><span class="line">惊诧表情</span><br><span class="line">惊叫表情</span><br><span class="line">惊叫脸</span><br></pre></td></tr></table></figure>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <tags>
        <tag>tech</tag>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title>ADAS</title>
    <url>/2020/04/23/93_ADAS/</url>
    <content><![CDATA[<p>自动驾驶汽车可以实现<code>驾驶员驾驶</code>、<code>系统辅助驾驶员驾驶</code>以及<code>系统自动驾驶</code>。而无人驾驶汽车是不需要驾驶员的，即便在紧急情况时也能自行处理。</p>
<p>最近调研了一下ADAS，感觉是一个不错的方向。然而个人认为自动驾驶的<code>corner case</code>太多，未来充满挑战。</p>
<span id="more"></span>
<h2 id="自动驾驶汽车">自动驾驶汽车</h2>
<p><code>自动驾驶汽车(Autonomous vehicles; Self-piloting automobile)</code>是一种通过电脑系统实现无人驾驶的智能汽车。</p>
<p>自动驾驶汽车可以实现<code>驾驶员驾驶</code>、<code>系统辅助驾驶员驾驶</code>以及<code>系统自动驾驶</code>。而无人驾驶汽车是不需要驾驶员的，即便在紧急情况时也能自行处理。</p>
<p><img src="/images/adas/1.png" width="80%"></p>
<h2 id="自动驾驶的五个等级">自动驾驶的五个等级</h2>
<pre><code>NHTSA：美国国家交通管理局
SAE：美国汽车工程师学会</code></pre>
<p><img src="/images/adas/2.png" width="80%"></p>
<p>在车企们万众一心向L3级自动驾驶冲刺的关口，以自动驾驶为最终目标的<code>ADAS系统</code>自然而然的火了。</p>
<p>目前世界上大部分的汽车，都处于<code>L1</code>级别，只有少数汽车，比如<code>特斯拉</code>，已经可以处于<code>L2</code>级别，并在不断完善当中。</p>
<p><code>ADAS</code>实现的是<code>第3级</code>，而<code>自动驾驶</code>实现的是<code>第4级</code>。想要从第3级发展到第4级，还需要更多的汽车上配有自动驾驶技术，而且得配合道路基础设施建设（道路上的摄像头，清晰的车道线），以及需要汽车互联，汽车手机互联等等，是个非常庞大的工程.</p>
<h2 id="adas">ADAS</h2>
<p>高级驾驶辅助系统<code>ADAS(Advanced Driver Assistance System)</code></p>
<blockquote>
<p>利用安装在车上的各式各样传感器（毫米波雷达、激光雷达、单），在汽车行驶过程中随时来感应周围的环境，收集数据，进行静态、动态物体的辨识、侦测与追踪，并结合导航仪地图数据，进行系统的运算与分析，从而预先让驾驶者察觉到可能发生的危险，有效增加汽车驾驶的<code>舒适性</code>和<code>安全性</code>。</p>
</blockquote>
<ul>
<li><p>ADAS核心是<code>环境感知</code></p></li>
<li><p>自动驾驶是<code>人工智能</code></p></li>
<li><p>ADAS可以视作自动驾驶汽车的<code>前提</code></p></li>
</ul>
<p><strong>虽然ADAS被很多人视作自动驾驶汽车的前提，但就其本质而言，ADAS是辅助驾驶，核心是环境感知，而自动驾驶是人工智能，两者体系有很大差别。</strong></p>
<hr />
<p>ADAS是最接近于自动驾驶的智能辅助系统。但是目前生活中比较常用的是<code>ADAS防碰撞预警</code>+<code>DSM防疲劳驾驶检测系统</code>。</p>
<h2 id="adas分类">ADAS分类</h2>
<p><img src="/images/adas/3.png" width="90%"></p>
<h3 id="adas前车碰撞预警fcw">ADAS——前车碰撞预警FCW</h3>
<p><img src="/images/adas/4.png" width="30%"></p>
<p>由安装在车头的雷达，侦测自车和前方车辆的距离及速度，初期会发出警告声来提醒驾驶人注意车距，若车距依然持续拉近，车辆便会先自动轻踩刹车，并轻拉安全带2-3次，警告驾驶人。</p>
<p>若系统判定追撞是没办法避免，启动自动紧急刹车（AEB）后，会同时立刻拉紧安全带固定驾驶人，降低意外发生后的伤害。</p>
<h3 id="adas盲点侦测系统">ADAS——盲点侦测系统</h3>
<p><img src="/images/adas/5.png" width="30%"></p>
<p>汽车驾驶人的盲点是指三面后视镜，左、右、内看不到的区域</p>
<p>盲点侦测系统就是运用雷达和传感器，来侦测车辆后方的盲点区，在盲点区侦测到车辆靠近时会向驾驶员提供警示，帮助驾驶人将意外的机率降至最低。</p>
<h3 id="adas停车辅助系统">ADAS——停车辅助系统</h3>
<p><img src="/images/adas/6.png" width="30%"></p>
<p>停车辅助系统又分为 2种，分别是主动式与被动式 -
主动式系统自动控制方向盘以帮助驾驶完成停车，当然油门、刹车与档位切换还是要车主自行操控。
-
被动式是以影像（摄影机）与影音（超音波）为感测单元所组成的，提供更多车身周围信息给车主掌握，减少碰撞机会。</p>
<h3 id="adas车道偏离警示系统ldw">ADAS——车道偏离警示系统LDW</h3>
<p><img src="/images/adas/7.png" width="30%"></p>
<p>由摄影机、传感器及控制器所组成，原理是运用在车身侧面或后视镜的摄影机，采样目前行驶车道的标识线，再通过图像处理取得当前汽车在车道的位子，这时只要汽车偏离车道，控制器就会发出警报信号。</p>
<p>从感测到发出警报，过程只需约0.5
秒的时间，以实时提醒、叫醒驾驶，避免意外的发生。</p>
<h3 id="adas适应性车灯系统">ADAS——适应性车灯系统</h3>
<p><img src="/images/adas/8.png" width="30%"></p>
<p>这套系统可依照不同的路况、环境、车速及天气状况，自动调整车灯的照明范围及角度，让车灯照射范围可以更深远下，又不会影像到其他用路人的视线，以提供驾驶人与对向来车更安全及舒适的照明。</p>
<p>从过去的AFS主动转向式头灯，到现在结合传感器的多颗LED智能型头灯，都是属于此系统的范畴。</p>
<h3 id="adas夜视系统">ADAS——夜视系统</h3>
<p><img src="/images/adas/9.png" width="30%"></p>
<p>可帮助驾驶可以在视线不明、看不清楚的夜晚或恶劣天气时，自动识别动物或大型异物，同时警告驾驶前方路况，以避免意外的发生。</p>
<p>辨别方式为以红外线来感知热量的不同，区分人、动物、车辆以及环境的差异，经过处理转变成图像，将原本看不清楚的物体清楚呈现在驾驶眼前，以降低行车风险。</p>
<h3 id="adas主动车距控制巡航系统-acc">ADAS——主动车距控制巡航系统
ACC</h3>
<p><img src="/images/adas/10.png" width="30%"></p>
<p>通过安装在车辆前部的车距传感器，持续扫描车辆前方道路来得知前车的车速与相对距离，行驶中会自动侦测车速，当与前车的距离越来越小时，会对应调整自身车速，与前方车辆保持安全距离，减少碰撞意外的发生，也就是所谓的高级版自动巡航系统。</p>
<p>目前许多车款上都已可看见此系统的踪影。</p>
<h3 id="adas驾驶人生理状态监视">ADAS——驾驶人生理状态监视</h3>
<p><img src="/images/adas/11.png" width="30%"></p>
<p>目前系统大多都是利用摄影机侦测驾驶者脸部，判断专注力程度、是否有打瞌睡的象征，还有系统更是利用驾驶人眼睛开闭频率情况，来辨别安全等级，提供适合的警告或是协助动作。</p>
<p>如果驾驶者的脸部表情变化减少，甚至出现闭眼的情况，车辆就会透过声响与灯号来警示车主注意，以减少意外事故发生。</p>
<h2 id="adas关键节点">ADAS关键节点</h2>
<ul>
<li><p>感知</p>
<p>目前ADAS主要采用的<code>传感器</code>有摄像头、雷达、激光和超声波等，可以探测光、热、压力或其它用于监测汽车状态的变量，通常位于车辆的前后保险杠、侧视镜、驾驶杆内部或者挡风玻璃上。</p>
<p>大部份ADAS采用<code>摄像头</code>+<code>雷达</code>的组合方式，以实现雷达测距与摄像头图像识别功能的互补。主动式与被动式红外夜视系统是两种主流的技术路线。<code>主动式</code>通过CCD接受物体反射的对应敏感频谱成像，而<code>被动式</code>的红外焦平面探测器接受物体的红外辐射成像。两者各有优势，将在较长时间内共存。</p>
<p>不同的系统需要使用不同类型的车用传感器，包含毫米波雷达、超声波雷达、红外雷达、激光雷达、CCD
CMOS影像传感器及轮速传感器等，来收集整车的工作状态及其参数变化情形，并将不断变化的机械运动变成电子参数(电压、电阻及电流)。</p>
<ul>
<li>举例来说，车道偏离警告系统使用CMOS影像传感器、夜视系统则使用红外线传感器、适应性定速控制通常使用雷达、停车辅助系统则会使用超声波等。</li>
</ul></li>
<li><p>判断</p>
<p>算法是ADAS系统可靠性、准确度的<code>决定性因素</code>，主要包括摄像头/雷达测距、行人识别、道路交通标志识别等，针对前装应用的可靠性要求高，需要进行大量的场景测试与标定。其中雷达标定的门槛最高。</p>
<p>电子控制单元(ECU)会在针对传感器所收集到的信息进行分析处理，然后再向控制的执行装置下达动作指令。</p></li>
<li><p>执行
包含油门、刹车、灯光、声响等系统都是属于执行器的范畴内，会依据ECU输出的讯号，来执行各种反应动作，让汽车安全行驶于道路上。</p>
<p>ADAS系统通过传感器获取数据，主芯片完成判断后，初级应用通过声音、图像、振动对驾驶者进行<code>警示</code>。</p>
<p>与电子控制功能结合后，逐渐进化到对车辆的<code>自动控制</code>。</p></li>
</ul>
<h2 id="adas产业链">ADAS产业链</h2>
<p><img src="/images/adas/12.png" width="60%"></p>
<p>在产业链中，<code>传感器技术</code>(摄像头与雷达)、<code>芯片</code>与<code>算法</code>是其中的关键。</p>
<ul>
<li><p>其中，摄像头核心部件CMOS感光芯片主要掌握在以索尼、三星为代表的日韩企业中。</p></li>
<li><p>雷达分为超声波雷达、毫米波雷达、激光雷达</p>
<ul>
<li>超声波雷达技术门槛较低，供应商较多；</li>
<li>激光雷达成本高昂，未商业化；</li>
<li>毫米波雷达成本在两者之间，技术门槛较高，主要掌握在ZF
TRW、博世等行业领先公司手中。</li>
</ul></li>
<li><p>而芯片、算法在ADAS系统中至关重要，行业集中度高，主要有Mobileeye、ADI等公司</p></li>
</ul>
<h2 id="视觉adas">视觉ADAS</h2>
<ul>
<li><p>原理</p>
<p>图像处理，将图片转换为<code>二维数据</code></p>
<p>模式识别，通过图像匹配进行识别，如车辆、行人、车道线、交通标志</p>
<p>利用物体的运动模式，或双目定位，估算目标物体与本车的相对距离和相对速度</p></li>
<li><p>优点</p>
<p>像素级别的颜色、偏移、距离等信息</p>
<p>成本低，信息量丰富、特征识别好</p></li>
<li><p>缺点</p>
<p>获取准确<code>三维信息</code>难度大</p>
<p>受环境光限制比较大，速度、距离分辨率差</p></li>
</ul>
<p>目前在行车辅助功能中，摄像头可用来单独实现很多功能，并且按照自动驾驶发展规律逐渐演进。</p>
<p><img src="/images/adas/13.png" width="60%"></p>
<p>这些功能更加强调对输入图像的处理，从拍摄的视频流中提取<code>有效目标运动信息</code>做进一步分析，给出预警信息或直接调动控制机构。相比视频输出类功能，强调<code>高速下的实时性</code>，这部分技术正处在发展上升期。</p>
<h2 id="视觉adas软硬需求">视觉ADAS软硬需求</h2>
<ul>
<li><p>摄像头模组</p>
<p>需要定制化开发，一般要满足宽动态、高感光、不一味追求高像素
ADAS的摄像头更讲究为行车时预留更多判断时间，需要看得更远。</p></li>
<li><p>核心算法芯片</p>
<p>如果在算法上叠加深度学习来帮助识别率提升，对硬件性能的要求只增不减，主要考虑的性能指标是运算速度、功耗、以及成本。</p></li>
<li><p>软件算法</p>
<p>源头是计算机视觉-&gt;深度学习的渗透</p>
<p>在感知方面，深度学习将会弯道超车传统视觉算法</p>
<p>无论哪种算法，数据都是用来训练测试的宝贵资源，而且不是单纯的越多越好，而是越有效越好（符合实际用车环境并保证多样化）。</p></li>
</ul>
<h2 id="小结">小结</h2>
<p>本次调研对<code>ADAS</code>有了一个系统的认知，感觉是<code>非常有潜力</code>以及<code>非常有挑战</code>的发展方向。</p>
<p>视觉ADAS相比于毫米波雷达识别效果更好，可以充分利用<code>视觉信息</code>进行车辆、路标等识别。</p>
<p>我们的重点在于利用<code>感知的信息</code>做决策，其中算法是ADAS系统可靠性、准确度的决定性因素，高效的部署会影响实时性。</p>
<p>接下来会对<code>FCW</code>及相关算法进行进一步调研</p>
<h2 id="参考博客">参考博客</h2>
<p><a
href="https://blog.csdn.net/u010607947/article/details/70847028?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0">一篇文章看懂ADAS</a></p>
<p><a
href="https://blog.csdn.net/zhanshen112/article/details/80374815?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-6">ADAS的八大系统</a></p>
<p><a
href="https://blog.csdn.net/sinat_32970179/article/details/99833646?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-13">特斯拉、吉利车上的那些ADAS系统你都会用吗？</a></p>
<p><a
href="https://blog.csdn.net/wanzew/article/details/82828579?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4">ADAS功能盘点及实现过程解析</a></p>
<p><a
href="https://blog.csdn.net/usstmiracle/article/details/95461183?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3">ADAS是什么？</a></p>
<p><a
href="https://blog.csdn.net/weixin_42229404/article/details/82623742?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-1">ADAS各功能模块及ADAS解决方案提供商详解</a></p>
<p><a
href="https://blog.csdn.net/Gary_Yan_cc/article/details/80717069?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-12">车载摄像头在ADAS中的应用</a></p>
<p><a
href="https://blog.csdn.net/u011776903/article/details/72866442">ADAS视觉方案盘点上篇：摄像头、芯片和算法</a></p>
<h2 id="参考视频">参考视频</h2>
<p><a
href="https://haokan.baidu.com/v?vid=6343999001934023106&amp;pd=bjh&amp;fr=bjhauthor&amp;type=video">ADAS自动辅助驾驶系统为何如此受青睐？看看它到底有多强大！</a></p>
<p><a
href="https://haokan.baidu.com/v?vid=3780861684865301120&amp;pd=bjh&amp;fr=bjhauthor&amp;type=video">ADAS智能驾驶辅助系统</a></p>
<p><a
href="https://haokan.baidu.com/v?vid=8288616481555786115&amp;tab=recommend">WEY
VV6智能驾驶辅助系统深度体验</a></p>
<p><a
href="https://v.qq.com/x/page/m0373fiwrvp.html?">第一视角体验通用自动驾驶第二弹</a></p>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <tags>
        <tag>ADAS</tag>
        <tag>tech， research</tag>
      </tags>
  </entry>
  <entry>
    <title>内存对齐的实现</title>
    <url>/2022/12/13/95_memory-align/</url>
    <content><![CDATA[<p>各个硬件平台对存储空间的处理上有很大的不同。一些平台对某些<code>特定类型的数据</code>只能从某些<code>特定地址</code>开始存取,而不是内存中任意地址都是可以读取的。</p>
<p>内存对齐不仅<strong>便于CPU快速访问</strong>，同时合理的利用字节对齐可以<strong>有效的节省存储空间</strong>。</p>
<p>花了一点点时间搞懂了内存对齐的实现与细节——</p>
<span id="more"></span>
<h2 id="什么是内存对齐">0.什么是内存对齐？</h2>
<p>计算机中内存空间是按照<code>byte</code>划分的，从理论上讲似乎对任何类型的变量的访问可以从任何地址开始，但实际情况是：在访问<code>特定类型变量</code>的时候通常在<code>特定的内存地址</code>访问，这就需要对这些数据在内存中存放的位置有限制，各种类型数据按照一定的规则在空间上排列，而不是顺序的一个接一个的排放，这就是对齐。</p>
<p>内存对齐是编译器的管辖范围。表现为：编译器为程序中的每个“数据单元”安排在适当的位置上。</p>
<h2 id="为什么要内存对齐">1.为什么要内存对齐？</h2>
<p>各个硬件平台对存储空间的处理上有很大的不同。一些平台对某些<code>特定类型的数据</code>只能从某些<code>特定地址</code>开始存取,而不是内存中任意地址都是可以读取的。</p>
<p><strong>比如有些架构的CPU在访问一个没有进行对齐的变量的时候会发生错误,那么在这种架构下编程必须保证字节对齐.</strong></p>
<p>其他平台可能没有这种情况，但是最常见的是如果不按照适合其平台要求对数据存放进行对齐，会在存取效率上带来损失。</p>
<p>内存对齐不仅<strong>便于CPU快速访问</strong>，同时合理的利用字节对齐可以<strong>有效的节省存储空间</strong>。</p>
<p>参考链接：https://juejin.cn/post/6870162226032934926</p>
<h2 id="我的分析实践">2.我的分析实践</h2>
<p><img src="/images/memory_align/MemoryAlign1.png" width="90%"></p>
<p><img src="/images/memory_align/MemoryAlign2.png" width="90%"></p>
]]></content>
      <tags>
        <tag>tech</tag>
        <tag>memory align</tag>
      </tags>
  </entry>
  <entry>
    <title>一文搞懂转置卷积的定义、计算和加速</title>
    <url>/2023/05/04/97_transposed-convolution/</url>
    <content><![CDATA[<p>最近做了一些转置卷积的相关部署工作，最开始搞的时候其实有点晕头转向的，总是试图在用卷积的计算方式反过来理解转置卷积，尤其是看到<code>padding</code>部分和<code>stride</code>部分，搞得我头更大了，心想怎么这么反人类？（后来发现是我理解的问题，其实一点也不反人类）</p>
<p>现在也算是了解了具体工作机制以及加速方式，在这里整理总结一下。欢迎留言、指正
:)</p>
<span id="more"></span>
<h2 id="参考文档">0、参考文档</h2>
<p>先敬上各位大佬的文档，这对我非常非常有帮助！</p>
<p><a
href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">tf.keras.layers.Conv2DTranspose</a></p>
<p><a
href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">What
is Transposed Convolutional Layer?</a></p>
<p><a
href="https://blog.csdn.net/LoseInVain/article/details/81098502">一文搞懂反卷积，转置卷积</a></p>
<p><a
href="https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0">Up-sampling
with Transposed Convolution</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/115070523">转置卷积(Transpose
Convolution)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/283091528">conv_transpose
depth-wise优化技巧</a></p>
<p>图解转置卷积，我分别在<a
href="https://github.com/vdumoulin/conv_arithmetic">conv_arithmetic</a>和<a
href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">What
is Transposed Convolutional Layer</a>里看到，感觉后者更容易理解。</p>
<h2 id="转置卷积是什么">1、转置卷积是什么？</h2>
<h3 id="定义">1.1 定义</h3>
<p><code>转置卷积</code>有时候也被称为<code>反卷积</code>，我个人认为<code>反卷积</code>有很强的误导性，因为这并不是卷积的逆运算，还是叫<code>转置卷积</code>比较好。</p>
<p><code>转置卷积</code>在深度学习中表示为<code>卷积</code>的一个逆向过程，可以根据卷积核大小和输出的大小，恢复卷积前的<code>feature map</code>尺寸，而不是恢复原始值。</p>
<p>如果将卷积表示为y=Cx,转置卷积则是将的输入输出互换：x =
C<sup>T</sup>y</p>
<p>其中， C<sup>T</sup>表示矩阵转置。</p>
<p>详细定义这里就不仔细介绍了,上文里的各个参考文档里说的都很明白。</p>
<h3 id="需要注意">1.2 需要注意</h3>
<p>总结一下我认为的最重要的（最开始纠结了很久的）几个点：</p>
<ul>
<li><p>转置卷积不是恢复原始值，而是恢复原始尺寸（所以不要试图从卷积的逆运算角度考虑）</p></li>
<li><p><code>padding</code>方式和卷积的padding是不一样的，转置卷积的实际padding是<code>k-p-1</code></p></li>
<li><p><code>stride</code>在这里用途不是跳几个数，而是用于判断填充几个0</p></li>
<li><p>用公式法直接计算的话，首先对卷积核做中心对称操作(矩阵旋转180°)</p></li>
<li><p>不考虑性能的话，直接按照转置卷积定义写。反之，一定要优化，不然慢得很。</p></li>
</ul>
<p>The table below summarizes the two convolutions, standard and
transposed.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Conv Type</th>
<th style="text-align: center;">Operation</th>
<th style="text-align: center;">Zero Insertions</th>
<th style="text-align: center;">Padding</th>
<th style="text-align: center;">Stride</th>
<th style="text-align: center;">Output Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">Downsampling</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">s</td>
<td style="text-align: center;">(i+2p-k)/s+1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Transposed</td>
<td style="text-align: center;">Upsampling</td>
<td style="text-align: center;">(s-1)</td>
<td style="text-align: center;">(k-p-1)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">(i-1)*s+k-2p</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意注意，有没有觉得转置卷积的<code>padding</code>和<code>stride</code>很反人类?这和我理解的完全不一样啊？其实不然，这里的<code>padding</code>和<code>stride</code>指的其实是转置卷积结果的<code>padding</code>和<code>stride</code>。即，逆向来看的情况下，卷积的<code>padding</code>和<code>stride</code>。</p>
</blockquote>
<blockquote>
<p>为了说明转置卷积如何<code>推理计算</code>，本文第二章节会使用表格所示的参数计算方式，直到第三章节才会从卷积的角度来看，到时候你会恍然大悟，<code>padding</code>和<code>stride</code>的含义，原来如此。</p>
</blockquote>
<h2 id="转置卷积的计算">2、转置卷积的计算</h2>
<h3 id="从最简单的开始">2.1 从最简单的开始</h3>
<p>conv_transpose有一种最直接的计算方式：首先对卷积核做中心对称操作(矩阵旋转180°)，并对输入feature
map进行插0，然后把旋转后的卷积核和插0后的feature map进行卷积操作</p>
<hr />
<p>现在假设输入的<code>feature map</code>是3x3大小，<code>kernel size</code>是3x3大小，<code>stride</code>为1，
<code>padding</code>为0，即：</p>
<pre><code>input_sz:     3
kernel_sz =   3
stride =      1
padding_sz =  0</code></pre>
<p>写一段torch代码计算一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">K = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">Y = torch.nn.functional.conv_transpose2d(X, K, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<p>得到输出结果： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensor([[[[  1.,   4.,  10.,  12.,   9.],</span><br><span class="line">          [  8.,  26.,  56.,  54.,  36.],</span><br><span class="line">          [ 30.,  84., 165., 144.,  90.],</span><br><span class="line">          [ 56., 134., 236., 186., 108.],</span><br><span class="line">          [ 49., 112., 190., 144.,  81.]]]])</span><br></pre></td></tr></table></figure> 计算过程：</p>
<ol type="1">
<li>对输入<code>X</code>进行处理，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding</li>
</ol>
<p>在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-0-1)=2</code>的padding。输入<code>X</code>则转化为</p>
<p><img src="/images/transposed_conv/x_s1p0.png" width="50%"></p>
<ol start="2" type="1">
<li>对卷积核<code>K</code>进行中心对称操作</li>
</ol>
<p>卷积核<code>K</code>则转化为</p>
<p><img src="/images/transposed_conv/k_reverse.png" width="30%"></p>
<ol start="3" type="1">
<li>进行卷积计算</li>
</ol>
<p><img src="/images/transposed_conv/s1p0.png" width="80%"></p>
<ol start="4" type="1">
<li>gif图解</li>
</ol>
<p><img src="/images/transposed_conv/transposed_conv_S1P0.gif" width="70%"></p>
<h3 id="考虑stride">2.2 考虑stride</h3>
<p>我个人建议不要用卷积的stride来理解转置卷积的stride，<code>stride</code>在这里用途不是跳几个数，而是用于判断填充几个0。</p>
<hr />
<p>现在假设输入的<code>feature map</code>是3x3大小，<code>kernel size</code>是3x3大小，<code>stride</code>为2，
<code>padding</code>为0，即：</p>
<pre><code>input_sz:     3
kernel_sz =   3
stride =      2
padding_sz =  0</code></pre>
<p>同样，写一段torch代码计算一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">K = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">Y = torch.nn.functional.conv_transpose2d(X, K, stride=<span class="number">2</span>, padding=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<p>得到输出结果： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensor([[[[  1.,   2.,   5.,   4.,   9.,   6.,   9.],</span><br><span class="line">          [  4.,   5.,  14.,  10.,  24.,  15.,  18.],</span><br><span class="line">          [ 11.,  16.,  40.,  26.,  60.,  36.,  45.],</span><br><span class="line">          [ 16.,  20.,  44.,  25.,  54.,  30.,  36.],</span><br><span class="line">          [ 35.,  46., 100.,  56., 120.,  66.,  81.],</span><br><span class="line">          [ 28.,  35.,  74.,  40.,  84.,  45.,  54.],</span><br><span class="line">          [ 49.,  56., 119.,  64., 135.,  72.,  81.]]]])</span><br></pre></td></tr></table></figure> 计算过程：</p>
<ol type="1">
<li>对输入<code>X</code>进行处理，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding</li>
</ol>
<p>在这个例子中，<code>s=2</code>，需插入1个0，进行<code>(k-p-1)=(3-0-1)=2</code>的padding。输入<code>X</code>则转化为</p>
<p><img src="/images/transposed_conv/x_s2p0.png" width="50%"></p>
<ol start="2" type="1">
<li>对卷积核K进行中心对称操作</li>
</ol>
<p>卷积核K则转化为</p>
<p><img src="/images/transposed_conv/k_reverse.png" width="30%"></p>
<ol start="3" type="1">
<li>进行卷积计算</li>
</ol>
<p><img src="/images/transposed_conv/s2p0.png" width="80%"></p>
<ol start="4" type="1">
<li>gif图解</li>
</ol>
<p><img src="/images/transposed_conv/transposed_conv_S2P0.gif" width="70%"></p>
<h3 id="考虑padding">2.3 考虑padding</h3>
<p>我最开始在<code>padding</code>这里疑惑了好一会儿，老是在从卷积的角度想转置卷积的<code>padding</code>。就很疑惑，怎么<code>padding</code>越大，计算结果的<code>feature map</code>越小呢？</p>
<p>后来暂时不想具体物理含义了，直接认为转置卷积的实际<code>padding</code>是<code>k-p-1</code>，万事大吉。（第三章节会解<code>padding</code>这个参数）</p>
<hr />
<blockquote>
<p>实际上，tensorflow的padding计算还是有点差异的，除了上面所说的计算，在计算padding的时候还有一个专门针对转置卷积的<code>offset</code>，这可能会导致
左右/上下 的padding数不一致。
为什么这么做呢？个人认为要从转置卷积的目的来看————还原原始<code>feature map</code>的尺寸。
本文暂不考虑这种情况，感兴趣的可以查看tensorflow源码。</p>
</blockquote>
<hr />
<p>现在假设输入的<code>feature map</code>是3x3大小，<code>kernel size</code>是3x3大小，<code>stride</code>为1，
<code>padding</code>为1，即：</p>
<pre><code>input_sz:     3
kernel_sz =   3
stride =      1
padding_sz =  1</code></pre>
<p>写一段torch代码计算一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">K = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">Y = torch.nn.functional.conv_transpose2d(X, K, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<p>得到输出结果： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensor([[[[ 26.,  56.,  54.],</span><br><span class="line">          [ 84., 165., 144.],</span><br><span class="line">          [134., 236., 186.]]]])</span><br></pre></td></tr></table></figure> 计算过程：</p>
<ol type="1">
<li>对输入<code>X</code>进行处理，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding</li>
</ol>
<p>在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-1-1)=1</code>的padding。输入X则转化为</p>
<p><img src="/images/transposed_conv/x_s1p1.png" width="50%"></p>
<ol start="2" type="1">
<li>对卷积核<code>K</code>进行中心对称操作</li>
</ol>
<p>卷积核<code>K</code>则转化为</p>
<p><img src="/images/transposed_conv/k_reverse.png" width="30%"></p>
<ol start="3" type="1">
<li>进行卷积计算</li>
</ol>
<p><img src="/images/transposed_conv/s1p1.png" width="80%"></p>
<ol start="4" type="1">
<li>gif图解</li>
</ol>
<p><img src="/images/transposed_conv/transposed_conv_S1P1.gif" width="70%"></p>
<h3 id="考虑dilation">2.4 考虑dilation</h3>
<p>这里就不考虑了，和卷积一样的，很容易理解。</p>
<h2 id="转置卷积的理解">3、转置卷积的理解</h2>
<p>初次见到转置卷积的时候，我看到<a
href="https://github.com/vdumoulin/conv_arithmetic">conv_arithmetic</a>和<a
href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">What
is Transposed Convolutional
Layer</a>里面的动图计算是很开心的，毕竟这有助于我理解转置卷积是如何计算的。</p>
<p>但后来发现，了解计算过程看这些动图是比较好的，但是要是理解转置卷积，这些动图<strong>很容易误导</strong>。于是我写代码生成了一些动图，虽然有点丑，但是<code>padding</code>部分和<code>stride</code>部分立刻搞得清清楚楚明明白白了。</p>
<hr />
<p>先来个最简单的，stride=1，padding=0.</p>
<p><img src="/images/transposed_conv/solution1_stride1_pad0.gif" width="70%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-0-1)=2</code>的padding。</li>
</ul>
<p>我们来换个角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride1_pad0.gif" width="55%"></p>
<ul>
<li><strong>从output的角度来看，stride=1，padding=0（你get到这个点了吗?!）</strong></li>
</ul>
<hr />
<p>进一步，stride=1，padding=1.</p>
<p><img src="/images/transposed_conv/solution1_stride1_pad1.gif" width="60%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-1-1)=1</code>的padding。</li>
</ul>
<p>同样，我们来换个角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride1_pad1.gif" width="50%"></p>
<ul>
<li><strong>从output的角度来看，stride=1，padding=1</strong></li>
</ul>
<p>相信你已经明白，我在最开始说的，<strong><code>padding</code>和<code>stride</code>指的其实是转置卷积结果的<code>padding</code>和<code>stride</code></strong>这句话了吧？</p>
<hr />
<p>再看一个stride=2的情况——</p>
<p>stride=2，padding=0.</p>
<p><img src="/images/transposed_conv/solution1_stride2_pad0.gif" width="70%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=2</code>，需插入1个0，然后进行<code>(k-p-1)=(3-0-1)=2</code>的padding。</li>
</ul>
<p>我们来换个角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride2_pad0.gif" width="50%"></p>
<ul>
<li><strong>从output的角度来看，stride=2，padding=0</strong></li>
</ul>
<p>尤其是在stride&gt;1的情况下，第二种的计算量是远小于第一种的，因为第一种有大量的无效0计算。</p>
<hr />
<p>最后，stride=2，padding=1.</p>
<p><img src="/images/transposed_conv/solution1_stride2_pad1.gif" width="70%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=2</code>，需插入1个0，然后进行<code>(k-p-1)=(3-1-1)=1</code>的padding。</li>
</ul>
<p>output的角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride2_pad1.gif" width="50%"></p>
<ul>
<li><strong>从output的角度来看，stride=2，padding=1</strong></li>
</ul>
<hr />
<p>至此，各种情况展示的差不多了，转置卷积的<code>padding</code>和<code>stride</code>的含义，相信你也理解了。大家还是习惯用卷积那套参数来定义转置卷积，甚至在很多推理框架中，转置卷积这个算子的input指的是计算结果，output指的是输入（这又是另一个曾让我困扰的地方了hhh）。</p>
<h2 id="转置卷积的加速">4、 转置卷积的加速</h2>
<p>在第三章节，从output的角度来看的那种计算方式，已经避免了大量的无效0计算，其实是已经大大减小<code>乘加计算量</code>的。因此可以从这个角度做加速，效果很明显的。</p>
<p>除此之外还有其他的加速方式，能够大幅度增加推理速度。</p>
<p>由于某些原因，转置卷积的进一步加速工作暂不列出。以后再更新吧</p>
<hr />
<p>本文用图参考了<a
href="https://github.com/aqeelanwar/conv_layers_animation">aqeelanwar</a>的代码，非常感谢。</p>
<p>图像压缩用了<a
href="https://www.iloveimg.com/zh-cn">iloveimg</a>，非常好用～</p>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <tags>
        <tag>HPC</tag>
        <tag>tech</tag>
        <tag>engineering</tag>
        <tag>Transposed Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimize sgemm on RISC-V platform</title>
    <url>/2024/02/26/103_sgemm_riscv_en/</url>
    <content><![CDATA[<p>This project records the process of optimizing SGEMM
(single-precision floating point General Matrix Multiplication) on the
riscv platform.</p>
<p><img src=/images/sgemm/data/result_all.png width=70% /></p>
<span id="more"></span>
<p>This project records the process of optimizing SGEMM
(single-precision floating point General Matrix Multiplication) on the
riscv platform.</p>
<p>General Matrix Multiplication (GEMM) is one of the core computing
units in deep learning frameworks, widely used for implementing
operators such as Convolution, Full Connection, and Matmul.</p>
<p>I conducted experiments and explorations on the <strong>Allwinner
Nezha D1</strong> development board. <code>Versions 0 through 5</code>
were implemented using the C language, while
<code>versions 6 through 9</code> partially utilized assembly language,
involving the <code>RISC-V V</code> extension instructions.</p>
<p>Note: Unlike some other GEMM optimization projects, this project uses
<strong>row-major order</strong> matrices exclusively. Because I prefer
<strong>row-major order</strong>!</p>
<h1 id="prerequisite-knowledge">Prerequisite Knowledge</h1>
<p><img src=/images/sgemm/pics/riscv.gif width=50% /></p>
<p><a href="https://riscv.org/">RISC-V</a> is an open standard
Instruction Set Architecture (ISA) enabling a new era of processor
innovation through open collaboration.</p>
<hr />
<p><img src=/images/sgemm/pics/GEMM.png width=70% /></p>
<p><a
href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">GEMM</a>
General matrix multiply, one of the Basic Linear Algebra
Subprograms.</p>
<hr />
<p><a href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a> stands for
Floating-point Operations Per Second, also known as peak speed per
second. It refers to the number of floating-point operations executed
per second. One <strong>gigaflop (GFLOPS)</strong> equals ten to the
power of nine (10^9) floating-point operations per second.</p>
<p>The computational complexity of matrix multiplication is calculated
as <code>2 * M * N * K</code>. The ratio of computational complexity to
time taken gives the GFLOPS of the current GEMM version. - Multiplying
by 2 is because each operation involves one multiplication and one
addition.</p>
<h1 id="prepare">Prepare</h1>
<p>The related code is located in <code>./prepare/</code>.</p>
<h2 id="test-cross-compilation">Test Cross-Compilation</h2>
<p>I use the <strong>Allwinner Nezha D1</strong> development board, and
downloaded the cross-compilation link from <a
href="https://xuantie.t-head.cn/community/download?id=4090445921563774976">here</a>.</p>
<p>For detailed instructions, please refer to the <a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/blob/main/prepare/README.md">readme</a>.</p>
<h2 id="memory-bandwidth-test">Memory Bandwidth Test</h2>
<p>I conducted memory bandwidth tests on the development board using the
following projects:</p>
<ul>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/2.memcpy_bandwidth_test">2.memcpy_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/3.flw_bandwidth_test">3.flw_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/4.vlw_bandwidth_test">4.vlw_bandwidth_test</a></li>
</ul>
<p><img src=/images/sgemm/prepare/imgs/memory_bandwidth_test.png width=70% /></p>
<h2 id="roofline-model">Roofline Model</h2>
<p><a href="https://en.wikipedia.org/wiki/Roofline_model">Roofline</a>
proposes a method for quantitative analysis using "Operational
Intensity" and provides a formula for the theoretical performance limit
achievable on computational platforms.</p>
<p>According to <a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL
Public Course | RISC-V Technical Analysis</a>:</p>
<ul>
<li>The computing power of D1 can reach <code>4 GFlops</code> (<span
class="citation" data-cites="1GHz">@1GHz</span>).</li>
<li>Memory: <code>2.727 GB/s</code> (DDR3 792 MHz).
<ul>
<li>Although I measured the highest as <code>2.592 GB/s</code>, there
may be some problems somewhere?</li>
<li>Let's trust Sensetime for now, temporarily accept its value.</li>
</ul></li>
</ul>
<p><img src=/images/sgemm/pics/roofline.png width=70% /></p>
<h1 id="sgemm-optimization">SGEMM Optimization</h1>
<p>Related code is located in <code>./sgemm/</code>.</p>
<h2 id="usage">Usage</h2>
<p>Take <code>step0</code> as an example, you need to edit the Makefile
first to configure your cross-compilation chain.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> sgemm/step0/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb push test_bl_sgemm_step0.x ./.</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb shell <span class="string">&#x27;./test_bl_sgemm_step0.x&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h2 id="version-0-naive-version">Version 0: Naive Version</h2>
<p>This version seems to be <strong>the most intuitive</strong> to me,
after all, this is how I learned, understood, and computed matrix
multiplication: &gt; Multiply one row of A by one column of B to get one
element of C.</p>
<p><img src=/images/sgemm/data/result_0.png width=70% /></p>
<p><img src=/images/sgemm/pics/step0.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;          <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;      <span class="comment">// Start 0-st loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>I think <code>version 0</code> very well explains the formula <span
class="math inline">\(C_{mn} = \sum_{k=1}^{K} A_{mk}B_{kn}\)</span>.</p>
<p>However, this version has obvious shortcomings: on a platform with a
theoretical computing power of <code>4 GFLOPS</code>, it only achieves a
maximum computational performance of <code>0.03 GFLOPS</code>. This is
because <strong>the access to matrix B has a very low cache hit rate,
i.e., "poor spatial locality"</strong>. Throughout the calculation, it
is equivalent to accessing matrix B many, many times.</p>
<p>It is advisable to access the elements of multi-dimensional arrays in
sequential order. This can improve the spatial locality of memory access
and make it more friendly to the cache.</p>
<p>Furthermore, it can be observed that with the increase in size, the
performance fluctuates significantly. Analysis of the data shows that
when m=n=k is 128 164 192 228 256 288 320 352 384, the performance is
poor. These numbers differ by 32, and 32 * 4 (sizeof(float)) = 128
B.</p>
<p>It is speculated that the performance fluctuation is related to
cacheline and hardware prefetching -- cacheline = 64B, after cache miss,
hardware prefetching, i.e., HWPrefetcher, reads one more cacheline.</p>
<h2 id="version-1-loop-interchange-version">Version 1: Loop Interchange
Version</h2>
<p>Reusing data in the cache is the most basic and efficient use of
cache. For nested loops, <code>loop interchange</code>,
<code>loop reversal</code>, <code>loop fusion</code>,
<code>loop distribution</code>, <code>loop tiling</code>,
<code>loop unrolling and jam</code>, etc., can be used to improve
program performance.</p>
<p>Selecting an appropriate loop transformation method can both maintain
the semantics of the program and improve its performance.</p>
<p><img src=/images/sgemm/data/result_1.png width=70% /></p>
<p><img src=/images/sgemm/pics/step1.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;          <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;      <span class="comment">// Start 0-nd loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>Compared with <code>version 0</code>, <code>version 1</code> has
better spatial locality for the operation on matrix B, and the
performance has been greatly improved (especially for larger sizes,
while for m = n = k &lt;= 68, the efficiency of version 0 is
higher).</p>
<p>Adjusting the order of m, n, and k does not affect the result (i.e.,
maintaining the semantics of the program), but it can affect the
performance. Testing the performance of different loop orders (using the
Allwinner Nezha D1 platform, with m=n=k=512 as an example)</p>
<table>
<thead>
<tr class="header">
<th>Loop Order</th>
<th>GFLOPS</th>
<th>Analysis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNK</td>
<td>0.012</td>
<td>High cache miss for accessing B</td>
</tr>
<tr class="even">
<td>MKN</td>
<td>0.180</td>
<td></td>
</tr>
<tr class="odd">
<td>NMK</td>
<td>0.012</td>
<td>High cache miss for accessing B</td>
</tr>
<tr class="even">
<td>NKM</td>
<td>0.009</td>
<td>High cache miss for accessing A</td>
</tr>
<tr class="odd">
<td>KMN</td>
<td>0.165</td>
<td></td>
</tr>
<tr class="even">
<td>KNM</td>
<td>0.009</td>
<td>High cache miss for accessing A</td>
</tr>
</tbody>
</table>
<p>However, the hardware utilization of <code>version 1</code> is still
very low, and further optimizations are needed.</p>
<h2 id="version-2-blocking-version">Version 2: Blocking Version</h2>
<p><img src=/images/sgemm/data/result_2.png width=70% /></p>
<p><img src=/images/sgemm/pics/step2.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;          <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;      <span class="comment">// Start 1-st loop</span></span><br><span class="line">        AddDot_4x4_opt( k, &amp;A( i, <span class="number">0</span> ), lda, &amp;B( <span class="number">0</span>, j ), ldb, &amp;C( i, j ), ldc );</span><br><span class="line">    &#125;                                          <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                              <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p><img src=/images/sgemm/pics/gemm_block.png width=70% /></p>
<p>To avoid unnecessary cache swapping, blocking processing is
performed. <a href="https://zhuanlan.zhihu.com/p/342923482">Discussing
Why Blocking Matrix Optimization Works</a> is a good read, I recommend
learning from it.</p>
<p>After performing block operations in <code>version 2</code>, the
performance is still not satisfactory. This is because, although this
version superficially implements blocking logic, there are still some
small tricks in the calculation within the block that have not been
applied.</p>
<h2 id="version-3-blocked-optimization-version">Version 3: Blocked
Optimization Version</h2>
<p><img src=/images/sgemm/data/result_3.png width=70% /></p>
<p><img src=/images/sgemm/pics/step3.gif width=70% /></p>
<p><code>AddDot_4x4_opt</code> has been added.</p>
<p>Several tricks are mentioned in<a
href="https://github.com/flame/blislab/blob/master/tutorial.pdf">BLISlab-tutorial</a>:</p>
<ul>
<li>2.4.2 Loop unrolling
<ul>
<li>Updating loop index i and the pointer cp every time through the
inner loop creates considerable overhead. For this reason, a compiler
will perform loop unrolling.</li>
</ul></li>
<li>2.4.3 Register variables
<ul>
<li>Notice that computation can only happen if data is stored in
registers. A compiler will automatically transform code so that the
intermediate steps that place certain data in registers is
inserted.</li>
</ul></li>
</ul>
<p>After using these tricks, this version has significantly improved
performance!</p>
<p>However, for larger matrix sizes, the performance of this version is
still relatively low. Upon investigation, for example, after accessing
B[0,0], B[0,1], B[0,2], B[0,3], when accessing B[1,0], when the size is
large, there must be a <code>cache miss</code>. Therefore, it would be
great if the data could be rearranged in advance.</p>
<h2 id="version-4-b-prepack-version">Version 4: B prepack Version</h2>
<p><img src=/images/sgemm/data/result_4.png width=70% /></p>
<p><img src=/images/sgemm/pics/step4.gif width=70% /></p>
<p>I assume matrix B is <strong>parameter</strong>, so we can perform
the <code>pack</code> operation in advance. Version 4
<code>prepack</code> matrix B, leading to further performance
improvement!</p>
<p>The reason for the performance improvement is evident: there is a
significant reduction in <code>cache misses</code> when accessing matrix
B. This is the first time I deeply realized the importance of
<code>prepacking neural network weights</code> before model
inference.</p>
<p>It can be seen that when the size is relatively large, the
performance still declines. This should be due to a high number of cache
misses when accessing matrix A. Should we pack A?</p>
<p>I assume matrix A is <strong>input</strong>, so packing A cannot be
done in advance and must be included in the overall timing. Is it
necessary?</p>
<h2 id="version-5-a-pack-b-prepack-version">Version 5: A pack &amp; B
prepack Version</h2>
<p><img src=/images/sgemm/data/result_5.png width=70% /></p>
<p><img src=/images/sgemm/pics/step5.gif width=70% /></p>
<p>Based on Version 4, Version 5 performs packing on matrix A.</p>
<p>Here, since matrix A is assumed to be an input, packing A needs to be
performed during computation, and this time consumption needs to be
included in the timing.</p>
<p>The results are still pleasing, especially with large matrix sizes,
achieving further performance improvements.</p>
<p>I initially approached this experiment with a trial-and-error
mindset, considering the additional read of A and writing of packA. It
seems the main challenge ahead lies in combating cache misses.</p>
<p>The current optimization direction has reached its limit. It's worth
trying to do some <code>preload</code> during the calculation
process.</p>
<p>Next, we'll move to assembly, work on vector calculations, and
implement <code>preload</code> in assembly.</p>
<h2 id="version-6-assembly-version">Version 6: Assembly Version</h2>
<p><img src=/images/sgemm/data/result_6.png width=70% /></p>
<p>Brief explanation: A is not packed, but B is prepacked with 16
numbers.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;       <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">    <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;   <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">        <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>Regarding the use of rvv instructions, I believe <code>vsetvli</code>
is essential, and <code>vfmacc.vf</code> is the mainstay.</p>
<p>I have learned a lot from <a
href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL Course | RISC-V
Technical Analysis</a>. They are truly professional! I recommend
learning theoretical guidance and knowledge points from them, paying
tribute to OpenPPL!</p>
<p>As for assembly operators, there are many details in assembly, and I
strongly complain: <strong>writing assembly is really annoying!
Especially the debugging process, it's torturous.</strong> The last time
I wrote assembly was during my undergraduate classes. Picking it up
again brings some novelty and excitement, and being able to control the
execution of operators at a very fine granularity gives a great sense of
accomplishment.</p>
<p>Regarding how the assembly files are implemented specifically, I
believe the fastest way is to look at the assembly code. I won't explain
it further here.</p>
<p>It should be noted that this version's performance is very poor. Why
is that? It's another issue of <strong>loop order</strong>.</p>
<h2 id="version-7-assembly-version-with-loop-order-swapped">Version 7:
Assembly Version with Loop Order Swapped</h2>
<p><img src=/images/sgemm/data/result_7.png width=70% /></p>
<p>Brief explanation: A is not packed, but B is prepacked with 16
numbers.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;       <span class="comment">// Start 2-st loop</span></span><br><span class="line">    <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">    <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">    <span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;   <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">        <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>Reversing the order of loops, starting with the n-direction followed
by the m-direction, significantly improves performance.</p>
<p>However, the performance of large-sized matrices is still not very
good. The root cause remains in <code>memory access</code>. The
computation of large-sized matrices in the <code>roofline model</code>
is considered <code>compute-bound</code>, where ideally the
<code>compute time</code> and <code>memory access time</code> should
overlap as much as possible. Currently, a significant amount of time is
spent on memory access (mostly due to <code>cache miss</code>!).</p>
<h2 id="version-8-assembly-version-with-preload">Version 8: Assembly
Version with Preload</h2>
<p><img src=/images/sgemm/data/result_8.png width=70% /></p>
<p>Brief Explanation: Matrix A is not <code>packed</code>, while Matrix
B undergoes <code>prepackaging</code> of 16 elements and includes a
<code>preload</code> operation.</p>
<p>The performance is explosive! It reaches a maximum of
<code>2.212 GFLOPS</code>.</p>
<p>Core operations:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfmacc.vf v16,  ft0, v0</span><br><span class="line">vlw.v v4, (bp0)         # b0&#x27;-&gt;v4</span><br><span class="line">flw fs4, 384(bp0)       # pre-load B</span><br><span class="line">addi bp0,bp0,64</span><br><span class="line">vfmacc.vf v20,  ft1, v0</span><br></pre></td></tr></table></figure>
<p>Inserting some <code>load</code> operations between
<code>vfmacc.vf</code> instructions preloads the data that will be used
later into the <code>cache</code>, significantly reducing
<code>cache miss</code>.</p>
<p>Initially, I was puzzled—how can the <code>compute time</code> and
<code>memory access time</code> overlap when the code seems to execute
sequentially? It wasn't until later that I understood the essence here,
which lies in the principle of <code>cacheline</code>. Indeed,
foundational knowledge is crucial!</p>
<h2 id="version-9-assembly-version-with-a-packed">Version 9: Assembly
Version with A Packed</h2>
<p><img src=/images/sgemm/data/result_9.png width=70% /></p>
<p>Based on previous experience, an attempt was made to
<code>pack</code> Matrix A, but surprisingly, the results were not very
good. A brief analysis suggests that the preload for Matrix A in this
version of the assembly code might not be optimized.</p>
<p>In the previous version, although A wasn't packed, there was preload
for A's 4 rows, which also addressed the pain point of cache miss for
Matrix A.</p>
<h1 id="conclusion">Conclusion</h1>
<p>To continue optimizing this operator, there is still much to be done,
such as rearranging pipelines in assembly.</p>
<p>As mentioned in the OpenPPL Course | RISC-V Technical Analysis, using
<code>vf</code> instructions can achieve 80% of the theoretical peak,
which is 4*80%, <code>3.2 GFLOPs</code>. I currently have only
<code>2.121 GFLOPs</code>, indicating there's still a lot of room for
optimization theoretically.</p>
<p>Furthermore, the RVV currently uses version 0.7.1, and it seems
there's still much work to be done in RVV instruction optimization, such
as the serious efficiency problem encountered with <code>vlw</code>.</p>
<p>In conclusion, working on these tasks has allowed me to learn a lot
from many experts, and I'm very grateful. I also hope this article can
help more people.</p>
<h1 id="acknowledgement">Acknowledgement</h1>
<ul>
<li><p><a href="https://github.com/flame/blislab">BLISlab: A Sandbox for
Optimizing GEMM</a></p>
<p>This project introduced me to how to optimize GEMM</p></li>
<li><p><a
href="https://github.com/surez-ok/blislab_riscv">riscv平台优化矩阵乘(基于blislab优化实践)</a></p>
<p>I conduct experiments and exploration based on this project</p></li>
<li><p>Thanks to Mr. Ding for your guidance.</p></li>
</ul>
<h1 id="references">References</h1>
<p><a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a></p>
<p><a href="http://riscvbook.com/">RISC-V-Reader</a></p>
<p><a
href="https://github.com/riscv/riscv-v-spec/releases">riscv-v-spec-0.7.1</a></p>
<p><a
href="https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf">Anatomy
of High-Performance Matrix Multiplication</a></p>
<p><a
href="https://developer.huawei.com/consumer/cn/forum/topic/0201105374686101528">编译器优化丨Cache优化</a></p>
<p><a
href="https://www.leiphone.com/category/yanxishe/Puevv3ZWxn0heoEv.html">OpenBLAS项目与矩阵乘法优化
| AI 研习社</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34204282">Roofline
Model与深度学习模型的性能分析</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/342923482">浅谈分块矩阵优化方法为什么有用</a></p>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>RISCV</tag>
        <tag>GEMM</tag>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Homogeneous_ShangHai</title>
    <url>/2023/08/04/99_Homogeneous-ShangHai/</url>
    <content><![CDATA[<h1 id="背景">0、背景</h1>
<h2 id="前年2021">0.0 前年（2021）</h2>
<p>我在北京的校园里，设计并制作了一个六重竹笋，<a
href="https://mp.weixin.qq.com/s/O6tBVdg4Saa4KtITM-UNng">《六重蓝笋成长记（上）》</a>和<a
href="https://mp.weixin.qq.com/s/6t3HY1REaKn-AUS1TxTOdQ">《六重蓝笋成长记（下）》</a>，开始体会到竹笋的乐趣，并立志于做更高重的笋。</p>
<h2 id="去年2022">0.1 去年（2022）</h2>
<p>我在上海的某个特殊时期，闲得实在是无聊，制作了 <a
href="https://mp.weixin.qq.com/s/cNCfhqoNB2wIC-5lAzIR3w">视频：从入门到夺笋</a>，以缓解不能出去玩的郁闷，并立志于做更高重的七重(七色彩虹)竹笋。</p>
<span id="more"></span>
<h2 id="今年2023">0.2 今年（2023）</h2>
<p>随着我做高重蓝笋的念头越来越强烈，便开始着手准备这件事情。</p>
<p>非常重要的问题来了——做几重？</p>
<p><img src="/images/ingress/homogeneous_table.png" width="60%"></p>
<ul>
<li><p>我上一次做单人六重，耗时 5 个小时，行程 31.37 公里。</p></li>
<li><p><code>@ALHDLIOX</code> 上一次做<a
href="https://mp.weixin.qq.com/s/mqPZ0Rjx0QR3yCGgSz1Hxg">单人七重</a>，耗时
35 小时，行程 184.69 公里</p></li>
</ul>
<p>经过鲁莽的思考，我觉得九这个数字很好，代表数之极。做九重！</p>
<p>正所谓：</p>
<blockquote>
<p><strong>九重开，灵之斿，垂惠恩，鸿祜休。</strong></p>
</blockquote>
<p>不对，做九重的 AP 只够我转生半次的，我要十重！</p>
<h1 id="获取数据">1、获取数据</h1>
<h2 id="三年前的数据">1.1 三年前的数据</h2>
<p>我想起 <code>@Nanoape</code> 曾在 <a
href="https://mp.weixin.qq.com/s/ewEHaY8gKo2o31ApP5T7GQ">来刷新新的个人
Dronenet 飞行记录吧</a> 一文中提到他拿到了全球所有 Portal 的数据。共
1.31G，Portal 总数 11,475,124，数据统计截止到 2020.06.12。</p>
<p>于是我下载了portal数据<a
href="https://mega.nz/file/PsYnkCoD#ujlASFyomu1ZT8WZXi1_57ogH3pneGRXT2EC156u-lg">Mega</a>(这个网盘真良心啊，不限速，不强制登陆)</p>
<p>然而三年过去了，这一数据已经过时，比起得不到最优解，我更担心的是有的
portal 移动了位置，致使整个计划失败。</p>
<h2 id="获取新数据">1.2 获取新数据</h2>
<p>我本想通过某个脚本来抓取上海区域的所有 portal
的信息，但是抓到了这个信息：</p>
<blockquote>
<p>{'message': 'Please, dont do so. Contact <span class="citation"
data-cites="Lanched">@Lanched</span> insted.'}</p>
</blockquote>
<p>谢天谢地，这似乎制止了我违反<a
href="https://nianticlabs.com/terms">ToS</a>的行为。</p>
<p>于是我联系了
<code>@Lanched</code>，在他的热心帮助下，我顺利拿到了一份整个上海portal的信息。</p>
<blockquote>
<p>数据截止到 2023-7-30 日，区域范围是北纬 30.662225-31.879785，东经
120.860389-121.969865，基本包含整个上海区域，一共 30,179 个potal。</p>
</blockquote>
<p>在与他的交流过程中得知，全球 portal 的数量已经从2020年的 11,475,124
个，2023年上涨到了 18,284,480 个，增长为原来的
159%。看这个数据很可观，依稀还是能看到它茁壮成长的影子的（拜托拜托，再撑十年吧）。</p>
<p>最后，我向他表示感谢，并表示 以后有需要我的地方我会尽全力帮忙，
得到的回复却是:</p>
<blockquote>
<p><code>I dont think, you can help me in ingress - anyway i cant play it anymore.</code></p>
</blockquote>
<p>我看了他的名字是
<code>Alex Zverev</code>，猜测到他的国籍，顿时心里五味杂陈，但愿以后游戏可以更纯粹一些。</p>
<h1 id="分析数据">2、分析数据</h1>
<p>写了一些小脚本，对拿到的数据进行了分析，结果如下：</p>
<p><img src="/images/ingress/shanghai_portals.gif" width="80%"></p>
<ul>
<li>上海的经纬度是东经120°52′-122°12′，北纬30°40′-31°53′之间。</li>
<li>数据截止到2023-7-30日，区域范围是北纬30.662225-31.879785，东经120.860389-121.969865，基本包含整个上海区域</li>
<li>共30179个potal。</li>
</ul>
<p><img src="/images/ingress/part_of_shanghai.png" width="60%"></p>
<p>我喜欢密集的区域！于是进一步将经纬度限制在比较密集的
北纬30.988-31.430，东经121.249-121.718，一共22431个portal:</p>
<p><img src="/images/ingress/part_of_shanghai_gif.gif" width="80%"></p>
<pre><code>上边长 =  44472m, 下边长 =  44679m
左边长 =  48716m, 右边长 =  48716m</code></pre>
<h1 id="计算竹笋的解">3、计算竹笋的解</h1>
<h2 id="思路一求助大厉害">3.1 思路一：求助大厉害</h2>
<p>又一次翻出了 <code>@NanoApe</code> 的<a
href="https://github.com/Konano/Ingress-Field-Design">大厉害</a>，又一次感叹高三的时候能写出这代码实在是太太太强了！</p>
<p>稍微修改了一下代码，使其能够兼容读取非json形式的portal信息。这时我突然有一个大胆的想法——如果不对竹笋的计算重数进行限制，将整个上海的
potals 送进代码，看一看整个魔都能容纳几重笋？</p>
<p>对一些代码进行修改，比如从<code>栈</code>上分配内存改为从<code>堆</code>上分配内存，增大对<code>portal</code>和<code>field</code>数量的限制等，最终发现这个方案不太可行——</p>
<p>以筛选出的 22431 个portal为例，这可以形成 <span
class="math inline">\(C^3_{22431} = 1880773814095\)</span> 个
field，如果对每一个 field
都进行保存计算，这比ChatGPT的参数量都多一个量级，直接劝退。</p>
<h2 id="思路二分而治之">3.2 思路二：分而治之</h2>
<p>采用<code>分治思想(Divide and Conquer)</code>，先选出若干小区域，对每个小区域使用大厉害进行竹笋计算，然后在所有计算结果中寻求高重竹笋的解。</p>
<p><img src="/images/ingress/divided_img.gif" width="80%"></p>
<p>如上，每一小块区域包含了1000个portal，通过对每个小区域计算竹笋的解，然后对所有的解进行拼接，拼出一个大竹笋。</p>
<p>然后就是兴致勃勃的准备进行暴力求解，辛苦电脑跑几天，然而代码跑起来突然意识到这条路其实是行不通的。</p>
<p>大致原因就是，竹笋确实是能获得很多解，但是没法拼大竹笋啊。于是也放弃了。</p>
<p>（不过这个代码在后面发挥了很大的作用）</p>
<h2 id="思路三不求优解但求有解">3.3 思路三：不求优解，但求有解</h2>
<p>灵感来源于以前聊天时 <code>@hydronephrosis</code> 的撒豆子说法：</p>
<blockquote>
<p>我喜欢这样理解field的形成：类似于学概率的时候往桌布上撒豆子的模式，在一块三角形的布上撒一颗豆子(po)，连接所在区域的三个顶点，把布分成三个小区域；然后再撒下一颗，连接所在小区域的三个顶点，以此类推，直到所有po都囊括，形成完美多重。根据撒豆子顺序的不同，同样的一组po可以形成成千上万种不同的完美多重，每一个完美多重也可以通过这种形式解读成唯一的一系列po的顺序。这种多样性正是完美多重的魅力所在。如果每一次不止撒一颗，而是根据现有区域的数量来撒相同数量的一把豆子，使得刚好每个区域都落下一颗，就是竹笋了。如此竟与你看field的三维视角有异曲同工之妙。</p>
</blockquote>
<blockquote>
<p>每撒一颗豆子都是多加三个field和三条link，也可以解释为什么完美多重的结构改变不影响AP和field的数量。</p>
</blockquote>
<p>以及<code>@HesalTJ</code>的补充：</p>
<blockquote>
<p>在游戏内实现的话就是，利用在Field之下连Link的新规则，在形成第一重之后依次抵达下一级的内点射3out
3out 3out…
实际上执行起来会绕很多圈，但是它的美丽在于，如果用Intel截了仅Field的图一帧一帧地做成录像，那么你会看到的是一个填充色由浅逐渐逐渐变深的三角形，每一帧都有一个小三角形的深度+1重，每次提升了重数之后新的小三角形面积变小数量×3：
从一重+1帧变为二重、+3帧变为三重、+9帧变为四重、+27帧变为五重…</p>
</blockquote>
<p>可惜的是，按照这种思路写代码比较难拿到最优解，做笋的时候会多走很多路。不过，不求优解，但求有解嘛:)</p>
<p>而且这种解法是我最喜欢的竹笋形状，我心目中的完美竹笋——</p>
<p><strong>高重的顶点位于次高重field的重心位置，这样的竹笋看起来更加规则，非常完美！</strong></p>
<blockquote>
<p>备注：三条中线相交的点叫做重心。</p>
</blockquote>
<p>针对这种思路，po场的分布均匀程度更重要，于是进一步将经纬度限制在分布比较密集且均匀的
北纬31.18-31.29，东经121.40-121.54，一共 8704 个 portal.</p>
<pre><code>上边长 =  13296m, 下边长 =  13312m
左边长 =  12226m, 右边长 =  12226m</code></pre>
<p>于是，我得到了上海的一个<strong>八重竹笋</strong>的解：</p>
<p><img src="/images/ingress/8_layer_homogeneous.gif" width="80%"></p>
<p>当然，这是一个假的。它甚至连一个6重竹笋都不是。</p>
<p>于是换一个思路，不再寻找三角形的重心位置，而是能够
<strong>将po的数量三等分的位置</strong>。这里就不展示了，因为跑出来个更不靠谱的结果。</p>
<p><strong>想来想去，还是得依靠 Nano 的大厉害啊！</strong></p>
<p>其实，<strong>思路二：分而治之</strong>
的方法其实是可行的，只不过我的那个方法实在是蠢到家了。</p>
<p>而且，要想计算高重竹笋，分而治之大概是最好的方法。</p>
<h1 id="搁置">4、搁置</h1>
<p>暂时没有得到8重解，这件事情就<strong>搁置</strong>了两个月。</p>
<p>11月的一天早上，<span class="citation"
data-cites="Hesal">@Hesal</span> 发给了我一个兑换码，兑换出来 800 个
la，我瞬间狂喜，因为这可以通过运动胶囊合成竹笋党最爱的
SBUL，也就在此刻，上海大竹笋计划又一次被我记起。</p>
<p>然后，这件事情又<strong>搁置</strong>了近两个月。</p>
<p>12月底，在 <code>@NanoApe</code> 的频道看到了这么一条消息——</p>
<blockquote>
<p>六年前的 2017 年，26 名北京 Res Agents
在金融街完成了全球最快六重蓝竹笋，12 月 24 日凌晨 02 点 17 分到凌晨 02
点 58 分，耗时 39 分钟。</p>
</blockquote>
<p><img src="/images/ingress/homogeneous_L6_2017.gif" width="80%"></p>
<p>想一想，26位玩家，在凌晨，齐心协力种出竹笋，真是一件浪漫的事情啊！</p>
<p>然后，这件事情又<strong>搁置</strong>了近两个月。</p>
<p>2024年2月底，我终于用光了那 800 个
la，上海大竹笋计划又一次被我记起！</p>
<h1 id="可行性分析">5、可行性分析</h1>
<h2 id="场内连线是否能实现真竹笋">5.1 场内连线是否能实现真竹笋？</h2>
<p>大约在2022年5月份的时候，已经允许 field 内 500米 的 link
了，那时候我产生过类似的疑问，于是做过相关实验。</p>
<p>结论是：在field内往顶点po射，是能够起到顶点po往里面射一样的效果的（即层数增加一致），所以不需要担心这种规则下会做一个假竹笋。</p>
<p>以前和平周活动可以允许长度不超过 8 公里的 Link 在 Field
下面建立，这非常利好7重以上的竹笋。 但是仍然需要注意一点：</p>
<ul>
<li>7重笋要保证封口时最近的 65 - 40 - 2 = 23 条 link 小于 8 公里</li>
<li>8重笋要保证封口时最近的 129 - 40 - 2 = 78 条 link 小于 8 公里</li>
<li>9重笋要保证封口时最近的 257 - 40 - 2 = 215 条 link 小于 8 公里</li>
</ul>
<h2 id="上海能否出现8重竹笋">5.2 上海能否出现8重竹笋？</h2>
<p>目前8重应该是可以的，尤其按照最近半年来上海portal肉眼可见的增长速度，这个难度是降低的。</p>
<p>但是按照现在双方特工们的活跃程度以及极个别的不和谐因素，出现高重竹笋的困难度还是非常高的。</p>
<h2 id="玩家够多吗">5.3 玩家够多吗？</h2>
<p>上海这边做竹笋尤其需要速战速决，这就必须要多人合作共同完成。</p>
<p>比如8重竹笋，要想在几个小时内完成至少要动用10名以上有竹笋经验的特工，</p>
<p>好在上海这边活跃玩家有很多，但是有竹笋经验的玩家比较少一些。尤其这两年，感觉已经很少听到竹笋的大新闻了，竹笋教快要没落了哇。</p>
<h1 id="起航">6、起航</h1>
<p>遍地开花</p>
<hr />
<p>至于上海高重竹笋计划，大概会在某个月黑风高夜顺利完成吧。</p>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <tags>
        <tag>ingress</tag>
        <tag>Homogeneous</tag>
      </tags>
  </entry>
  <entry>
    <title>Telegram bot</title>
    <url>/2024/02/17/98_Telegram_bot/</url>
    <content><![CDATA[<p>最近学习一些telegram bot，记录一下</p>
<span id="more"></span>
<h1 id="installing">Installing</h1>
<p>安装 python-telegram-bot 库</p>
<p><code>pip install python-telegram-bot --upgrade</code></p>
<p>安装完毕 &gt; Successfully installed h11-0.14.0 httpcore-1.0.3
httpx-0.26.0 python-telegram-bot-20.8</p>
<h1 id="bot-申请">bot 申请</h1>
<p>私聊<a href="https://t.me/BotFather">BotFather</a>,</p>
<ul>
<li>/start</li>
<li>/newbot</li>
<li>Alright, a new bot. How are we going to call it? Please choose a
name for your bot.</li>
<li>IngressHomogeneous</li>
<li>Good. Now let's choose a username for your bot. It must end in
<code>bot</code>. Like this, for example: TetrisBot or tetris_bot.</li>
<li>IngressHomogeneousBot</li>
<li>Done! Congratulations on your new bot. You will find it at
t.me/IngressHomogeneousBot. You can now add a description, about section
and profile picture for your bot, see /help for a list of commands. By
the way, when you've finished creating your cool bot, ping our Bot
Support if you want a better username for it. Just make sure the bot is
fully operational before you do this.</li>
</ul>
<p>妥善保存最后的 API Token。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">You can control me by sending these commands:</span><br><span class="line"></span><br><span class="line">/newbot - create a new bot</span><br><span class="line">/mybots - edit your bots</span><br><span class="line"></span><br><span class="line">Edit Bots</span><br><span class="line">/setname - change a bot&#x27;s name</span><br><span class="line">/setdescription - change bot description</span><br><span class="line">/setabouttext - change bot about info</span><br><span class="line">/setuserpic - change bot profile photo</span><br><span class="line">/setcommands - change the list of commands</span><br><span class="line">/deletebot - delete a bot</span><br><span class="line"></span><br><span class="line">Bot Settings</span><br><span class="line">/token - generate authorization token</span><br><span class="line">/revoke - revoke bot access token</span><br><span class="line">/setinline - toggle inline mode</span><br><span class="line">/setinlinegeo - toggle inline location requests</span><br><span class="line">/setinlinefeedback - change inline feedback settings</span><br><span class="line">/setjoingroups - can your bot be added to groups?</span><br><span class="line">/setprivacy - toggle privacy mode in groups</span><br><span class="line"></span><br><span class="line">Web Apps</span><br><span class="line">/myapps - edit your web apps</span><br><span class="line">/newapp - create a new web app</span><br><span class="line">/listapps - get a list of your web apps</span><br><span class="line">/editapp - edit a web app</span><br><span class="line">/deleteapp - delete an existing web app</span><br><span class="line"></span><br><span class="line">Games</span><br><span class="line">/mygames - edit your games</span><br><span class="line">/newgame - create a new game</span><br><span class="line">/listgames - get a list of your games</span><br><span class="line">/editgame - edit a game</span><br><span class="line">/deletegame - delete an existing game</span><br></pre></td></tr></table></figure>
<h1 id="获取我的-id">获取我的 id</h1>
<p>每个 tg 用户都有一串唯一标识，即为 user_id，可以私聊 <a
href="https://t.me/kmuav2bot"><span class="citation"
data-cites="kmua">@kmua</span></a> 发送 /id 来获取它</p>
<p>跟着<a
href="https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions---Your-first-Bot">Your-first-Bot</a>这个来就好了，理解的话可以参考<a
href="https://krau.top/posts/tg-bot-dev-note-kmua">使用Python写一个Telegram
bot吧|Telegram bot教程</a>。</p>
<p>感觉整体比较好理解，接下来就是赋予其灵魂了～</p>
<hr />
<p>参考文章：</p>
<p><a
href="https://docs.python-telegram-bot.org/en/v20.8/">python-telegram-bot.org</a></p>
<p><a
href="https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions---Your-first-Bot">Your-first-Bot</a></p>
<p><a
href="https://krau.top/posts/tg-bot-dev-note-kmua">使用Python写一个Telegram
bot吧|Telegram bot教程</a></p>
]]></content>
      <tags>
        <tag>ingress</tag>
        <tag>Homogeneous</tag>
        <tag>Telegram bot</tag>
      </tags>
  </entry>
  <entry>
    <title>tflite-micro</title>
    <url>/2023/10/27/91_tflite-micro/</url>
    <content><![CDATA[<p>最近又搞了一次tflite-micro，上一次搞还是在两年前，那时候简直是个工程小白，遇到个问题就卡住了。这一次相对比较顺利，记录一遍过程。</p>
<span id="more"></span>
<p>论文：<a href="https://arxiv.org/abs/2010.08678">TensorFlow Lite
Micro: Embedded Machine Learning on TinyML Systems</a></p>
<h1 id="克隆仓库">克隆仓库</h1>
<p>在tensorflow的<a
href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro">git里面</a>，看到git已经迁移至一个单独的<a
href="https://github.com/tensorflow/tflite-micro">仓库</a>。</p>
<ul>
<li>As of June 25, 2021, the TFLM codebase has moved to a <a
href="https://github.com/tensorflow/tflite-micro">stand-alone github
repository</a>.</li>
</ul>
<p>于是在ubuntu执行命令：</p>
<p><code>git clone https://github.com/tensorflow/tflite-micro.git</code></p>
<p>此时<code>commit 7ac0b662b6402c1fccc8ec8d1db84f0f34dd4a85</code></p>
<h1 id="编译静态库">编译静态库</h1>
<p>求助了ChatGPT 4.0：</p>
<p>「在这个 https://github.com/tensorflow/tflite-micro ，
怎么编译静态库？」</p>
<p>在TensorFlow Lite
Micro的GitHub仓库中，你可以使用以下步骤来编译静态库：</p>
<ol type="1">
<li>首先，你需要克隆仓库：</li>
</ol>
<p><code>git clone https://github.com/tensorflow/tflite-micro.git</code></p>
<ol start="2" type="1">
<li>进入克隆下来的仓库目录：</li>
</ol>
<p><code>cd tflite-micro</code></p>
<ol start="3" type="1">
<li>使用make命令编译静态库：</li>
</ol>
<p><code>make -f tensorflow/lite/micro/tools/make/Makefile TARGET=&lt;TARGET&gt; TARGET_ARCH=&lt;ARCH&gt; microlite</code></p>
<p>在上述命令中，<code>&lt;TARGET&gt;</code>应替换为你的目标平台（如<code>linux</code>），<code>&lt;ARCH&gt;</code>应替换为你的目标架构（如<code>x86_64</code>）。这将在<code>tensorflow/lite/micro/tools/make/gen/&lt;TARGET&gt;_&lt;ARCH&gt;/lib/</code>目录下生成一个名为<code>libtensorflow-microlite.a</code>的静态库。</p>
<p>请注意，你可能需要根据你的具体需求和环境来修改这些步骤。例如，你可能需要安装额外的依赖库，或者修改makefile来包含你需要的特性。</p>
<p>真是太棒了，于是我执行指令：</p>
<p><code>make -f tensorflow/lite/micro/tools/make/Makefile TARGET=linux TARGET_ARCH=x86_64 microlite</code></p>
<p>在 <code>make/download/</code>
目录下，分别下载了<code>flatbuffers</code>、<code>gemmlowp</code>、<code>kissfft</code>、<code>pigweed</code>、<code>ruy</code>。</p>
<p>编译完成后，得到了静态库文件</p>
<p><code>ar: creating gen/linux_x86_64_default/lib/libtensorflow-microlite.a</code></p>
<h1 id="编写-demo-debug">编写 demo &amp; debug</h1>
<p>仿照helloworld工程写了一个小demo</p>
<h2 id="遇到问题">遇到问题：</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Failed to allocate tail memory. Requested: 2760, available 1376, missing: 1384</span><br><span class="line">Failed to allocate memory for context-&gt;eval_tensors, 2760 bytes required</span><br><span class="line">Failed starting model allocation.</span><br></pre></td></tr></table></figure>
<h2 id="解决方法">解决方法：</h2>
<p>先暂时增大ArenaSize，后面再根据实际使用情况调整回来：</p>
<p><code>constexpr int kTensorArenaSize = 300000;</code></p>
<h2 id="遇到问题-1">遇到问题：</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Didn&#x27;t find op for builtin opcode &#x27;LEAKY_RELU&#x27;</span><br><span class="line">Failed to get registration from op code LEAKY_RELU</span><br><span class="line"></span><br><span class="line">Segmentation fault (core dumped)</span><br></pre></td></tr></table></figure>
<h2 id="解决方法-1">解决方法：</h2>
<p>增加算子注册
<code>TF_LITE_ENSURE_STATUS(op_resolver.AddLeakyRelu());</code></p>
<h2 id="遇到问题-2">遇到问题</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Couldn&#x27;t register builtin op #98, resolver size </span><br><span class="line">is too small (1).</span><br></pre></td></tr></table></figure>
<h2 id="解决方法-2">解决方法</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">using HelloWorldOpResolver = tflite::MicroMutableOpResolver&lt;1&gt;;</span><br></pre></td></tr></table></figure>
<p>这里的1代表注册个数，草率了，应该<code>op_resolver</code> add
了几个这里就写几的。</p>
<h2 id="遇到问题-3">遇到问题</h2>
<p>一旦执行到<code>interpreter.input(0)-&gt;data.f[0] = 1.f;</code>就段错误。</p>
<h2 id="解决办法">解决办法</h2>
<p><code>CFLAGS = -DTF_LITE_STATIC_MEMORY</code></p>
<h1 id="进一步减小库体积">进一步减小库体积</h1>
<p>为了压缩体积，<code>BUILD_TYPE</code>使用了<code>release</code>进行编译，这期间会遇到<code>MicroPrintf</code>不支持的问题（release_with_logs是可以的），进行一些注释就可以。</p>
<p>以及进行<code>-Os</code>编译，可以减少很多体积占用。</p>
<p><a href="https://buymeacoffee.com/zhaodongyu">
<img src="/images/bmc_qr.jpg" width="50%" style="display: inline-block; vertical-align: middle;" />
</a></p>
]]></content>
      <tags>
        <tag>notes</tag>
        <tag>tech</tag>
        <tag>tflite-micro</tag>
        <tag>TinyML</tag>
      </tags>
  </entry>
  <entry>
    <title>Marlin代码解读</title>
    <url>/2024/09/26/104_marlin/</url>
    <content><![CDATA[<p>作为一个不会cuda的小白，研究完<a
href="https://github.com/IST-DASLab/marlin"><code>marlin</code></a>算子之后神清气爽，</p>
<p>【长文预警 &amp; 多图预警】</p>
<span id="more"></span>
<h1 id="准备工作">准备工作</h1>
<h2 id="分析工具">分析工具</h2>
<p>先把工具学习了，学习nsight-compute。NVIDIA Nsight Compute
是一款适用于 CUDA
应用程序的交互式内核分析器。它通过用户界面和命令行工具提供详细的性能指标和
API 调试。</p>
<p>https://docs.nvidia.com/nsight-compute/NsightCompute/index.html</p>
<p>https://www.bilibili.com/video/BV13w411o7cu/?vd_source=a706cf2a9a33f4c298a6ef4764cea661</p>
<h2 id="搭建环境">搭建环境</h2>
<ul>
<li><code>git clone https://github.com/IST-DASLab/marlin.git</code></li>
<li><code>export TORCH_CUDA_ARCH_LIST="8.6"</code>
(这里要根据实际的版本来)</li>
<li><code>pip install .</code></li>
<li><code>python test.py</code></li>
</ul>
<h1 id="代码分析">代码分析</h1>
<p>在 <strong>test.py</strong> 中起一个 m = 128, sms = 5 的
<code>marlin.mul</code></p>
<p>即 <code>self.run_problem(128, 768, 256, -1, -1)</code>。</p>
<p>⚠️注意： 1. 本文全部使用这个case的数值进行带入。即</p>
<ul>
<li>m = 128</li>
<li>k = 256</li>
<li>n = 768</li>
</ul>
<ol start="2" type="1">
<li>听 Ding
大佬的话，这个算子不要老是按照拿到的数进行理解，要结合代码进行理解。</li>
</ol>
<p>接下来进入 <strong>marlin_cuda_kernel.cu</strong> 文件的
<code>marlin_cuda</code> 函数逐步分析。</p>
<h2 id="int-marlin_cuda函数">int marlin_cuda()函数</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> tot_m = prob_m;</span><br><span class="line"><span class="type">int</span> tot_m_blocks = ceildiv(tot_m, <span class="number">16</span>);</span><br><span class="line"><span class="type">int</span> pad = <span class="number">16</span> * tot_m_blocks - tot_m;</span><br></pre></td></tr></table></figure>
<p>由于传入的<code>prob_m = 128</code>，则
<code>tot_m = 128</code>，将其除以16并进行上取整，得<code>tot_m_blocks = 8</code>，这里是整除，没有进行pad，所以<code>pad</code>计算出来也就是0。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (sms == <span class="number">-1</span>)</span><br><span class="line">  cudaDeviceGetAttribute(&amp;sms, cudaDevAttrMultiProcessorCount, dev);</span><br></pre></td></tr></table></figure>
<p>这里由于指定了<code>sms = 5</code>，所以不会走后面的代码，否则会获取指定
CUDA 设备（dev）的多处理器数量。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (thread_k == <span class="number">-1</span> || thread_n == <span class="number">-1</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (prob_m &lt;= <span class="number">16</span>) &#123;</span><br><span class="line">    <span class="comment">// For small batchizes, better partioning is slightly more important than better compute utilization</span></span><br><span class="line">    thread_k = <span class="number">128</span>;</span><br><span class="line">    thread_n = <span class="number">128</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    thread_k = <span class="number">64</span>;</span><br><span class="line">    thread_n = <span class="number">256</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如注释所说：对于小批量处理来说，<strong>较好的分区(better
partioning)</strong> 比 <strong>较好的计算资源利用(better compute
utilization)</strong> 稍微更重要。​在目前的例子中，prob_m =
128，所以得到的分区是</p>
<ul>
<li><code>thread_k = 64</code></li>
<li><code>thread_n = 256</code></li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> thread_k_blocks = thread_k / <span class="number">16</span>;</span><br><span class="line"><span class="type">int</span> thread_n_blocks = thread_n / <span class="number">16</span>;</span><br><span class="line"><span class="type">int</span> group_blocks = (groupsize == <span class="number">-1</span>) ? <span class="number">-1</span> : groupsize / <span class="number">16</span>;</span><br><span class="line"><span class="type">int</span> blocks = sms;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>thread_k_blocks = 64/16 = 4</code></li>
<li><code>thread_n_blocks = 256/16 = 16</code></li>
</ul>
<blockquote>
<p>为什么是除以16呢？因为Marlin Kernel使用的Tensor Core指令为
<code>m16n8k16</code>
size的MMA指令，所以一次MMA指令执行的矩阵size为m16n8k16。而Marlin
Kernel在设计的时候，以n方向的2次MMA计算的矩阵作为一个基本的sub_tile，即sub_tile的尺寸为<code>m16n16k16</code>。——源自<a
href="https://zhuanlan.zhihu.com/p/707470647">Marlin
W4A16&amp;W4A8代码走读</a></p>
</blockquote>
<p>接下来进入沿着m方向的循环</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; tot_m_blocks; i += <span class="number">4</span>) &#123;</span><br><span class="line">  <span class="type">int</span> thread_m_blocks = tot_m_blocks - i;</span><br><span class="line">  prob_m = tot_m - <span class="number">16</span> * i;</span><br><span class="line">  <span class="type">int</span> par = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> (thread_m_blocks &gt; <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="comment">// Note that parallel &gt; 1 currently only works for inputs without any padding</span></span><br><span class="line">    par = (<span class="number">16</span> * thread_m_blocks - pad) / <span class="number">64</span>;</span><br><span class="line">    <span class="keyword">if</span> (par &gt; max_par)</span><br><span class="line">      par = max_par;</span><br><span class="line">    prob_m = <span class="number">64</span> * par;</span><br><span class="line">    i += <span class="number">4</span> * (par - <span class="number">1</span>);</span><br><span class="line">    thread_m_blocks = <span class="number">4</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>之前算过 tot_m_blocks=8，则</p>
<ul>
<li><code>i = 0</code></li>
<li><code>thread_m_blocks = 8</code></li>
<li><code>prob_m = 128 - 16 * 0 = 128</code></li>
</ul>
<p>由于 <code>thread_m_blocks &gt; 4</code>，则
<code>par = (16 * 8 - 0) / 64 = 2</code>，这里可以看出来，<strong>m方向最大的基本执行单元是
64</strong>。</p>
<p><code>prob_m = 64 * par = 128</code>, 这里是做了pad后的 prob_m。</p>
<p>然后<code>i += 4 * (2 - 1)</code>，此时i = 4。</p>
<pre><code>我认为这里的(par - 1)的原因就是因为 for 循环里对 i 也有一个增加操作，感觉二者结合起来看比较容易理解（或者我认为改成i += 4 * par, for循环里不做i增操作更好理解一些）。</code></pre>
<p>此时，得到<code>thread_m_blocks = 4</code>。</p>
<p><code>i</code>进入for循环的<code>i+=4</code>，不再满足循环条件，循环结束。</p>
<pre><code>这里的par是2，如果par超过了max_par，那么应该就会多循环几次了。</code></pre>
<p>这一段的作用就是确定了m方向上的blocks = 4 和 parallel = 2。</p>
<p>目前的结果是</p>
<ul>
<li>thread_m_blocks = 4</li>
<li>thread_n_blocks = 16</li>
<li>thread_k_blocks = 4</li>
<li>group_blocks = -1</li>
</ul>
<p>于是进入<code>CALL_IF(4,16,4,-1)</code></p>
<p>各个参数都确定下来了，先画个图整理一下：</p>
<p><img src="/images/marlin/0_gemm.png" width="95%"></p>
<p>以M=128, K=256, N=768为例，每个小方格的大小是64x64</p>
<h2 id="global-void-marlin函数"><strong>global</strong> void
Marlin()函数</h2>
<p>开始核心代码 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> parallel = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (prob_m &gt; <span class="number">16</span> * thread_m_blocks) &#123;</span><br><span class="line">  parallel = prob_m / (<span class="number">16</span> * thread_m_blocks);</span><br><span class="line">  prob_m = <span class="number">16</span> * thread_m_blocks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
这里是对prob_m进行了限制，如前文所说，<strong>m方向最大的基本执行单元是64</strong>，对于较大的
GEMM，并行运行多个batch大小为 64 的版本。</p>
<p>所以这几句的结果是</p>
<ul>
<li>thread_m_blocks = 4</li>
<li>parallel = 2</li>
<li>prob_m = 64</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> k_tiles = prob_k / <span class="number">16</span> / thread_k_blocks;</span><br><span class="line"><span class="type">int</span> n_tiles = prob_n / <span class="number">16</span> / thread_n_blocks;</span><br><span class="line"><span class="type">int</span> iters = ceildiv(k_tiles * n_tiles * parallel, gridDim.x);</span><br></pre></td></tr></table></figure>
<p>得到k_tiles = 4, n_tiles = 3, 所以目前一共有 k_tiles * n_tiles *
parallel =
24个tile。而SM=5，因此，每个block需要计算的tile数量为5（24除以5后上取整，最后一个block只需要计算4个tile），即iters
= 5. 在讲接下来的部分前先讲一下marlin里面的stripe概念。</p>
<p><img src="/images/marlin/1_stripe.png" width="95%"></p>
<p><a href="https://www.arxiv.org/pdf/2408.11743">图片来源</a></p>
<p>条带分区（Striped
Partitioning）是一种在并行计算中常用的技术，特别是在大型矩阵乘法计算中，通过该技术可以<strong>提高负载均衡并最小化计算过程中的开销</strong>。</p>
<p>MARLIN内核中，条带分区是指由多个SM（流式多处理器）处理不同的矩阵“条带”，这意味着<strong>每个SM负责处理多块矩阵数据</strong>，这种分区方法保证了工作负载在处理器之间的<strong>均匀分布</strong>。</p>
<p>条带分区的核心思想是：</p>
<ul>
<li>工作均衡分配：通过跨列或跨行分配条带，可以确保处理器均匀分配任务，防止部分处理器过载而其他处理器闲置。</li>
<li>减少全局同步开销：由于条带分区将任务均匀分配给各个处理器，减少了全局同步的需求，降低了并行计算中的通信开销。</li>
<li>提高缓存和内存效率：通过分割矩阵数据到条带，系统能够更有效地使用GPU缓存和内存带宽，从而最大化内存加载的吞吐量，并提高整体计算效率。</li>
</ul>
<p>这种方法有助于在不同的GPU架构中优化计算性能，特别是在大规模深度学习模型的推理任务中。</p>
<p>前文讲过，每个block需要计算的tile数量为5。这里对B画个图表示一下：</p>
<p><img src="/images/marlin/2.png" width="95%"></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> slice_row = (iters * blockIdx.x) % k_tiles;</span><br><span class="line"><span class="type">int</span> slice_col_par = (iters * blockIdx.x) / k_tiles;</span><br></pre></td></tr></table></figure>
<p>这两句有点抽象，结合上面的图，这里的计算其实是确定了每个block起始位置的纵坐标和横坐标。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>block 0</th>
<th>block 1</th>
<th>block 2</th>
<th>block 3</th>
<th>block 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>slice_row</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>0</td>
</tr>
<tr class="even">
<td>slice_col_par</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>5</td>
</tr>
</tbody>
</table>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// We can easily implement parallel problem execution by just remapping indices and advancing global pointers</span></span><br><span class="line"><span class="keyword">if</span> (slice_col_par &gt;= n_tiles) &#123;</span><br><span class="line">  A += (slice_col_par / n_tiles) * <span class="number">16</span> * thread_m_blocks * prob_k / <span class="number">8</span>;</span><br><span class="line">  C += (slice_col_par / n_tiles) * <span class="number">16</span> * thread_m_blocks * prob_n / <span class="number">8</span>;</span><br><span class="line">  locks += (slice_col_par / n_tiles) * n_tiles;</span><br><span class="line">  slice_col = slice_col_par % n_tiles;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只需重新映射索引(remapping indices)和推进全局指针(advancing global
pointers)，就可以轻松实现并行问题执行。</p>
<p>比如这里的 <code>n_tiles = 3</code>，那对于 block 3 和 block 4
而言，要处理的就是第二个parallel，所以要推进一下A和C的指针。</p>
<h2 id="init_slice函数">init_slice()函数</h2>
<p>接下来看比较容易迷惑的部分，如果不想细看这部分代码，可以直接看结论
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Compute all information about the current slice which is required for synchronization.</span></span><br><span class="line"><span class="keyword">auto</span> init_slice = [&amp;] () &#123;</span><br><span class="line">  slice_iters = iters * (blockIdx.x + <span class="number">1</span>) - (k_tiles * slice_col_par + slice_row);</span><br><span class="line">  <span class="keyword">if</span> (slice_iters &lt; <span class="number">0</span> || slice_col_par &gt;= n_tiles * parallel)</span><br><span class="line">    slice_iters = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (slice_iters == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span> (slice_row + slice_iters &gt; k_tiles) </span><br><span class="line">    slice_iters = k_tiles - slice_row;</span><br><span class="line">  slice_count = <span class="number">1</span>;</span><br><span class="line">  slice_idx = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> col_first = iters * ceildiv(k_tiles * slice_col_par, iters);</span><br><span class="line">  <span class="keyword">if</span> (col_first &lt;= k_tiles * (slice_col_par + <span class="number">1</span>)) &#123;</span><br><span class="line">    <span class="type">int</span> col_off = col_first - k_tiles * slice_col_par;</span><br><span class="line">    slice_count = ceildiv(k_tiles - col_off, iters);</span><br><span class="line">    <span class="keyword">if</span> (col_off &gt; <span class="number">0</span>)</span><br><span class="line">      slice_count++;</span><br><span class="line">    <span class="type">int</span> delta_first = iters * blockIdx.x - col_first;</span><br><span class="line">    <span class="keyword">if</span> (delta_first &lt; <span class="number">0</span> || (col_off == <span class="number">0</span> &amp;&amp; delta_first == <span class="number">0</span>))</span><br><span class="line">      slice_idx = slice_count - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      slice_idx = slice_count - <span class="number">1</span> - delta_first / iters;</span><br><span class="line">      <span class="keyword">if</span> (col_off &gt; <span class="number">0</span>)</span><br><span class="line">        slice_idx--;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (slice_col == n_tiles) &#123;</span><br><span class="line">    A += <span class="number">16</span> * thread_m_blocks * prob_k / <span class="number">8</span>;</span><br><span class="line">    C += <span class="number">16</span> * thread_m_blocks * prob_n / <span class="number">8</span>;</span><br><span class="line">    locks += n_tiles;</span><br><span class="line">    slice_col = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/marlin/3.png" width="95%"></p>
<p><code>slice_iters</code></p>
<ul>
<li>当前slice中的线程块(threadblock)块数</li>
<li>block 0有4个，要迭代4次，block 1有3个，要迭代3次，依此类比</li>
</ul>
<p><code>slice_count</code></p>
<ul>
<li>当前slice中活跃线程块(active threadblock)的总数</li>
<li>第一个slice有1个（只有block 0），第二个slice有2个（有block 0和block
1）</li>
</ul>
<p><code>slice_idx</code></p>
<ul>
<li>当前slice中的线程块索引(index of threadblock)</li>
<li>这个索引从下到上编号。以第二个slice为例，block 1的idx是0，block
0的idx是1。</li>
</ul>
<h2 id="a_sh_wr_iters变量">a_sh_wr_iters变量</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> a_gl_stride = prob_k / <span class="number">8</span>; <span class="comment">// stride of the A matrix in global memory</span></span><br><span class="line"><span class="comment">// We typically use `constexpr` to indicate that this value is a compile-time constant</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_stride = <span class="number">16</span> * thread_k_blocks / <span class="number">8</span>; <span class="comment">// stride of an A matrix tile in shared memory</span></span><br><span class="line">constexpr <span class="type">int</span> a_gl_rd_delta_o = <span class="number">16</span> * thread_k_blocks / <span class="number">8</span>; <span class="comment">// delta between subsequent A tiles in global memory</span></span><br><span class="line"><span class="type">int</span> a_gl_rd_delta_i = a_gl_stride * (threads / a_gl_rd_delta_o); <span class="comment">// between subsequent accesses within a tile</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_wr_delta = a_sh_stride * (threads / a_gl_rd_delta_o); <span class="comment">// between shared memory writes</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_rd_delta_o = <span class="number">2</span> * ((threads / <span class="number">32</span>) / (thread_n_blocks / <span class="number">4</span>)); <span class="comment">// between shared memory tile reads</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_rd_delta_i = a_sh_stride * <span class="number">16</span>; <span class="comment">// within a shared memory tile</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_stage = a_sh_stride * (<span class="number">16</span> * thread_m_blocks); <span class="comment">// overall size of a tile</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_wr_iters = ceildiv(a_sh_stage, a_sh_wr_delta); <span class="comment">// number of shared write iterations for a tile</span></span><br></pre></td></tr></table></figure>
<p>接下来<strong>着重</strong>看一下 <code>a_sh_wr_iters</code>
这个变量：</p>
<p>从 Global mem 加载数据到 Shared mem
的时候，每一个thread会读取一个int4（即4个int，128 bits），一个thread
blocks 256个线程需要分多次才能将完整的tile数据块读取完毕。</p>
<pre><code>为什么用int4？
因为 kernel 使用的读取全局显存数据的`cp.async.cg.shared.global`指令最大处理长度是128 bits。</code></pre>
<p>在本文中，读取A矩阵tile需要的次数</p>
<p><code>a_sh_wr_iters = ceildiv(a_sh_stage, a_sh_wr_delta)</code>,</p>
<p>表示A的一个tile的shared write迭代次数。 其中，</p>
<ul>
<li>a_sh_stage = 512，表示A的一个 tile 的整体尺寸
<ul>
<li>A矩阵一个 tile
为<code>[16 x thread_m_blocks, 16 x thread_k_blocks]</code>，所以大小是
64 * 64 / 8</li>
<li>除以8是因为A的指针是int4类型，4个int32_t, 128
bit，对应8个fp16，</li>
</ul></li>
<li>a_sh_wr_delta = 256，表示between shared memory writes。
<ul>
<li>a_sh_wr_delta = a_sh_stride * (threads / a_gl_rd_delta_o);</li>
<li>8 * (256 / 8) = 256</li>
</ul></li>
</ul>
<p><img src="/images/marlin/4.png" width="95%"></p>
<p>也就意味着，一个thread blocks
256个线程能够读取256个<code>int4</code>，一个tile有 512
个<code>int4</code>，读取A矩阵tile要循环两次。在后面的
<code>fetch_to_shared</code> 函数里，一次是fetch
A的一个tile，这里会体现出循环两次。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Precompute which thread should not read memory in which iterations; this is needed if there are more threads than</span></span><br><span class="line"><span class="comment">// required for a certain tilesize or when the batchsize is not a multiple of 16.</span></span><br><span class="line"><span class="type">bool</span> a_sh_wr_pred[a_sh_wr_iters];</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a_sh_wr_iters; i++)</span><br><span class="line">  a_sh_wr_pred[i] = a_sh_wr_delta * i + a_sh_wr &lt; a_sh_stride * prob_m;</span><br><span class="line"><span class="type">bool</span> s_sh_wr_pred = threadIdx.x &lt; s_sh_stride;</span><br></pre></td></tr></table></figure>
<p>在矩阵运算中预先计算出哪些线程在特定的迭代过程中不应该从内存中读取数据。这是为了优化计算资源的使用，特别是在以下两种情况下：</p>
<ul>
<li>线程数超过所需的 tile
大小时：即当前的任务需要的线程数比实际提供的线程数要少，这可能会导致一些线程不需要参与内存读取。</li>
<li>batchsize不是 16
的倍数时：当批处理大小无法整齐地划分时，可能会出现某些线程无需读取数据的情况。</li>
</ul>
<p>#pragma
unroll：这个编译器指令提示编译器将循环展开（unroll），以减少循环控制的开销，优化性能。通常用于
GPU 编程中的小规模循环，因为展开循环可以减少分支跳转。</p>
<h2 id="a矩阵的load">A矩阵的load</h2>
<h3 id="关于-bank-conflict-的背景知识">关于 bank conflict
的背景知识</h3>
<p>Shared memory是片上存储器，因此与 local memory 或 global memory
相比更高的带宽和更低的延迟。前提是线程之间没有 bank conflicts。</p>
<p>为了实现高带宽，共享内存被划分为大小相等的内存模块，称为
Banks，可以同时访问。因此，任何由 n
个地址组成的内存读取或写入请求都可以同时提供服务，从而产生比单个模块带宽高
n 倍的总带宽。</p>
<p>但是，如果warp内多个线程的内存请求的两个地址位于同一Bank中，则存在bank
conflict，并且必须序列化访问。也就是说，<strong>Bank conflict 是在一个
warp 内，有2个或者以上的线程访问了同一个 bank
上不同地址的内存。</strong></p>
<p>shared memory被分为 32 bank，每个bank的位宽是 4 byte。最大
transaction 大小为 128 bit。如果每个线程请求 16 bit，那么 warp
宽度将为每次请求（warp 宽度）的 512 bit。</p>
<p>当GPU每个线程访存大于 4 bytes，即每个 warp 大于
<code>4 * 32 = 128</code> bytes 时,GPU 不会发出单个transaction，GPU
会将其分成 4 个 transactions（在这种情况下：T0-T7
组成一个transaction，T8-T15
是一个transaction，依此类推），每个transaction的宽度为 128 bit。</p>
<p>需要注意的是，bank conflicts 的确定是按 <strong>transaction</strong>
进行的，而不是按request、warp 或instruction进行的。</p>
<p>因此，每个wrap则会分割成多个 transaction 去执行，每个 transaction
保证线程内的访存不落在同一bank即可，所以当我们用最大访存指令时，需要保证1/4个连续线程不会存在地址重叠。</p>
<p>这也就是这个说法：</p>
<p>an access to Shared Memory will be conflict-free if the following
conditions are satisfied across each warp:</p>
<ul>
<li>{T0, T1, .., T7} do not access the same 128-bit bank</li>
<li>{T8, T9, .., T15} do not access the same 128-bit bank</li>
<li>{T16, T17, .., T23} do not access the same 128-bit bank</li>
<li>{T24, T25, .., T31} do not access the same 128-bit bank</li>
</ul>
<hr />
<p>在marlin中，为了提高 load
效率，一般会使用<strong>向量化</strong>的读取命令，一次读取 128
bit，也就是 16 byte，对应<strong>4个bank</strong>。那么 8
个线程就可以一次完成 32 个bank 的load，所以问题简化为研究
<code>T0 - T7</code> or <code>T8 - T15</code> or <code>T16 - T23</code>
or <code>T24 - T32</code> 这 8 个线程内没有 bank 冲突。</p>
<p>如上文所讲，bank conflict 是针对单次 memory transaction
而言的。如果单次 memory transaction 需要访问的 128 bytes 中有多个 word
属于同一个 bank，就产生了 bank conflict。</p>
<p>对于A矩阵这种<strong>激活值矩阵</strong>而言，没有办法提前pack，因此
ldmatrix 指令<strong>读取</strong>的时候，会发生bank
conflict。为了解决这个问题，需要对A矩阵的存储地址进行转换。对于ij位置可以通过转存到i(i
⊕ j)位置来避免冲突， 其中⊕是异或计算。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> transform_a = [&amp;] (<span class="type">int</span> i) &#123;</span><br><span class="line">  <span class="type">int</span> row = i / a_gl_rd_delta_o;</span><br><span class="line">  <span class="keyword">return</span> a_gl_rd_delta_o * row + (i % a_gl_rd_delta_o) ^ row;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// Since the computation of this remapping is non-trivial and, due to our main loop unrolls, all shared memory </span></span><br><span class="line"><span class="comment">// accesses are static, we simply precompute both transformed reads and writes.</span></span><br><span class="line"><span class="type">int</span> a_sh_wr_trans[a_sh_wr_iters];</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a_sh_wr_iters; i++)</span><br><span class="line">  a_sh_wr_trans[i] = transform_a(a_sh_wr_delta * i + a_sh_wr);</span><br></pre></td></tr></table></figure>
<p>由于重映射计算较为复杂，且主循环进行了展开（unroll），所有的共享内存访问都是静态的，因此预先计算好重映射的读取和写入索引可以提高性能。</p>
<p>A矩阵的swizzle实现是<code>transform_a</code>这个函数，其实现很简单</p>
<ul>
<li><code>row = i / a_gl_rd_delta_o</code> 算出来是在第几行</li>
<li><code>i % a_gl_rd_delta_o</code> 算出来是在第几列</li>
<li><code>(i % a_gl_rd_delta_o) ^ row</code>
进行异或，修改了列的位置</li>
<li><code>a_gl_rd_delta_o * row + (i % a_gl_rd_delta_o) ^ row</code>加上整体偏移</li>
</ul>
<p><code>a_sh_wr_trans</code>这个数组记录了进行swizzle的相关映射。</p>
<p>做了个动图看一下，就非常明晰了：</p>
<p><img src="/images/marlin/5.gif" width="95%"></p>
<p>从 global memory 到 shared memory
搬运的代码实现是在<code>fetch_to_shared</code>函数中 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">int4* sh_a_stage = sh_a + a_sh_stage * pipe;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a_sh_wr_iters; i++) &#123;</span><br><span class="line">  cp_async4_pred(</span><br><span class="line">    &amp;sh_a_stage[a_sh_wr_trans[i]],</span><br><span class="line">    &amp;A[a_gl_rd_delta_i * i + a_gl_rd + a_gl_rd_delta_o * a_off],</span><br><span class="line">    a_sh_wr_pred[i]</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Predicated asynchronous global-&gt;shared copy; used for inputs A where we apply predication to handle batchsizes that</span></span><br><span class="line"><span class="comment">// are not multiples of 16.</span></span><br><span class="line">__device__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">cp_async4_pred</span><span class="params">(<span class="type">void</span>* smem_ptr, <span class="type">const</span> <span class="type">void</span>* glob_ptr, <span class="type">bool</span> pred = <span class="literal">true</span>)</span> &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> BYTES = <span class="number">16</span>;</span><br><span class="line">  <span class="type">uint32_t</span> smem = static_cast&lt;<span class="type">uint32_t</span>&gt;(__cvta_generic_to_shared(smem_ptr));</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="string">&quot;&#123;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;   .reg .pred p;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;   setp.ne.b32 p, %0, 0;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;   @p cp.async.cg.shared.global [%1], [%2], %3;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;&#125;\n&quot;</span> :: <span class="string">&quot;r&quot;</span>((<span class="type">int</span>) pred), <span class="string">&quot;r&quot;</span>(smem), <span class="string">&quot;l&quot;</span>(glob_ptr), <span class="string">&quot;n&quot;</span>(BYTES)</span></span><br><span class="line"><span class="params">  )</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 由此可见，copy矩阵A
使用的是异步拷贝指令。后面在讲流水线的时候还会再提到这块。</p>
<hr />
<p>接下来看一下A矩阵是怎么load到寄存器的。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; thread_m_blocks; i++)</span><br><span class="line">  ldsm4(frag_a[k % <span class="number">2</span>][i], &amp;sh_a_stage[a_sh_rd_trans[k % b_sh_wr_iters][i]]);</span><br></pre></td></tr></table></figure>
<p>可以看到，在 shared memory 到 frag 的过程中，用到了
<code>a_sh_rd_trans</code>
进行索引，而这也是通过上述的<code>transform_a</code>函数实现的，相当于往shared
memory写的时候是按照异或规则写的，读的时候也要按照异或规则读出来，这样才能正确地映射。</p>
<hr />
<p>说一下题外话：</p>
<p>其实这里我纠结了很久，关于bank conflict 和
swizzle的内容我能够理解，但是我不能理解 marlin 用 <code>swizzle</code>
实现 <code>bank conflict free</code>: 在 marlin 中，A矩阵是 row-major
排布的，所以我一直疑惑为什么需要进行 swizzle ：这个矩阵在 global memory
本身就是 row-major 排布的，岂不是按照原始顺序 load 到 shared
memory，然后顺序读取的话也不会 bank conflict？</p>
<p>为此我在<a
href="https://www.zhihu.com/question/667972067">cuda的swizzle是怎么实现bank
conflict free的？</a>进行了提问，<a
href="https://www.zhihu.com/question/667972067/answer/3634692524">Arthur</a>的回答我认为很合理，即：</p>
<ul>
<li>swizzle 是在更复杂访问模式下，确保每个线程不会访问到相同的memory
bank。</li>
<li>如果矩阵是row-major的且读取是连续的，那么可能无需swizzle操作就能避免bank
conflict。</li>
<li>但如果存在交错访问或者更复杂的访问模式，则swizzle是有必要的，用以确保bank
conflict free。</li>
</ul>
<hr />
<p>后来发现，其实是会发生<code>bank conflict</code>的，有一个细节：</p>
<p>在计算<code>a_sh_rd_trans</code>的时候 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; b_sh_wr_iters; i++) &#123;</span><br><span class="line">  <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; thread_m_blocks; j++)</span><br><span class="line">    a_sh_rd_trans[i][j] = transform_a(a_sh_rd_delta_o * i + a_sh_rd_delta_i * j + a_sh_rd); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
要注意输入<code>a_sh_rd</code>变量： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Shared read index.</span></span><br><span class="line"><span class="type">int</span> a_sh_rd = a_sh_stride * ((threadIdx.x % <span class="number">32</span>) % <span class="number">16</span>) + (threadIdx.x % <span class="number">32</span>) / <span class="number">16</span>;</span><br><span class="line">a_sh_rd += <span class="number">2</span> * ((threadIdx.x / <span class="number">32</span>) / (thread_n_blocks / <span class="number">4</span>));</span><br></pre></td></tr></table></figure>
所以读的时候是竖向读的。横向写，竖向读，那确实是要做swizzle。</p>
<p>这里画了一个图展示Global memory -&gt; Shared Memory -&gt; Frag
的过程：</p>
<p><img src="/images/marlin/6.png" width="95%"></p>
<h2 id="b矩阵的load">B矩阵的load</h2>
<p><code>b_sh_stride = 32 * thread_n_blocks / 4;</code></p>
<p>这里困扰了我很久，在这里一定要记录一下：</p>
<ul>
<li>32 = 16 * 16 * 4 / 32
<ul>
<li>16 * 16 是warp大小</li>
<li>4是重复4次</li>
<li>32是类型转换 int4-&gt;INT4（128bit/4bit=32）</li>
</ul></li>
<li>4是重复4次</li>
</ul>
<p>在每个stage，thread block分配的</p>
<ul>
<li><p>A矩阵shared mem size为[16xthread_m_blocks,
16xthread_k_blocks]，</p></li>
<li><p>B矩阵的shared mem size为[thread_k_blocks,
32xthread_n_blocks]。</p></li>
</ul>
<p>Bank conflict free</p>
<p>B矩阵经过重排，使得MMA计算需要的[8, 16]
tile全部处于一行，读取的时候不会发生bank conflict，</p>
<p>参考资料：</p>
<p>https://arxiv.org/pdf/2408.11743 进击的Killua：MARLIN:
Mixed-Precision Auto-Regressive Parallel Inference on Large Language
Models论文解读 suluner：Marlin W4A16&amp;W4A8代码走读 进击的Killua：cute
Swizzle细谈 cutlass/media/docs/implicit_gemm_convolution.md at main ·
NVIDIA/cutlass reed：cute 之 GEMM流水线</p>
<p>reed：cute 之 GEMM流水线</p>
<p>在GPU中，线程通常共享一块高速缓存存储，称为共享内存。共享内存分为多个bank，每个bank可以在同一时间处理一个请求。如果多个线程同时访问同一个bank，称为bank
conflict，会导致性能下降。为避免bank
conflict，通常需要对数据布局进行特定的优化。
为了确保在共享内存中写入和读取A矩阵的tiles时不发生bank
conflict，采用了一种基于XOR的布局。这种布局确保了来自8个连续线程的16字节（int4）块不会涉及相同的共享内存bank。
通过预计算变换后的读写索引，以优化共享内存的访问模式，确保在主循环展开（unrolling）过程中，能够高效且无冲突地进行读写操作。</p>
<p>在高性能GPU编程中，循环展开（loop
unrolling）是一种常用的优化技术。通过展开循环，编译器可以减少循环控制的开销，进一步加速执行。
此外，共享内存的访问模式非常重要，尤其是在并行计算中，避免bank
conflict和优化读写顺序是提升性能的关键。
根据NSight-Compute工具的分析结果，似乎每个warp（32个线程组成的线程组）必须写入一个连续的内存片段。这一观察与使用XOR变换重新排列数据的方式有关。
a_sh_rd_trans[b_sh_wr_iters][thread_m_blocks]：这是一个二维数组，存储了读取共享内存的变换后的索引。b_sh_wr_iters表示外层迭代次数，thread_m_blocks表示内层迭代次数（每个线程块中的计算单元数量）。</p>
<p>总结：</p>
<ol type="1">
<li><p>预计算索引：在循环展开的情况下，所有共享内存的读写访问模式是静态的（即索引是固定的）。为了减少每次访问时的计算开销，代码通过预计算读写索引，提前准备好每个线程需要访问的共享内存位置。</p></li>
<li><p>循环展开优化：使用#pragma
unroll对循环进行展开，有助于减少循环控制的开销，特别是在大规模并行的GPU计算中，循环展开可以极大提升性能。</p></li>
<li><p>共享内存优化：通过使用XOR变换函数transform_a()，保证了读取和写入共享内存时，线程不会访问相同的内存银行，从而避免银行冲突，提高了并行性能。</p></li>
</ol>
<p>•
B矩阵访问的非恒定步幅：在很多情况下，矩阵的访问模式并不是恒定的步幅（constant
stride），特别是在执行一些块（tile）操作时。非恒定步幅意味着每次访问的内存地址增量不相同，这可能会导致性能瓶颈，尤其是在高并发的GPU程序中，因为连续的依赖会阻碍并行化。</p>
<p>•
打破依赖关系：通常在多次内存访问之间，如果存在某种依赖关系（即下一个访问依赖于上一个访问的结果），它会限制并行计算的效率。通过维护多个指针，这段代码能够消除访问之间的依赖关系，从而提高并行性。</p>
<p>从全局内存（global
memory）异步地将下一块A、B以及可能的scale数据传输到共享内存（shared
memory）</p>
<p>• 异步数据传输
(cp_async)：为了提高并行执行效率，代码使用了cp_async函数，它是一种GPU的异步内存传输指令，允许从全局内存向共享内存加载数据，而不需要等待数据传输完成。这种机制可以在数据传输的同时进行其他计算，从而隐藏内存延迟。</p>
<p>•
流水线（Pipeline）机制：pipe表示流水线中的阶段。通过将数据加载分成多个阶段，可以在数据传输的同时继续进行计算，利用流水线机制进一步提高效率。</p>
<p>A矩阵使用cp_async4_pred()：这是异步加载数据的指令，它从全局内存中将A矩阵的下一块数据加载到共享内存中。</p>
<p>B和S矩阵使用cp_async4_stream()：这是另一种异步加载的指令，它从B矩阵的全局内存中将数据加载到共享内存中。</p>
<p>cp_async_fence()：插入一个异步栅栏，确保在后续操作中，数据加载已经完成。即使当前流水线阶段即将结束，仍然需要确保数据传输已经完成，避免数据访问错误。</p>
<p>通过
cp_async4_pred()，程序可以有选择性地从全局内存加载数据到共享内存，以节省内存带宽并减少不必要的数据传输。
•
优化内存访问：在某些情况下，A矩阵的部分数据块可能并不需要传输。通过使用谓词控制，程序可以灵活地决定是否要加载特定数据，避免不必要的全局内存访问，提高内存访问效率。
• A矩阵使用cp_async4_pred()
是为了提供灵活的、条件性的内存加载控制，避免在不需要时加载不必要的数据。这在复杂或不规则的访问模式下尤为重要。
• B矩阵使用cp_async4_stream()，因为 B
矩阵的内存访问模式更为固定和顺序化，因此可以简化为无条件加载，确保每次都能高效地传输所需数据。
a_sh_wr_pred[]
是一个布尔数组，预先计算了哪些线程在哪些迭代中不需要从全局内存读取数据。这是为了应对以下两种情况：
• 更多线程：如果有比当前所需
tile（块）的数据更多的线程，则不是所有线程都需要进行数据读取。 •
批量大小不是16的倍数：如果批量大小不是16的倍数，意味着可能在某些情况下，最后一组数据不足以填满所有线程的读取需求，因此部分线程在某些迭代中不需要执行数据读取。
因此，cp_async4_pred() 的作用是在读取 A 矩阵时，结合 a_sh_wr_pred[i]
判断当前迭代是否真的需要进行数据传输。通过这个机制，代码可以避免不必要的全局内存读取，减少内存带宽的浪费，优化性能。</p>
<p>总结： 1.
异步加载优化：这段代码使用了异步加载的指令（cp_async系列），从全局内存将A、B以及scale（如果需要）数据加载到共享内存中，减少了内存延迟对计算的影响。
2.
流水线机制：通过维护多个流水线阶段（pipe），可以在数据传输的同时继续进行其他计算，从而提高并行性和执行效率。
3.
条件和预判：代码通过pred和谓词a_sh_wr_pred[i]等条件控制哪些数据需要传输，灵活处理不同场景下的数据加载需求。
4.
栅栏保证：在数据传输完成后，通过cp_async_fence()确保数据一致性，避免出现数据访问冲突或错误。</p>
<p>// Wait until the next thread tile has been loaded to shared memory.
auto wait_for_stage = [&amp;] () { // We only have
<code>stages - 2</code> active fetches since we are double buffering and
can only issue the next fetch when // it is guaranteed that the previous
shared memory load is fully complete (as it may otherwise be
overwritten). cp_async_wait&lt;stages - 2&gt;(); __syncthreads(); };
这段代码通过双缓冲机制（double
buffering）来管理共享内存的数据传输，并确保同步加载的数据块不被覆盖。其核心思想是在并行计算过程中，控制全局内存和共享内存之间的数据加载顺序，避免数据被意外覆盖，同时保持高效的流水线数据流。
等待 stages - 2
个阶段的数据加载完毕，即所有处于当前活跃阶段的加载操作都已完成。
确保在继续执行下一步计算之前，当前的共享内存数据已经完全加载成功，并且可以安全地进行使用。这样做的好处是防止数据冲突或未完全加载时被访问。
• __syncthreads() 是 CUDA
中的同步指令，它确保同一个线程块中的所有线程在到达该指令时都必须等待，直到所有线程都执行完此操作，才能继续执行后续的代码。这是一个全局同步点，确保数据的一致性。
• 在这段代码中，__syncthreads()
的作用是确保所有线程在继续执行之前，当前阶段的所有共享内存加载都已完成。这与
cp_async_wait 配合使用，以确保双缓冲过程中数据加载和计算的正确性。</p>
<p>mma() 是张量核心的矩阵乘法函数，执行 A × B 的操作，并将结果累积到
frag_c 中。由于量化后的数据拆分为两部分（frag_b0 和
frag_b1），因此执行了两次乘法，每次使用不同的子块进行运算。 // Since we
slice across the k dimension of a tile in order to increase the number
of warps while keeping the n // dimension of a tile reasonable, we have
multiple warps that accumulate their partial sums of the same output //
location; which we have to reduce over in the end. We do in shared
memory. auto thread_block_reduce = [&amp;] () {
在许多并行计算的情况下，k 维度（内积的长度）可能非常大，因此通过将 k
维度切分为多个块可以增加并行计算的粒度和线程的利用率。 • k
维度的切分：为了增加并行计算的线程块（warps）的数量，这里选择将 k
维度进行切分。切分后的每个 k 维度的子块可以由不同的 warp 进行并行计算。
• 增加 warp 数量：通过将 k 维度切分，不仅可以增加参与计算的 warp
数量，还能更好地利用 GPU 的并行计算能力，提高整体吞吐量。
为了确保性能，代码选择只切分 k 维度，而保持 n
维度（列数）大小合理。这样可以保证每个线程块（warp）有足够的计算工作，但不会因为过多线程竞争导致
n 维度变得过大，从而降低局部数据重用的效率。 由于不同的 warp
都在并行处理相同的输出位置，因此每个 warp 只计算了部分的结果（partial
sums），这些部分结果需要在最后进行归约（reduction）操作.通过在共享内存中累积每个
warp
的部分和，可以高效地进行归约操作，而不必依赖全局内存。这避免了全局内存带来的延迟，同时利用共享内存实现快速的同步和数据共享。
通过切分 k 维度，这段代码增加了并行度，从而提高了 GPU 的利用率。每个
warp
计算部分和的结果，最终需要通过共享内存进行归约操作，将多个部分和合并为最终结果。这种方式可以有效提升矩阵乘法的性能，同时利用共享内存的高效性来完成必要的同步操作。
// Since multiple threadblocks may process parts of the same column
slice, we finally have to globally reduce over // the results. As the
striped partioning minimizes the number of such reductions and our
outputs are usually rather // small, we perform this reduction serially
in L2 cache. auto global_reduce = [&amp;] (bool first = false, bool last
= false) { 注释提到 条状分区（striped
partitioning），这是一种数据划分策略，可以减少需要进行全局归约的次数。 •
条状分区的作用：这种分区方法通过合理划分数据，减少了需要进行归约的不同线程块的数量，降低了全局归约操作的复杂度。
•
减少归约次数：通过条状分区，同一列的数据会尽量集中在较少的线程块上处理，从而减少归约操作的频率。尽可能减少全局归约是优化并行程序性能的重要手段之一。</p>
<p>注释提到最终的归约操作是在 L2 缓存 中串行完成的。L2 缓存是 GPU
中的一种中层缓存，存取速度比全局内存快，但比共享内存慢。使用 L2
缓存进行归约操作具有以下优点： • L2 缓存的优势：相比全局内存，L2
缓存的延迟较低，访问速度更快。在全局归约过程中，使用 L2
缓存可以加快数据的合并速度，减少内存带宽的消耗。 •
串行归约：注释提到归约操作是串行进行的，这可能是因为最终的输出数据较小，串行执行归约的开销很低。并且，串行操作可以简化编程复杂度，不需要额外的并行归约算法。</p>
<p>注释还提到
输出通常较小，这意味着在这些情况下，归约操作不会成为性能瓶颈。由于输出数据量不大，归约操作在
L2 缓存中串行完成已经足够高效，不必使用复杂的并行归约算法。 总结 •
当多个线程块处理同一列片段时，必须进行全局归约操作，将各自的部分结果合并成完整的输出。
• 通过
条状分区，可以减少需要进行全局归约的线程块数量，从而降低归约的复杂度。 •
最终的归约操作在 L2 缓存 中串行进行，因为输出较小，使用 L2
缓存可以加快数据访问速度，而串行归约的开销也较低。</p>
<h2 id="关于-bank-conflict-和-swizzle">关于 bank conflict 和
swizzle</h2>
<p>作为一个cuda小白最开始对这里非常疑惑，这里整理一下看过的一些相关资料：</p>
<p><a href="https://zhuanlan.zhihu.com/p/671419093">cute 之 Swizzle</a>
- reed</p>
<p><a href="https://zhuanlan.zhihu.com/p/684250988">cute Swizzle细谈</a>
- 进击的Killua</p>
<p><a href="https://zhuanlan.zhihu.com/p/706796240">cute swizzle</a> -
shengying.wei</p>
<p><a
href="https://zhuanlan.zhihu.com/p/659142274">CUDA编程概念:什么是bank
conflict？</a> - likewind1993</p>
<p><a
href="https://zhuanlan.zhihu.com/p/681966685">[深入分析CUTLASS系列] 0x03
cutlass 源码分析(二) --- bank conflict free 的shared memory layout
(附tvm等价pass)</a> - JoeNomad</p>
<p><a href="https://zhuanlan.zhihu.com/p/690052715">搞懂 CUDA Shared
Memory 上的 bank conflicts 和向量化指令（LDS.128 /
float4）的访存特点</a> - Alan 小分享</p>
<p><a
href="https://forums.developer.nvidia.com/t/how-to-understand-the-bank-conflict-of-shared-mem/260900">How
to understand the bank conflict of shared_mem</a></p>
<p>国庆前还没看完的页面</p>
<p><a
href="https://zhuanlan.zhihu.com/p/697228676">cuda的ldmatrix指令的详细解释</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/702818267">CUTLASS CuTe
GEMM细节分析（一）——ldmatrix的选择</a></p>
<p><a
href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf">Optimizing
Parallel Reduction in CUDA</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/698715169">fp16*int4计算kernel--MARLIN代码走读</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/665082713">cute 之
GEMM流水线</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/716412368">MARLIN:
Mixed-Precision Auto-Regressive Parallel Inference on Large Language
Models论文解读</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/707470647">Marlin
W4A16&amp;W4A8代码走读</a></p>
<h1 id="ncu-rep">ncu-rep</h1>
<p>输入 m = 128, k = 256, n = 768</p>
<ul>
<li>0_base.ncu-rep: 原始版本，不限制SM数量</li>
<li>1_base_sm5.ncu-sep: 限制SM数量为5</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>GEMM</tag>
        <tag>HPC</tag>
        <tag>LLM</tag>
        <tag>CUDA</tag>
        <tag>MARLIN</tag>
      </tags>
  </entry>
</search>
