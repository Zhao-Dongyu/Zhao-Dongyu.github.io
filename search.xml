<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>WeightonlyGEMM:dequantize_s4_to_fp16x2代码解析</title>
    <url>/2024/08/12/101_WeightonlyGEMM:%20dequantize_s4_to_fp16x2%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>本文聚焦于AWQ的W4A16 (4-bit weight, 16-bit activation) CUDA
kernel的反量化。</p>
<span id="more"></span>
<h1 id="背景">背景</h1>
<p>在 ZZK大佬的文章 <a
href="https://zhuanlan.zhihu.com/p/675427125">可能是讲的最清楚的WeightonlyGEMM博客</a>
中学到很多知识</p>
<hr />
<p>最近在学AWQ的量化，<a
href="https://arxiv.org/pdf/2306.00978">AWQ论文</a></p>
<p>其中我比较关注GPU的部分，其中数据排布这块很有意思：</p>
<p><img src="/images/101/1.jpeg" width="60%"></p>
<p>于是到<a href="https://arxiv.org/pdf/2211.10017">Who Says Elephants
Can’t Run</a> 这篇论文看了一下，<strong>WeightOnly GEMM</strong>
技术就是在这里提出的，其中权重反量化是这么操作的：</p>
<p><img src="/images/101/2.jpeg" width="60%"></p>
<p>这个论文提出了一种 `IntToFloat
的巧妙的方式，可以大大提速反量化速度。</p>
<p><img src="/images/101/3.jpeg" width="60%"></p>
<p>背景介绍结束，更详细的讲解可以直接看ZZK大佬的文章，接下来解释
<code>dequantize_s4_to_fp16x2</code> 代码。</p>
<h1 id="代码">代码</h1>
<p>首先放出完整代码（80行）： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__device__ uint4 <span class="title function_">dequantize_s4_to_fp16x2</span><span class="params">(<span class="type">uint32_t</span> <span class="type">const</span>&amp; source)</span> &#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__CUDA_ARCH__) &amp;&amp; __CUDA_ARCH__ &lt; 750</span></span><br><span class="line">  assert(<span class="literal">false</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  uint4 result;</span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span>* h = reinterpret_cast&lt;<span class="type">uint32_t</span>*&gt;(&amp;result);</span><br><span class="line">  <span class="type">uint32_t</span> <span class="type">const</span> i4s = reinterpret_cast&lt;<span class="type">uint32_t</span> <span class="type">const</span>&amp;&gt;(source);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First, we extract the i4s and construct an intermediate fp16 number.</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> immLut = (<span class="number">0xf0</span> &amp; <span class="number">0xcc</span>) | <span class="number">0xaa</span>;</span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> BOTTOM_MASK = <span class="number">0x000f000f</span>;</span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> TOP_MASK = <span class="number">0x00f000f0</span>;</span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> I4s_TO_F16s_MAGIC_NUM = <span class="number">0x64006400</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Note that the entire sequence only requires 1 shift instruction. This is</span></span><br><span class="line">  <span class="comment">// thanks to the register packing format and the fact that we force our</span></span><br><span class="line">  <span class="comment">// integers to be unsigned, and account for this in the fp16 subtractions. In</span></span><br><span class="line">  <span class="comment">// addition, I exploit the fact that sub and fma have the same throughput in</span></span><br><span class="line">  <span class="comment">// order to convert elt_23 and elt_67 to fp16 without having to shift them to</span></span><br><span class="line">  <span class="comment">// the bottom bits before hand.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW</span></span><br><span class="line">  <span class="comment">// dependency if we issue immediately before required.</span></span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> top_i4s = i4s &gt;&gt; <span class="number">8</span>;</span><br><span class="line">  <span class="comment">// Extract elt_01 - (i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line">  <span class="comment">// Extract elt_23 (i4s &amp; 0x00f000f0) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(TOP_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line">  <span class="comment">// Extract elt_45 (top_i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">2</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(top_i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line">  <span class="comment">// Extract elt_67 (top_i4s &amp; 0x00f000f0) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">3</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(top_i4s), <span class="string">&quot;n&quot;</span>(TOP_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// I use inline PTX below because I am not sure if the compiler will emit</span></span><br><span class="line">  <span class="comment">// float2half instructions if I use the half2 ctor. In this case, I chose</span></span><br><span class="line">  <span class="comment">// performance reliability over code readability.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// This is the half2 &#123;1032, 1032&#125; represented as an integer.</span></span><br><span class="line">  <span class="comment">// static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64086408;</span></span><br><span class="line">  <span class="comment">// Haotian: subtract &#123;1024, 1024&#125; instead, we do not need to map to [-8, 7]</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> FP16_TOP_MAGIC_NUM = <span class="number">0x64006400</span>;</span><br><span class="line">  <span class="comment">// This is the half2 &#123;1 / 16, 1 / 16&#125; represented as an integer.</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> ONE_SIXTEENTH = <span class="number">0x2c002c00</span>;</span><br><span class="line">  <span class="comment">// This is the half2 &#123;-72, -72&#125; represented as an integer.</span></span><br><span class="line">  <span class="comment">// static constexpr uint32_t NEG_72 = 0xd480d480;</span></span><br><span class="line">  <span class="comment">// Haotian: Let&#x27;s use &#123;-64, -64&#125;.</span></span><br><span class="line">  <span class="type">static</span> constexpr <span class="type">uint32_t</span> NEG_64 = <span class="number">0xd400d400</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Finally, we construct the output numbers.</span></span><br><span class="line">  <span class="comment">// Convert elt_01</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;sub.f16x2 %0, %1, %2;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">0</span>]), <span class="string">&quot;r&quot;</span>(FP16_TOP_MAGIC_NUM))</span>;</span><br><span class="line">  <span class="comment">// Convert elt_23</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;fma.rn.f16x2 %0, %1, %2, %3;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">1</span>]), <span class="string">&quot;r&quot;</span>(ONE_SIXTEENTH), <span class="string">&quot;r&quot;</span>(NEG_64))</span>;</span><br><span class="line">  <span class="comment">// Convert elt_45</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;sub.f16x2 %0, %1, %2;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">2</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">2</span>]), <span class="string">&quot;r&quot;</span>(FP16_TOP_MAGIC_NUM))</span>;</span><br><span class="line">  <span class="comment">// Convert elt_67</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;fma.rn.f16x2 %0, %1, %2, %3;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">3</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">3</span>]), <span class="string">&quot;r&quot;</span>(ONE_SIXTEENTH), <span class="string">&quot;r&quot;</span>(NEG_64))</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="代码解析">代码解析</h1>
<p>接下来逐句分析： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__device__ uint4 <span class="title function_">dequantize_s4_to_fp16x2</span><span class="params">(<span class="type">uint32_t</span> <span class="type">const</span>&amp; source)</span></span><br></pre></td></tr></table></figure></p>
<p>这行代码定义了一个名为 <code>dequantize_s4_to_fp16x2</code>
的函数，这个函数是一个 CUDA 设备函数
(<strong>device</strong>)，它的输入是一个常量引用 <code>uint32_t</code>
类型的 <code>source</code>，返回值是 <code>uint4</code> 类型。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__CUDA_ARCH__) &amp;&amp; __CUDA_ARCH__ &lt; 750</span></span><br><span class="line">  assert(<span class="literal">false</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br></pre></td></tr></table></figure>
<p>这段预处理指令检查 CUDA 架构版本是否小于
750。如果是，则触发断言错误，表明该代码不适用于 CUDA 750
以下的架构。否则，继续执行后面的代码。 比如这个函数里面用到了
<code>lop3.b32</code> 函数，可以在 <a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/">这里</a>
查到</p>
<pre><code>Requires sm_50 or higher.
Qualifier .BoolOp requires sm_70 or higher.</code></pre>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">uint4 result;</span><br><span class="line"></span><br><span class="line"><span class="type">uint32_t</span>* h = reinterpret_cast&lt;<span class="type">uint32_t</span>*&gt;(&amp;result);</span><br><span class="line"><span class="type">uint32_t</span> <span class="type">const</span> i4s = reinterpret_cast&lt;<span class="type">uint32_t</span> <span class="type">const</span>&amp;&gt;(source);</span><br></pre></td></tr></table></figure>
<p>这里定义了一个 <code>uint4</code> 类型的变量
<code>result</code>。</p>
<p>接着将 <code>result</code> 的地址转换为 <code>uint32_t*</code>
类型的指针 <code>h</code>，以便后续的位操作。</p>
<p>然后将 <code>source</code> 重新解释为 <code>uint32_t</code>
类型并赋值给 <code>i4s</code>。</p>
<p>通过类型转换和指针操作，使得对这些整数的访问和操作更加便捷。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> immLut = (<span class="number">0xf0</span> &amp; <span class="number">0xcc</span>) | <span class="number">0xaa</span>;</span><br><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> BOTTOM_MASK = <span class="number">0x000f000f</span>;</span><br><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> TOP_MASK = <span class="number">0x00f000f0</span>;</span><br><span class="line"><span class="type">static</span> constexpr <span class="type">uint32_t</span> I4s_TO_F16s_MAGIC_NUM = <span class="number">0x64006400</span>;</span><br></pre></td></tr></table></figure>
<p>这里定义了一些用于位操作的常量：</p>
<ul>
<li><p><code>immLut</code> 是一个查找表的立即数。</p></li>
<li><p><code>BOTTOM_MASK</code> 是用于提取较低位的掩码。</p></li>
<li><p><code>TOP_MASK</code> 是用于提取较高位的掩码。</p></li>
<li><p><code>I4s_TO_F16s_MAGIC_NUM</code>
是用于整数转换为半精度浮点数的魔数。</p></li>
</ul>
<p><img src="/images/101/4.jpeg" width="95%"></p>
<p>这几个数在内存中是这么表示的，后面用到的时候会仔细说，那时候会发现这几个数可真妙啊。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">uint32_t</span> top_i4s = i4s &gt;&gt; <span class="number">8</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/101/5.jpeg" width="95%"></p>
<p>这里是将 <code>i4s</code> 向右移位8位，以便处理 <code>elt_45</code>
和 <code>elt_67</code>。暂时先不要管为什么这么做。
到这里，预备工作都做好了，接下来开始重头戏。</p>
<h2 id="处理e0-e1元素">处理e0 e1元素</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Extract elt_01 - (i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br></pre></td></tr></table></figure>
<p>使用内联 PTX 指令 <code>lop3.b32</code> 执行逻辑操作，将
<code>i4s</code> 的低位（<code>BOTTOM_MASK</code> 掩码）与
<code>I4s_TO_F16s_MAGIC_NUM</code> 结合并存储在 <code>h[0]</code>
中。</p>
<p>首先看一下 <code>lop3.b32</code> 执行了什么操作，这个
<code>lop3.b32</code> 是一种CUDA汇编指令，用于执行
三元逻辑运算。它利用查找表（immLut）来简化逻辑操作。这个函数可以在<a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/">这里</a>查到.</p>
<pre><code>For a logical operation F(a, b, c) the value of immLut can be computed by applying the same operation to three predefined constant values as follows:

ta = 0xF0;
tb = 0xCC;
tc = 0xAA;

immLut = F(ta, tb, tc);</code></pre>
<p>比如： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">If F = (a &amp; b &amp; c);</span><br><span class="line">immLut = <span class="number">0xF0</span> &amp; <span class="number">0xCC</span> &amp; <span class="number">0xAA</span> = <span class="number">0x80</span></span><br><span class="line"></span><br><span class="line">If F = (a | b | c);</span><br><span class="line">immLut = <span class="number">0xF0</span> | <span class="number">0xCC</span> | <span class="number">0xAA</span> = <span class="number">0xFE</span></span><br><span class="line"></span><br><span class="line">If F = (a &amp; b &amp; ~c);</span><br><span class="line">immLut = <span class="number">0xF0</span> &amp; <span class="number">0xCC</span> &amp; (~<span class="number">0xAA</span>) = <span class="number">0x40</span></span><br><span class="line"></span><br><span class="line">If F = ((a &amp; b | c) ^ a);</span><br><span class="line">immLut = (<span class="number">0xF0</span> &amp; <span class="number">0xCC</span> | <span class="number">0xAA</span>) ^ <span class="number">0xF0</span> = <span class="number">0x1A</span></span><br></pre></td></tr></table></figure></p>
<p>现在我们可以看一下刚才定义的
<code>immLut = (0xf0 &amp; 0xcc) | 0xaa;</code> 那我们得到的操作就是
<code>(a &amp; b) | c</code>，也就是<code>( i4s &amp; BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM</code>.</p>
<p>这里先插一句，这个<code>immLut</code>设置的实在是太妙了！首先，abc这三个数可以形成一个
3 bit 的编码，可以表示 8 种状态，</p>
<p><img src="/images/101/6.jpeg" width="80%"></p>
<p>然后无论abc进行什么样的三元逻辑操作，得到的结果都是唯一的，这也就形成了immLut的表。实在是妙啊。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Extract elt_01 - (i4s &amp; 0x000f000f) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(BOTTOM_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br></pre></td></tr></table></figure>
<p>然后结合具体数值看一下是怎么操作的：</p>
<p><img src="/images/101/7.jpeg" width="95%"></p>
<p>这一步操作是提取元素0和元素1的，这里设置元素0是<code>0000</code>，即
0，元素1是<code>0001</code>，即 1。</p>
<p><code>i4s</code>与<code>BOTTOM_MASK</code>进行位与操作后，实际上就是把<code>e0</code>和<code>e1</code>筛选了出来，和<code>I4s_TO_F16s_MAGIC_NUM</code>进行位或操作后，是给指数位进行了赋值。</p>
<p>现在看这个 <code>I4s_TO_F16s_MAGIC_NUM</code>
，<code>0x64006400</code>，只看这个指数位部分是
<code>11001</code>，对应数字 25，fp16指数位的偏移量为
<code>2**4−1=15</code>, 那么偏移后指数为 <code>25 - 15 = 10</code>。</p>
<p>以e1为例，</p>
<p>尾数位是<code>0.0000000001</code>，加上1，那就是<code>1.0000000001</code>。</p>
<p>指数位是10，所以结果是<code>1.00000001*2^10 = 10000000001</code>。也就是小数点向后移动10位。</p>
<p>这个结果的十进制是 1025。</p>
<p>这时候就可以理解这个 <code>I4s_TO_F16s_MAGIC_NUM</code>
为什么这么设置了——原来的数值类型是<code>int4</code>，肯定是整数，那么问题的关键就是要把尾数放大到一定程度，使新的数值是整数。那么，显而易见，要把小数点向右移动10位（因为尾数位有10位），也就反推出了<code>magic number</code>的具体数值，妙啊。</p>
<p>显然，此时的结果并不正确，因为加了1并且右移了10位，也就是加了1024，所以我们要把结果减去1024才可以，所以还需要进行这一步操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Convert elt_01</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;sub.f16x2 %0, %1, %2;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">0</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">0</span>]), <span class="string">&quot;r&quot;</span>(FP16_TOP_MAGIC_NUM))</span>;</span><br></pre></td></tr></table></figure>
<p>其中，<code>FP16_TOP_MAGIC_NUM = 0x64006400;</code></p>
<p>这句代码实现了减1024这个操作，于是最终得到fp16格式的<code>e1=1</code>，
<code>e0=0</code>，数据无误。</p>
<p>需要注意的是，<code>FP16_TOP_MAGIC_NUM = 0x64006400;</code>这个数代表的其实是fp16格式的
1024。</p>
<p>我最开始在这里感到疑惑，心想为什么要减这么一个数，这也不是1024啊，直到后来意识到现在是在做fp16的减法，那么当然也要是fp16的格式。</p>
<p>ok，到现在为止，最难的部分就理解完了。接下来看一下别的情况：</p>
<h2 id="处理e2-e3元素">处理e2 e3元素</h2>
<p><img src="/images/101/8.jpeg" width="95%"></p>
<p>对于e2、e3而言，这时候就不能用 <code>BOTTOM_MASK</code>
来进行位与操作了，应该改为 <code>TOP_MASK</code></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Extract elt_23 (i4s &amp; 0x00f000f0) | 0x64006400</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;lop3.b32 %0, %1, %2, %3, %4;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(i4s), <span class="string">&quot;n&quot;</span>(TOP_MASK), <span class="string">&quot;n&quot;</span>(I4s_TO_F16s_MAGIC_NUM),</span></span><br><span class="line"><span class="params">                 <span class="string">&quot;n&quot;</span>(immLut))</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/101/9.jpeg" width="95%"></p>
<p>如果减去1024，呢么得到<code>e3=48</code>，<code>e2=32</code>，这和刚才不太一样，事情变得不太简单。</p>
<p>问题在哪？以e3为例，这时候小数点右移10位意味着把e3乘了<code>2^4=16</code>，所以当前的结果是<code>1024 + X * 16</code>。</p>
<p>所以针对e2、e3的下一步操作稍有不同：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Convert elt_23</span></span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;fma.rn.f16x2 %0, %1, %2, %3;\\n&quot;</span></span></span><br><span class="line"><span class="params">               : <span class="string">&quot;=r&quot;</span>(h[<span class="number">1</span>])</span></span><br><span class="line"><span class="params">               : <span class="string">&quot;r&quot;</span>(h[<span class="number">1</span>]), <span class="string">&quot;r&quot;</span>(ONE_SIXTEENTH), <span class="string">&quot;r&quot;</span>(NEG_64))</span>;</span><br></pre></td></tr></table></figure>
<p>其中，<code>ONE_SIXTEENTH = 0x2c002c00; NEG_64 = 0xd400d400;</code></p>
<p>这是做了什么操作？可以理解为</p>
<p><code>(1024 + X * 16) * (1/16) - 64 = X</code></p>
<p>ok，到现在为止，e0e1e2e3都搞定了，最后看一下e4e5，</p>
<h2 id="处理e4-e5元素">处理e4 e5元素</h2>
<p><img src="/images/101/10.jpeg" width="95%"></p>
<p>e4 e5元素怎么提取呢？可以先认真思考一下。</p>
<p>我最开始的想法是，针对e4
e5的位置，再写一个mask进行位与操作不就好了，然后和前面做一样的操作。</p>
<p>但是这样是不可行的，以e5为例，这时候e5在16位的高8位，与fp16的指数位有冲突。</p>
<p>那么，处理方式就很简单了，文章最开始的时候提到过这个操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">uint32_t</span> top_i4s = i4s &gt;&gt; <span class="number">8</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/101/11.jpeg" width="95%"></p>
<p>这里是将 i4s 向右移位8位，以便处理 <code>elt_45</code> 和
<code>elt_67</code>。</p>
<p>这样一来，就可以按照e0 e1的方式处理e4 e5了。</p>
<p>ok，到现在为止，e0 e1 e2 e3 e4 e5都搞定了，有了前面的基础，e6
e7理解起来就很简单，这里不做过多赘述。</p>
<p>整个代码实现到这里也就讲完了。</p>
<hr />
<h2 id="写在最后">写在最后</h2>
<p>最后，补充几点</p>
<ul>
<li><p>对于e2e3而言，为什么不像e5e6那样，向右移动8位，然后做e0e1相同的操作？</p>
<p>这样的操作从逻辑上确实更好理解，也避免了后面的乘1/16的操作，看上去似乎更简单。</p>
<p>但是要明白一点，e5e6那样，向右移动8位，是不得不这么做。</p>
<p>为了性能，e2e3没必要额外增加一个移位操作。所以关键点只剩一个：<code>fma.rn.f16x2</code>指令和<code>sub.f16x2</code>指令的区别，如果
移位+sub指令 快于
<code>fma.rn</code>指令，那就是赚的。代码中的注释标明了这一点：</p>
<pre><code>  In addition, I exploit the fact that sub and fma have the same throughput in order to convert elt_23 and elt_67 to fp16 without having to shift them to the bottom bits before hand.</code></pre>
<p>sub（减法）和
fma（乘法累加）具有相同的吞吐量，这意味着它们的性能相当。</p></li>
<li><p>为什么要做<code>interleave</code>？</p>
<p>本文缘起就是因为研究这个权重排布</p>
<p><img src="/images/101/12.jpeg" width="60%"></p>
<p>前文中一直没有提这件事——为什么权重是<code>interleave</code>排布？</p>
<p><img src="/images/101/13.png" width="95%"></p>
<p>按照zzk大佬的观点，对于<code>Int8</code>来说，本来就是不需要<code>Interleave</code>的，但是<code>Int4</code>快速转换不能使用PRMT指令，需要通过一些trick，进而需要这个交错权重的格式</p>
<p>也就是说，<code>int8</code>快速反量化中，使用了<code>PRMT</code>指令，这个指令的操作粒度是字节<code>byte</code>；而int4是<code>sub-byte</code>，不足一个字节；因此，原来在<code>INT8</code>快速反量化的那一套无法直接用在<code>int4</code>上。</p>
<p>所以针对<code>int4</code>，使用指令<code>LOP3</code>来替代<code>PRMT</code>，从而完成了<code>int4</code>快速反量化到<code>fp16</code>的核心逻辑。通过交错权重，可以减少<code>shift</code>指令，提高性能。关于这一点，我认为<a
href="https://zhuanlan.zhihu.com/p/657073857?utm_medium=social&amp;utm_oi=56635854684160&amp;utm_psn=1692002696943525888&amp;utm_source=wechat_session&amp;s_r=0">[LLM推理优化]🔥WINT8/4-(03):
LOP3指令详解及INT4转FP16/BF16分析</a> 讲解的比较详细。</p></li>
</ul>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>AWQ</tag>
        <tag>dequantize</tag>
      </tags>
  </entry>
  <entry>
    <title>在riscv平台优化SGEMM</title>
    <url>/2024/02/26/102_sgemm_riscv_cn/</url>
    <content><![CDATA[<p>本项目记录了在riscv平台上优化SGEMM（单精度浮点通用矩阵乘法）的过程。</p>
<span id="more"></span>
<p><img src=/images/sgemm/data/result_all.png width=70% /></p>
<p>本项目记录了在riscv平台上优化SGEMM（单精度浮点通用矩阵乘法）的过程。</p>
<p>通用矩阵乘（<code>General Matrix Multiplication</code>，简称<code>gemm</code>）是深度学习框架的核心计算单元之一，广泛用于<code>Convolution</code>、<code>Full connection</code>、<code>Matmul</code>等算子实现。</p>
<p>我是在 <strong>全志 Nezha D1</strong>
开发板上进行的实验与探索，<code>version 0 - version 5</code> 是使用的 C
语言，<code>version 6 - version 9</code> 则部分使用了汇编，涉及到
<code>RISC-V V</code> 扩展指令。</p>
<p>注意：不同于其他的某些gemm优化工程，在本工程中，全部使用
<strong>行主序</strong> 的矩阵。因为我喜欢 <strong>行主序</strong>！</p>
<h1 id="预备知识">预备知识</h1>
<p><img src=/images/sgemm/pics/riscv.gif width=50% /></p>
<p><a href="https://riscv.org/">RISC-V</a> 是一种开放标准指令集架构
(ISA)，通过开放协作开启处理器创新的新时代。</p>
<hr />
<p><img src=/images/sgemm/pics/GEMM.png width=70% /></p>
<p><a
href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">GEMM</a>
通用矩阵乘法，基本线性代数子程序之一。</p>
<hr />
<p><a
href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a>每秒浮点运算次数，亦称每秒峰值速度，（英语：<code>Floating-point operations per second</code>；缩写：<code>FLOPS</code>），即每秒所执行的浮点运算次数。一个
<code>GFLOPS(gigaFLOPS)</code> 等于每秒十亿（10^9）次的浮点运算；</p>
<p>矩阵乘的计算量是
<code>2 * M * N * K</code>，计算量除以耗时即为当前gemm版本的gflops。 -
乘以 2 是因为每次操作包含一次乘法和一次加法</p>
<h1 id="准备工作">准备工作</h1>
<p>相关代码位于 <code>./prepare/</code>。</p>
<h2 id="测试交叉编译">测试交叉编译</h2>
<p>我使用的 <strong>全志 Nezha D1</strong> 开发板，在<a
href="https://xuantie.t-head.cn/community/download?id=4090445921563774976">这里</a>下载的交叉编译链接。</p>
<p>详细教程见<a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/blob/main/prepare/README.md">readme</a></p>
<h2 id="内存带宽测试">内存带宽测试</h2>
<p>分别通过以下几个小工程对开发板内存带宽进行测试</p>
<ul>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/2.memcpy_bandwidth_test">2.memcpy_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/3.flw_bandwidth_test">3.flw_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/4.vlw_bandwidth_test">4.vlw_bandwidth_test</a></li>
</ul>
<p><img src=/images/sgemm/pics/memory_bandwidth_test.png width=70% /></p>
<h2 id="roofline模型">roofline模型</h2>
<p><a href="https://en.wikipedia.org/wiki/Roofline_model">Roofline</a>
提出了使用
<code>Operational Intensity</code>（计算强度）进行定量分析的方法，并给出了模型在计算平台上所能达到理论计算性能上限公式。</p>
<p>根据<a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a>所说 - <code>D1</code>的算力可达
<code>4 GFlops(@1GHz)</code> - Memory : <code>2.727 GB/s</code> (DDR3
792 MHz)。 -
虽然我测出来最高是<code>2.592 GB/s</code>，可能哪里出了问题？ -
商汤还是要相信一下的，暂时以他的为准吧</p>
<p><img src=/images/sgemm/pics/roofline.png width=70% /></p>
<h1 id="sgemm优化">sgemm优化</h1>
<p>相关代码位于 <code>./sgemm/</code>。</p>
<h2 id="使用说明">使用说明</h2>
<p>以 <code>step0</code> 为例，你需要先编辑 Makefile
，配置自己的交叉编译链。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> sgemm/step0/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb push test_bl_sgemm_step0.x ./.</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb shell <span class="string">&#x27;./test_bl_sgemm_step0.x&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h2 id="version-0-naive版本">Version 0: naive版本</h2>
<p>这个版本思路在我看来是<strong>最直观</strong>的，毕竟我当时就是这么<strong>学习</strong>、<strong>理解</strong>和<strong>计算</strong>矩阵乘法的:
&gt; A 的一行乘以 B 的一列得到 C 的一个元素。</p>
<p><img src=/images/sgemm/data/result_0.png width=70% /></p>
<p><img src=/images/sgemm/pics/step0.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;          <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;      <span class="comment">// Start 0-st loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>我认为<code>version 0</code>非常契合的解释了 <span
class="math inline">\(C_{mn} = \sum_{k=1}^{K} A_{mk}B_{kn}\)</span>
这个公式。</p>
<p>但是这个版本缺点比较明显：在理论算力为 <code>4 GFLOPS</code>
的平台仅发挥出最大 <code>0.03 GFLOPS</code>
的计算性能。这是因为<strong>对于矩阵B的访问，cache命中率极低，即「空间局部性很差」</strong>。整个计算下来，相当于访问矩阵B的次数多了很多很多次。</p>
<p>对于多维数组的元素尽量能按照顺序访问。这样可以改善内存访问的空间局部性，对缓存更加友好。</p>
<p>另外，观察到随着尺寸的增加，性能有较大的抖动。分析数据可以知道：当<code>m=n=k</code>为
128 164 192 228 256 288 320 352 384 时性能都很差。这几个数相差32，32 *
4(<code>sizeof(float)</code>) = 128 B。</p>
<p>猜测性能抖动与<code>cacheline</code>以及<code>硬件预取</code>有关——
cacheline = 64B，cache miss
后，硬件预取即<code>HWPrefetcher</code>，多读 1 个 cacheline。</p>
<h2 id="version-1-循环交换版本">Version 1: 循环交换版本</h2>
<p>重复使用Cache中的数据是最基本的高效使用Cache方法。对于多层嵌套循环，可以通过交换两个嵌套的循环（<code>loop interchange</code>）、逆转循环迭代执行的顺序（<code>loop reversal</code>）、将两个循环体合并成一个循环体（<code>loop fusion</code>）、循环拆分（<code>loop distribution</code>）、循环分块（<code>loop tiling</code>）、<code>loop unroll and jam</code>等循环变换操作。</p>
<p>选择适当的循环变换方式，既能保持程序的语义，又能改善程序性能。</p>
<p><img src=/images/sgemm/data/result_1.png width=70% /></p>
<p><img src=/images/sgemm/pics/step1.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;          <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;      <span class="comment">// Start 0-nd loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>相较于<code>version 0</code>，<code>version 1</code>对于矩阵B的操作，空间局部性较好，性能有较大提升（尤其是对于尺寸较大的情况，而在
m = n = k &lt;= 68 时，版本0的效率更高）。</p>
<p>调整m,n,k的顺序，对结果没有影响(即保持程序的语义)，但是可以影响性能。
测试不同循环顺序的性能（全志Nezha D1平台，以m=n=k=512为例）</p>
<table>
<thead>
<tr class="header">
<th>循环顺序</th>
<th>GFLOPS</th>
<th>分析</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNK</td>
<td>0.012</td>
<td>访问B的cache miss高</td>
</tr>
<tr class="even">
<td>MKN</td>
<td>0.180</td>
<td></td>
</tr>
<tr class="odd">
<td>NMK</td>
<td>0.012</td>
<td>访问B的cache miss高</td>
</tr>
<tr class="even">
<td>NKM</td>
<td>0.009</td>
<td>访问A的cache miss高</td>
</tr>
<tr class="odd">
<td>KMN</td>
<td>0.165</td>
<td></td>
</tr>
<tr class="even">
<td>KNM</td>
<td>0.009</td>
<td>访问A的cache miss高</td>
</tr>
</tbody>
</table>
<p>但是，<code>version 1</code>
的硬件使用率仍然很低，继续想办法优化。</p>
<h2 id="version-2-分块版本">Version 2: 分块版本</h2>
<p><img src=/images/sgemm/data/result_2.png width=70% /></p>
<p><img src=/images/sgemm/pics/step2.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;          <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;        <span class="comment">// Start 1-st loop</span></span><br><span class="line">        AddDot_4x4_opt( k, &amp;A( i, <span class="number">0</span> ), lda, &amp;B( <span class="number">0</span>, j ), ldb, &amp;C( i, j ), ldc );</span><br><span class="line">    &#125;                                          <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                              <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p><img src=/images/sgemm/pics/gemm_block.png width=70% /></p>
<p>为了避免多余的 cache 换入换出，于是进行分块处理。<a
href="https://zhuanlan.zhihu.com/p/342923482">浅谈分块矩阵优化方法为什么有用</a>一文讲的挺好，建议阅读学习。</p>
<p>版本2进行了分块操作后，性能仍然不理想，这是因为，这个版本只是表面逻辑上实现了分块，块内的计算还有一些小技巧没有应用。</p>
<h2 id="version-3-分块优化版本">Version 3: 分块优化版本</h2>
<p><img src=/images/sgemm/data/result_3.png width=70% /></p>
<p><img src=/images/sgemm/pics/step3.gif width=70% /></p>
<p>增加了AddDot_4x4_opt</p>
<p>在<a
href="https://github.com/flame/blislab/blob/master/tutorial.pdf">BLISlab-tutorial</a>中提到了几个小技巧：</p>
<ul>
<li>循环展开 2.4.2 Loop unrolling
<ul>
<li>Updating loop index i and the pointer cp every time through the
inner loop creates considerable overhead. For this reason, a compiler
will perform loop unrolling.</li>
</ul></li>
<li>寄存器缓存 2.4.3 Register variables
<ul>
<li>Notice that computation can only happen if data is stored in
registers. A compiler will automatically transform code so that the
intermediate steps that place certain data in registers is
inserted.</li>
</ul></li>
</ul>
<p>使用了这个技巧后，这个版本性能有了大幅度的提升！</p>
<p>然而，这个版本对于尺寸较大的矩阵，性能仍然比较低。查找原因，比如访问B[0,0],
B[0,1], B[0,2],
B[0,3]后，访问B[1,0]的时候，当尺寸较大的时候必定<code>cache miss</code>。因此，如果能提前把数据重新排列就好了。</p>
<h2 id="version-4-b-prepack版本">Version 4: B prepack版本</h2>
<p><img src=/images/sgemm/data/result_4.png width=70% /></p>
<p><img src=/images/sgemm/pics/step4.gif width=70% /></p>
<p>我默认矩阵B是<strong>参数</strong>，所以可以提前进行<code>pack</code>操作。版本4对矩阵B进行了<code>prepack</code>，性能更进一步！</p>
<p>性能提升的原因很明显：访问矩阵 B 的 <code>cache miss</code>
明显减少了。这也是我第一次深刻意识到模型推理前对神经网络权重做<code>prepack</code>的重要性。</p>
<p>可以看到，当尺寸比较大的时候，性能还是有所下降的。这应该是对矩阵A访问的
<code>cache miss</code> 比较多。要对A做 <code>pack</code> 吗？</p>
<p>我默认矩阵A是<strong>输入</strong>，因此对A做 <code>pack</code>
的话是不能提前做的，是要算在整体时间内的。那么，有必要吗？</p>
<h2 id="version-5-a-pack-b-prepack版本">Version 5: A pack &amp; B
prepack版本</h2>
<p><img src=/images/sgemm/data/result_5.png width=70% /></p>
<p><img src=/images/sgemm/pics/step5.gif width=70% /></p>
<p>在版本4的基础上，版本5对矩阵A进行了<code>pack</code></p>
<p>这里默认矩阵A是<strong>输入</strong>，所以需要在计算过程中进行<code>pack</code>操作，这个耗时是要放在计时里面的。</p>
<p>结果还是让人欣喜的，尤其是在大尺寸矩阵上，取得了进一步的性能提升。</p>
<p>我本来是试一试的心态做了这个尝试，毕竟这个操作会多读一遍<code>A</code>以及写一遍<code>packA</code>。看来接下来主要就是与<code>cache miss</code>作斗争了。</p>
<p>目前思路方面的优化基本到头了，在计算过程中做一些<code>preload</code>是值得尝试一下的。</p>
<p>接下来上汇编，搞向量计算，以及在汇编里面做<code>preload</code>。</p>
<h2 id="version-6-汇编版本">Version 6: 汇编版本</h2>
<p><img src=/images/sgemm/data/result_6.png width=70% /></p>
<p>简要说明：A没有<code>pack</code>，B进行了16个数的<code>prepack</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;       <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">    <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;   <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">        <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>关于 <code>rvv</code> 指令的使用，我认为<code>vsetvli</code>
是灵魂，<code>vfmacc.vf</code> 是主力。</p>
<p>关于这些内容，我从<a
href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 | RISC-V
技术解析</a>学到了很多，他们可真专业啊！建议理论指导和知识点学习去他们那里学，向
<code>OpenPPL</code> 致敬！</p>
<p>至于汇编算子，汇编里面的小细节就多了，强烈吐槽：<strong>写汇编真烦人啊！尤其是
debug 过程，很折磨人。</strong>
我上次写汇编还是本科上课的时候，重新拾起还是有一些新奇和兴奋的，而且能够非常细粒度的控制算子执行，成就感还是很大的。</p>
<p>关于汇编文件具体怎么实现的，我认为最快的方式就是去看汇编代码。这里就不多做解释了</p>
<p>需要注意的是，这个版本效果很差，这是为什么呢？又是
<strong>循环顺序</strong> 的问题。</p>
<h2 id="version-7-汇编版本-调换循环顺序">Version 7: 汇编版本
调换循环顺序</h2>
<p><img src=/images/sgemm/data/result_7.png width=70% /></p>
<p>简要说明：A没有<code>pack</code>，B进行了16个数的<code>prepack</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;       <span class="comment">// Start 2-st loop</span></span><br><span class="line">    <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">    <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">    <span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;   <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">        <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>调换了循环的次序，先n方向后m方向，性能大大提升。</p>
<p>但是，大尺寸矩阵的性能还不是很好。究其原因，还是在访存上。大尺寸矩阵的计算在<code>roofline</code>模型里属于是<strong>计算密集型</strong>，理想情况是<strong>计算的时间</strong>和<strong>访存的时间</strong>尽可能重叠，而现在基本是花很多时间在访存了（又是因为<code>cache miss</code>！）。</p>
<h2 id="version-8-汇编版本-加入preload">Version 8: 汇编版本
加入preload</h2>
<p><img src=/images/sgemm/data/result_8.png width=70% /></p>
<p>简要说明：A没有<code>pack</code>，B进行了16个数的<code>prepack</code>，做了
<code>preload</code> 操作。</p>
<p>性能相对爆炸！最高达到了 <code>2.212 GFLOPS</code>。</p>
<p>核心操作： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfmacc.vf v16,  ft0, v0</span><br><span class="line">vlw.v v4, (bp0)         # b0&#x27;-&gt;v4</span><br><span class="line">flw fs4, 384(bp0)       # pre-load B</span><br><span class="line">addi bp0,bp0,64</span><br><span class="line">vfmacc.vf v20,  ft1, v0</span><br></pre></td></tr></table></figure> 在 <code>vfmacc.vf</code> 之间加入一些
<code>load</code>
操作，把之后要用到的数据提前<code>load</code>到<code>cache</code>中，可以大大降低
<code>cache miss</code>。</p>
<p>我最开始很疑惑——这样看代码明明也是顺序计算，怎么做到<strong>计算的时间</strong>和<strong>访存的时间</strong>重叠的呢？直到后来了解到<code>cacheline</code>的原理才明白这里的精髓，果然基础知识很重要啊。</p>
<h2 id="version-9-汇编版本-a做pack">Version 9: 汇编版本 A做pack</h2>
<p><img src=/images/sgemm/data/result_9.png width=70% /></p>
<p>按照之前的经验，也尝试对 矩阵A 做了一下
<code>pack</code>，出乎意料的是结果不是很好。稍微分析了一下，应该是这个版本的汇编对
矩阵A 的<code>preload</code>写得不是很好。</p>
<p>上个版本虽然对A没有pack，但是对4排的A都有<code>preload</code>，所以也算是解决了矩阵A的
<code>cache miss</code>的痛点吧。</p>
<h1 id="总结">总结</h1>
<p>要想继续优化这个算子，后续要做的事情还有很多，比如在汇编里面重排流水线。</p>
<p>在<a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a>中也提到，如果使用 vf 指令，能够达到它理论峰值的
80%，即
4*80%，3.2GFLOPs。我现在只有<code>2.121 GFLOPS</code>，理论上还是有很大优化空间的。</p>
<p>另外，
RVV目前用的是<code>0.7.1</code>版本，感觉RVV的指令优化还是有很多工作要做，比如，目前遇到最严重的<code>vlw</code>效率低的问题。</p>
<p>总之，做这些工作让我从很多大佬那里学到了很多知识，非常感谢。同时也希望本文能帮助到更多的人。</p>
<h1 id="致谢">致谢</h1>
<ul>
<li><p><a href="https://github.com/flame/blislab">BLISlab: A Sandbox for
Optimizing GEMM</a></p>
<p>这个项目让我了解了如何优化GEMM。</p></li>
<li><p><a
href="https://github.com/surez-ok/blislab_riscv">riscv平台优化矩阵乘(基于blislab优化实践)</a></p>
<p>我基于这个项目进行了实验和探索。</p></li>
<li><p>感谢丁大佬的指导。</p></li>
</ul>
<h1 id="参考文章">参考文章</h1>
<p><a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a></p>
<p><a href="http://riscvbook.com/">RISC-V-Reader</a></p>
<p><a
href="https://github.com/riscv/riscv-v-spec/releases">riscv-v-spec-0.7.1</a></p>
<p><a
href="https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf">Anatomy
of High-Performance Matrix Multiplication</a></p>
<p><a
href="https://developer.huawei.com/consumer/cn/forum/topic/0201105374686101528">编译器优化丨Cache优化</a></p>
<p><a
href="https://www.leiphone.com/category/yanxishe/Puevv3ZWxn0heoEv.html">OpenBLAS项目与矩阵乘法优化
| AI 研习社</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34204282">Roofline
Model与深度学习模型的性能分析</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/342923482">浅谈分块矩阵优化方法为什么有用</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>RISCV</tag>
        <tag>GEMM</tag>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Battlegrounds</title>
    <url>/2023/07/09/100_Battlegrounds/</url>
    <content><![CDATA[<p>Set up a flag: the completion progress reaches 100%</p>
<p>Now at <strong>99%</strong></p>
<span id="more"></span>
<table>
<tr>
<td>
<b>Pretzel Mustard</b>
</td>
<td>
Without losing a round<br>(Completed 2024/06/03)
</td>
<td>
<b>Spicy Pretzel Mustard</b>
</td>
<td>
Winning every round<br>(Completed 2024/06/03)
</td>
</tr>
<tr>
<td>
<b>Finish in 1st place</b>
</td>
<td>
250/250<br>(Completed 2024/06/01)
</td>
<td>
<b>Finish in top 4</b>
</td>
<td>
1000/1000<br>(Completed 2024/10/04)
</td>
</tr>
<tr>
<td>
<b>Funnel Cakes</b>
</td>
<td>
Without losing a round in Duos<br>(Completed 2024/06/03)
</td>
<td>
<b>Cinnamon Funnel Cakes</b>
</td>
<td>
Winning every round in Duos<br>(Completed 2024/06/03)
</td>
</tr>
<tr>
<td>
<b>Finish in 1st place in Duos</b>
</td>
<td>
250/250<br>(Completed 2024/12/28)
</td>
<td>
Finish in top 2 in Duos
</td>
<td>
411/1000
</td>
</tr>
<tr>
<td>
<b>Beasts</b>
</td>
<td>
Completed 2024/01/31
</td>
<td>
<b>Golden Beasts</b>
</td>
<td>
Completed 2023/11/22
</td>
</tr>
<tr>
<td>
<b>Demons</b>
</td>
<td>
Completed 2024/05/30
</td>
<td>
<b>Golden Demons</b>
</td>
<td>
Completed 2024/05/16
</td>
</tr>
<tr>
<td>
<b>Dragons</b>
</td>
<td>
Completed 2024/06/15
</td>
<td>
<b>Golden Dragons</b>
</td>
<td>
Completed 2024/03/02
</td>
</tr>
<tr>
<td>
<b>Elementals</b>
</td>
<td>
Completed 2024/06/01
</td>
<td>
<b>Golden Elementals<b>
</td>
<td>
Completed 2024/05/16
</td>
</tr>
<tr>
<td>
<b>Mechs</b>
</td>
<td>
Completed 2024/03/17
</td>
<td>
<b>Golden Mechs</b>
</td>
<td>
Completed 2024/01/20
</td>
</tr>
<tr>
<td>
<b>Murlocs</b>
</td>
<td>
Completed 2024/06/06
</td>
<td>
<b>Golden Murlocs</b>
</td>
<td>
Completed 2024/05/21
</td>
</tr>
<tr>
<td>
<b>Naga</b>
</td>
<td>
Completed 2024/08/18
</td>
<td>
<b>Golden Naga</b>
</td>
<td>
Completed 2024/05/25
</td>
</tr>
<tr>
<td>
<b>Priates</b>
</td>
<td>
Completed 2024/3/11
</td>
<td>
<b>Golden Priates</b>
</td>
<td>
Completed 2023/12/24
</td>
</tr>
<tr>
<td>
<b>Quilboar</b>
</td>
<td>
Completed 2024/08/21
</td>
<td>
<b>Golden Quilboar</b>
</td>
<td>
Completed 2024/07/14
</td>
</tr>
<tr>
<td>
<b>Undead<b>
</td>
<td>
Completed 2023/12/10
</td>
<td>
<b>Golden Undead</b>
</td>
<td>
Completed 2023/10/23
</td>
</tr>
</table>
<table>
<tr>
<td align="center" style="width: 100px;">
Hero
</td>
<td style="width: 100px;">
</td>
<td style="width: 200px;">
Achievement
</td>
<td>
Description
</td>
</tr>
<tr>
<td rowspan="2">
<b>A.F.Kay</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/8f9f341289ba4279f8eb37f97f56abb4306ec379b16837830a2ac1b688558820.png">
</td>
<td>
Idle Idol<br>(Completed 2023/10/30)
</td>
<td>
Passive Skip your first two turns. Start with two minions from Tavern
Tier 3.<br>Finish in 1st place as A.F.Kay
</td>
</tr>
<tr>
<td>
brb<br>(Completed 2023/10/30)
</td>
<td>
Finish in 1st place after AFKing your third turn as A.F.Kay
</td>
</tr>
<tr>
<td>
<b>AI’Akir</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/208f628c6ae4340ca6889ff5ba883a919c021429b75f73c4903a4c647e4e6bcd.png">
</td>
<td>
Wrath of the Windlord<br>(Completed 2023/11/23)
</td>
<td>
Passive Start of Combat: Give your left-most minion Windfury, Divine
Shield, and Taunt.<br>Finish in 1st place as AI’Akir
</td>
</tr>
<tr>
<td>
<b>Alextrasza</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d7a061e5ab1c9c132725aaabe271d9fd60e0ce39d7831d7706e80171bce8e5f3.png">
</td>
<td>
Life and Hope<br>(Completed 2023/11/11)
</td>
<td>
Passive. After you upgrade Bob's Tavern to Tavern Tier 5, Discover two
Dragons of any Tier.<br>Finish in 1st place as Alextrasza
</td>
</tr>
<tr>
<td>
<b>Ambassador Faelin</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/812794e11850309243ae674e2523d8492a85d922a2dfd796ae317826ac090c32.png">
</td>
<td>
All According To Plan<br>(Completed 2023/05/30)
</td>
<td>
Passive. Skip your first turn. Discover a Tier 2, 4, and 6 minion to get
at those Tiers.<br>Finish in 1st place as Ambassador Farlin
</td>
</tr>
<tr>
<td>
<b>Aranna Starseeker</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/432b5f6dee120335d8c37258e03083cf559659af8196d32dc54a3581c209774e.png">
</td>
<td>
Outlandish<br>(Completed 2023/06/04)
</td>
<td>
Passive. After 16 friendly minions attack, the first minion you buy each
turn is free. ( left!)<br>Finish in 1st place as Aranna Starseeker
</td>
</tr>
<tr>
<td>
<b>Arch-Villain Rafaam</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d7f38259e52d3f372d1a5bdf212478e0b8b837aa62015ea41d92598102671dc3.png">
</td>
<td>
Supreme Tavernologist<br>(Completed 2023/07/20)
</td>
<td>
Next combat, add a plain copy of the first minion you kill to your
hand.<br>Finish in 1st place as Arch-Villain Rafaam
</td>
</tr>
<tr>
<td>
<b>Brann</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/58a3df4cc1ff17d37afad9957bd1b0c862e218d537425a0e56aa2c780dc4850d.png">
</td>
<td>
Where's the Action?!<br>(Completed 2023/10/05)
</td>
<td>
Passive. After you buy 5 Battlecry minions, add Brann Bronzebeard to
your hand. (Once per game.)<br>Finish in 1st place as Brann
</td>
</tr>
<tr>
<td>
<b>Bru’kan</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/831548757f8f792b550828859e1d286ff2ae56896006008c6d131a7559cb5a04.png">
</td>
<td>
Elements of Style<br>(Completed 2023/07/30)
</td>
<td>
Choose an Element. Start of Combat: Call upon that Element.<br>Finish in
1st place as Bru’kan
</td>
</tr>
<tr>
<td>
<b>Cap’n Hoggarr</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/47acd041253ef3a0bec521b75a8021d5cae454e05cabfa29d6ea15ada1695311.png">
</td>
<td>
Sailing to Victory<br>(Completed 2023/09/03)
</td>
<td>
Passive After you buy a Pirate, gain 1 Gold.<br>Finish in 1st place as
Cap’n Hoggarr
</td>
</tr>
<tr>
<td>
<b>Captain Eudora</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5cb89601f8020957f30607a4a161ea1a8d05e2324b7062886b72774944a5dbb3.png">
</td>
<td>
Can You Dig It?<br>(Completed 2023/10/01)
</td>
<td>
Dig for a Golden minion! ( (Dig, Digs) left.)<br>Finish in 1st place as
Captain Eudora
</td>
</tr>
<tr>
<td>
<b>Captain Hooktusk</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/171fd3a249bbd535c99d99eed2631c865cdb158d84c28f8beb7315992e7695fb.png">
</td>
<td>
Never Fight Fair<br>(Completed 2023/11/10)
</td>
<td>
Remove a friendly minion. Choose one of two from a Tavern Tier lower to
keep.<br>Finish in 1st place as Captain Hooktusk
</td>
</tr>
<tr>
<td>
<b>Cariel Roame</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d78fba8c2d05f214f5613e2d2406fc6b5860eaf2e73dc033ec479efdeb103190.png">
</td>
<td>
Courageous Conviction<br>(Completed 2024/06/01)
</td>
<td>
Give friendly minions + /+ . (After you upgrade the Tavern, choose an
improvement!) Give friendly minions + /+ .<br>Finish in 1st place as
Cariel Roame
</td>
</tr>
<tr>
<td>
<b>Chenvaala</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2a88458e43b359eef32c9cfc194bd48801ab65272b95a8e125b394a61a542e6d.png">
</td>
<td>
Is It Cold In Here?<br>(Completed 2023/09/09)
</td>
<td>
Passive After you play 3 Elementals, reduce the cost of upgrading Bob's
Tavern by (3).<br>Finish in 1st place as Chenvaala
</td>
</tr>
<tr>
<td>
<b>Cho</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ad4ea03a821830fdab6b763970f1dbcb10c2bd9b95a1e6c22d6ade14c969057d.png">
</td>
<td>
I won!<br>(Completed 2024/04/28)
</td>
<td>
Passive. Whenever you play a Golden minion, both you and Gall get a
Triple Reward. <br>Finish in 1st place as Cho
</td>
</tr>
<tr>
<td>
<b>Cookie the Cook</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/fbffd5745f86f2a5171831e01be2e8b7b421fb5ceb273b89d4d080bc74e8bc18.png">
</td>
<td>
3-Star Chef<br>(Completed 2023/08/20)
</td>
<td>
Throw a minion in your pot. When you've gathered 3, Discover from their
minion types. ( left!)<br>Finish in 1st place as Cookie the Cook
</td>
</tr>
<tr>
<td>
<b>C’Thun</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/56c5d1797655207f4606dbecaaf78ccf04c3db0b5e50146d8d41eae64723227d.png">
</td>
<td>
Mathematically Impossible<br>(Completed 2023/05/30)
</td>
<td>
At end of turn, give a friendly minion +1/+1. Repeat (time, times).
(Upgrades each turn!)<br>Finish in 1st place as C’Thun
</td>
</tr>
<tr>
<td rowspan="2">
<b>Dancing’ Deryl</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/11ad255ca614b077f1dfb7697b208c113d6009214ae7b11ccece5ddb97269c8c.png">
</td>
<td>
Show Me Your Moves<br>(Completed 2024/04/06)
</td>
<td>
Finish in 1st place as Dancing’ Deryl
</td>
</tr>
<tr>
<td>
Hats on Hats on Hats<br>(Completed 2023/10/16)
</td>
<td>
Place 30 hats on one minion as Dancing’ Deryl
</td>
</tr>
<tr>
<td>
<b>Death Speaker Blackthorn</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/38f3bf86dd5f0a7145a06078048d4d9aa887462aa6c486fc05e15ac0769f5046.png">
</td>
<td>
Killer Speech<br>(Completed 2024/05/18)
</td>
<td>
Get 2 Blood Gems. (Twice per turn.)<br>Finish in 1st place as Death
Speaker Blackthorn
</td>
</tr>
<tr>
<td>
<b>Deathwing</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5c475eb99fec4aeeb88fb1525061f969d472e3811e448050e59ca08d142a55fa.png">
</td>
<td>
POWER INCARNATE!!!<br>(Completed 2023/11/16)
</td>
<td>
Passive Start of Combat: Give ALL minions +2 Attack
permanently.<br>Finish in 1st place as Deathwing
</td>
</tr>
<tr>
<td>
<b>Doctor Holli'dae</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/65c6e0ef3c77e4e2e6b284300abb1d7c0c70805d3b797fd654c332e2ea4ea94a.png">
</td>
<td>
Happy Holli'daes!<br>(Completed 2023/12/09)
</td>
<td>
Get a random Tavern spell.<br>Finish in 1st place as Deathwing
</td>
</tr>
<tr>
<td>
<b>Drek’Thar</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6dbd7d61c752154fb08a9e6759f4e9f7bdf64b18e28cc61e7993319cc7c84e1c.png">
</td>
<td>
Frostwolf Fame<br>(Completed 2023/05/29)
</td>
<td>
Passive Avenge (2): Give your minions +1 Attack permanently.<br>Finish
in 1st place as Drek’Thar
</td>
</tr>
<tr>
<td rowspan="2">
<b>E.T.C Band Manager</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5b7bcf32835bca15197f609ec5a66e9690e61e620fa979db5363bc58b8e8c6cb.png">
</td>
<td>
Chart Topper<br>(Completed 2023/06/12)
</td>
<td>
Discover a Buddy. (Unlocks at Tier 2.)<br>Finish in 1st place as E.T.C
Band Manager
</td>
</tr>
<tr>
<td>
Friend of a Friend<br>(Completed 2023/06/29)
</td>
<td>
Play 16 Buddies
</td>
</tr>
<tr>
<td>
<b>Edwin VanCleef</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/e41d31d85a4a02afb1c8c2e1efae16f7d252187dd7ed2d32eabe0fd4be1fdb6b.png">
</td>
<td>
For the Brotherhood<br>(Completed 2023/06/16)
</td>
<td>
Give a minion + /+ . Upgrades after you buy 4 minions. (
left!)<br>Finish in 1st place as Edwin VanCleef
</td>
</tr>
<tr>
<td>
<b>Elise Starseeker</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/11eb07c5477fbdd522f626c7cf2df9c19e1cd0811c0a2692f1aef0c3a5d69fa9.png">
</td>
<td>
Always Bring a Map<br>(Completed 2023/11/21)
</td>
<td>
Discover a minion from your Tavern tier. Costs (1) more after each
use.<br>Finish in 1st place as Elise Starseeker
</td>
</tr>
<tr>
<td>
<b>Enhance-o Mechano</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6db8abb382f57b1e43719765426f009ca540f8f3416ec7ed6084b170ef5e3f92.png">
</td>
<td>
The Little Bot That Could<br>(Completed 2023/07/08)
</td>
<td>
Passive. After each Refresh, give a minion in Bob's Tavern Taunt,
Windfury, Reborn, or Divine Shield.<br>Finish in 1st place as Enhance-o
Mechano
</td>
</tr>
<tr>
<td>
Exarch Othaar
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/e4a9826601aab13d7df61e73c9e3629aab72577b3fa7582900fcb9ef8afb4438.png">
</td>
<td>
Astronomical!
</td>
<td>
The next Tavern spell you buy costs (1) less. (Unlocks on Turn
3.)<br>Finish in 1st place as Exarch Othaar
</td>
</tr>
<tr>
<td>
<b>Farseer Nobundo</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bbf88e7392a0a374aafcf2acbdecaaa9a57eb5d17f67c0f16c5601f08aaacc63.png">
</td>
<td>
Out of This World!<br>(Completed 2024/12/07)
</td>
<td>
Get a copy of the last Tavern spell you cast. Each turn, your next Hero
Power costs (1) less.<br>Finish in 1st place as Farseer Nobundo
</td>
</tr>
<tr>
<td>
<b>Flobbidinous Floop</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/7a4862481039beee31f60cb1abcf344afc48fa974647737d50dae194a4eb6d44.png">
</td>
<td>
Floop De Loop<br>(Completed 2024/04/27)
</td>
<td>
Choose a friendly minion. Start of Combat: Transform it into your
teammate's highest Tier minion.<br>Finish in 1st place as Flobbidinous
Floop
</td>
</tr>
<tr>
<td>
<b>Forest Warden Omu</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/03d19a23896ea91148a8793e7d90ea876750985e365183b5c69a6ff7b40c711c.png">
</td>
<td>
Nowhere Left To Grow<br>(Completed 2023/11/05)
</td>
<td>
Passive After you upgrade Bob's Tavern, gain 2 Gold.<br>Finish in 1st
place as Forest Warden Omu
</td>
</tr>
<tr>
<td>
<b>Fungalmancer Flurgl</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/37e634262bb5411afe01d26ed3b624e2b7bf49bac38d3a5bbc23dfc1e58cf9e3.png">
</td>
<td>
Wishies for Fishies<br>(Completed 2023/06/10)
</td>
<td>
Passive. After you sell 5 minions, get a random Murloc. (
left.)<br>Finish in 1st place as Fungalmancer Flurgl
</td>
</tr>
<tr>
<td rowspan="2">
<b>Galakrond</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bbe94b8cdad0b6130578d29278fc48ba4fdcaf1229e8337557cfb7d1743e0df9.png">
</td>
<td>
The Progenitor<br>(Completed 2023/08/21)
</td>
<td>
Choose a minion in Bob's Tavern. Discover a higher Tier minion to
replace it.<br>Finish in 1st place as Galakrond
</td>
</tr>
<tr>
<td>
Long Term Investment<br>(Completed 2023/05/30)
</td>
<td>
Upgrade a Tier 1 minion to a Tier 6 minion as Galakrond.
</td>
</tr>
<tr>
<td>
<b>Galewing</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/a19edf90bd53e346d1593f27b17d8b994ad97c00b8ed5c5deef46666719e6115.png">
</td>
<td>
Take to the Skies<br>(Completed 2023/05/31)
</td>
<td>
Choose a new flightpath. Complete it to get a bonus!<br>Finish in 1st
place as Galewing
</td>
</tr>
<tr>
<td>
<b>George the Fallen</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/fea24fd3049da7c75a46526a2784303099c6341c3bfe4a4673a2547728e1846f.png">
</td>
<td>
For Karl!<br>(Completed 2023/11/19)
</td>
<td>
Give a minion Divine Shield.<br>Finish in 1st place as George the Fallen
</td>
</tr>
<tr>
<td>
<b>Greybough</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3bb12d29a29fd44c9c6eeab5635ec9283a1843e00980c2787ccd54c7424db6e7.png">
</td>
<td>
Sprouting Your Roots<br>(Completed 2023/07/13)
</td>
<td>
Passive Give +1/+2 and Taunt to minions you summon during
combat.<br>Finish in 1st place as Greybough
</td>
</tr>
<tr>
<td>
<b>Guff Runetotem</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/aceadc9bb5f5cf6f282b60d9dd726053f5334b226044c65c04606ce275928df0.png">
</td>
<td>
Perfectly Balanced<br>(Completed 2023/08/27)
</td>
<td>
Give a friendly minion of each Tavern Tier +2/+2.<br>Finish in 1st place
as Guff Runetotem
</td>
</tr>
<tr>
<td>
<b>Heistaron Togaggle</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/33e7ebcd9c43edc2489e3c26b350852dd295085aa9d534729dd49cc8a5887df6.png">
</td>
<td>
Who The King? I THE KING!<br>(Completed 2023/11/05)
</td>
<td>
Steal all minions in Bob's Tavern. Each turn, your next Hero Power costs
(1) less.<br>Finish in 1st place as Heistaron Togaggle
</td>
</tr>
<tr>
<td>
<b>Illidan Stormrage</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/1b0741094f9630a6997e13f7fc16efd05d99daae42de6dc598b082bff4b47e1d.png">
</td>
<td>
You ARE Prepared<br>(Completed 2023/12/05)
</td>
<td>
Passive. Start of Combat: Your left and right-most minions gain +2/+1
and attack immediately.<br>Finish in 1st place as Illidan Stormrage
</td>
</tr>
<tr>
<td>
<b>Infinite Toki</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/df407a1a99c5cdb82bfcc51a9a8df3a04d62e34b5eb1ed39873cf1a8e3f9919a.png">
</td>
<td>
The Ideal Timeline<br>(Completed 2023/09/06)
</td>
<td>
Refresh Bob's Tavern. Include two minions from a Tavern Tier
higher.<br>Finish in 1st place as Infinite Toki
</td>
</tr>
<tr>
<td>
<b>Inge, the Iron Hymn</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ad3fe7b6b636db6fdbfcd969753535cb1abfe376bbc40078497386a43b5ee2ae.png">
</td>
<td>
Ingenious! <br>(Completed 2023/06/14)
</td>
<td>
Give a minion Attack equal to your Tavern Tier. (Swaps to Health next
turn!)<br>Finish in 1st place as Inge. The Iron Hymn
</td>
</tr>
<tr>
<td>
<b>Ini Stormcoil</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/28d3ec677705631e612383acee49a36bdfe7ee0060d82bfd13ee3d5261d0b571.png">
</td>
<td>
The Stormcoil Override<br>(Completed 2023/05/29)
</td>
<td>
Passive After 10 friendly minions die, get a random Mech. Passive After
10 friendly minions die, get a random Mech. ( left.)<br>Finish in 1st
place as Ini Stormcoil
</td>
</tr>
<tr>
<td>
<b>Jandice Barov</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/86e594de784417d255d4f67da713cf1f748f581984a764d35200d5efef647c24.png">
</td>
<td>
Swap Meet<br>(Completed 2023/11/01)
</td>
<td>
Swap a friendly non-golden minion with a random one in Bob's
Tavern.<br>Finish in 1st place as Jandice Barov
</td>
</tr>
<tr>
<td>
<b>Kael’thas Sunstride</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f721e0cd097010c2db5640f8f995d2e7e58efa1e444b7ca71e61d2119a161b06.png">
</td>
<td>
No More Setbacks<br>(Completed 2023/09/24)
</td>
<td>
Passive Every third minion you play gains +2/+2.<br>Finish in 1st place
as Kael’thas Sunstride
</td>
</tr>
<tr>
<td>
<b>Kurtrus Ashfallen</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bb57bb887b9f6d85bbb399464e2dfe886c4d1a5fb670039e8e8c0b0628d7959b.png">
</td>
<td>
Ashfallen in Love With You<br>(Completed 2023/07/19)
</td>
<td>
Passive. Once per turn, after you buy 4 minions, Discover a plain copy
of one of them. ( left!) Passive. Once per turn, after you buy 4
minions, Discover a plain copy of one of them. (Done!)<br>Finish in 1st
place as Kurtrus Ashfallen
</td>
</tr>
<tr>
<td>
<b>Lady Vashj</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3860d62f42e43e75687a666f6da549e0f3ff805316ee98e306d3715d432d7d63.png">
</td>
<td>
Handmaiden's Tail<br>(Completed 2023/05/27)
</td>
<td>
Discover a Spellcraft spell of your Tier or lower. Its effect is
permanent.<br>Finish in 1st place as Lady Vashj
</td>
</tr>
<tr>
<td>
<b>Lich Baz’hial</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/5984e14a91e46f5578fab2a184821029479837408c790d36cce226a369ade097.png">
</td>
<td>
Phylactery Mastery<br>(Completed 2023/08/22)
</td>
<td>
Steal a minion in Bob's Tavern. Take damage equal to its Tavern
Tier.<br>Finish in 1st place as Lich Baz’hial
</td>
</tr>
<tr>
<td rowspan="2">
<b>Lord Barov</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/b148b639c90aad9b3c2028ed482799e83aab3f69961aae3ca4ff8a0693fa4666.png">
</td>
<td>
Lord of the Manor(Completed 2024/08/20)
</td>
<td>
Guess which player will win their next combat. If they win, get 3
Coins.<br>Finish in 1st place as Lord Barov
</td>
</tr>
<tr>
<td>
Salty Bets<br>(Completed 2024/01/23)
</td>
<td>
Guess the correct outcome of Friendly Wagers 3 turns in a row.
</td>
</tr>
<tr>
<td>
<b>Lord Jaraxxus</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3956c3fd0b3df2c6cfecb46b0f60772c85f1291d535aeffd3a0b0bbfb076ab2d.png">
</td>
<td>
INFERNAL OVERDRIVE<br>(Completed 2024/07/12)
</td>
<td>
Choose a friendly Demon. It consumes a minion in Bob's Tavern to gain
its stats.<br>Finish in 1st place as Lord Jaraxxus
</td>
</tr>
<tr>
<td>
<b>Madam Goya</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d7e2dfe73e09dd4b3c6e0f6d8e74b6bac586de215f9fbdeecc17d191f6410aaf.png">
</td>
<td>
Passed With Flying Colors<br>(Completed 2024/06/10)
</td>
<td>
Pass a non-Golden minion.<br>Finish in 1st place as Madam Goya
</td>
</tr>
<tr>
<td>
<b>Maiev</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6918af890e98422bcf2bb5dbd3a2a57301a5a8398db2acf5c1c1f02bd849b9fd.png">
</td>
<td>
Iron Hand of Justice<br>(Completed 2023/12/21)
</td>
<td>
Make a minion in Bob's Tavern Dormant. After 3 turns, get it with
+2/+2.<br>Finish in 1st place as Maiev
</td>
</tr>
<tr>
<td>
<b>Malygos</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/eda55e0eba4bd97eb441a7cc44d56c990a847e32efd1663f5bbeb2d03775d6ee.png">
</td>
<td>
The Essence of Magic<br>(Completed 2023/10/22)
</td>
<td>
Replace a minion with a random one of the same Tavern Tier. (Twice per
turn.)<br>Finish in 1st place as Malygos
</td>
</tr>
<tr>
<td>
<b>Marin the Manager</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/51faa5dd4bafb288b5520f6d833e433a14107b04971b161ac72b158f4c24644d.png">
</td>
<td>
Under New Management<br>(Completed 2024/08/22)
</td>
<td>
Passive On Turn 5, choose a Lesser Trinket to buy.<br>Finish in 1st
place as Marin the Manager
</td>
</tr>
<tr>
<td>
<b>Master Nguyen</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dbd2f8d971d508bfb163854bcc8808c58e67b09bb77cf1aa24d2be01097937f0.png">
</td>
<td>
A Master's Tribute<br>(Completed 2023/08/31)
</td>
<td>
Passive At the start of every turn, choose from 2 new Hero
Powers.<br>Finish in 1st place as Master Nguyen
</td>
</tr>
<tr>
<td>
<b>Millhouse Manastorm</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d43b3fc7f7ec955b34824f1fdefaedee44e550949f276cac64f594be7544113d.png">
</td>
<td>
Light You Up. Sweetcheeks<br>(Completed 2023/09/10)
</td>
<td>
Passive Minions cost 2 Gold. Refresh costs 2 Gold. Tavern Tiers cost (1)
more.<br>Finish in 1st place as Millhouse Manastorm
</td>
</tr>
<tr>
<td>
<b>Millificent Manastorm</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/aa6f2e1fd9dcf285be12131c4e109939cc70eadcfd07d870e387deb6af776b22.png">
</td>
<td>
Meching It happen<br>(Completed 2023/11/05)
</td>
<td>
Passive Whenever you summon a Mech, give it +2 Attack.<br>Finish in 1st
place as Millificent Manastorm
</td>
</tr>
<tr>
<td>
<b>Mr. Bigglesworth</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ff85b0df5bf26537d5d98fc8eb4c7a2e44ecedebe2e110d072b1a54fe566cb76.png">
</td>
<td>
Whose Cat Is This?<br>(Completed 2023/05/30)
</td>
<td>
Passive When a player dies, Discover a minion from their warband. It
keeps any enchantments.<br>Finish in 1st place as Mr. Bigglesworth
</td>
</tr>
<tr>
<td>
<b>Mukla</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/28dcc046ea739dc8a036438539938e469cf06a1fd3b230cd1472c69c2726bd8e.png">
</td>
<td>
Banana Slamma'<br>(Completed 2023/09/29)
</td>
<td>
Passive At the start of your turn, get 2 Bananas and give everyone else
one.<br>Finish in 1st place as Mukla
</td>
</tr>
<tr>
<td>
<b>Murloc Holmes</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ee6f4c3a335863eee684b0d98de8fc44530ade7acee18de6a22581612c8a6676.png">
</td>
<td>
Amazing detective/Genius<br>(Completed 2023/06/15)
</td>
<td>
Look at 2 minions. Guess which one your next opponent had last combat
for a Coin.<br>Finish in 1st place as Murloc Holmes
</td>
</tr>
<tr>
<td>
<b>Mutanas the Devourer</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2dd5a9136566b4896ca735317b5450594197bc609fc79906eb010556652be500.png">
</td>
<td>
Insatiable<br>(Completed 2023/07/27)
</td>
<td>
Remove a friendly minion. Spit its stats onto another. Get 1
Gold.<br>Finish in 1st place as Mutanas the Devourer
</td>
</tr>
<tr>
<td>
<b>Nozdormu</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bba4e1ff4d9cefce0fc760bcd5f9ff152284cdfccb64d01926e53afc3fac8c54.png">
</td>
<td>
Just a Matter of Time<br>(Completed 2023/09/17)
</td>
<td>
Passive Your first Refresh each turn costs (0).<br>Finish in 1st place
as Nozdormu
</td>
</tr>
<tr>
<td>
<b>N’Zoth</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6f0671e5ca8eaf154dc205e04e0e1fbdf00b4e01be0841d7d3e9209ad72bf168.png">
</td>
<td>
Gone Fishing<br>(Completed 2023/06/21)
</td>
<td>
Passive Start the game with a 2/2 Fish that gains all your Deathrattles
in combat.<br>Finish in 1st place as N’Zoth
</td>
</tr>
<tr>
<td>
<b>Onyxia</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/c1d1d6ec0720af1a9e244eb6914b5c0bf91137ac59f8f9d0257037d90b6909da.png">
</td>
<td>
More Dots!<br>(Completed 2023/06/23)
</td>
<td>
Passive Avenge (3): Summon a 3/1 Whelp. It attacks
immediately.<br>Finish in 1st place as Onyxia
</td>
</tr>
<tr>
<td>
<b>Overlord Saurfang</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f242e3b547fb5ce53ea1eb1db3167b92fa15c831e799b5de392bbe857bc08fee.png">
</td>
<td>
Might of the Horde<br>(Completed 2023/12/02)
</td>
<td>
Give a minion in the Tavern + /+ . (Doubles every turns!) Give a minion
in the Tavern + /+ . (Doubles in (turn, turns)!)<br>Finish in 1st place
as Overlord Saurfang
</td>
</tr>
<tr>
<td>
<b>Ozumat</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dd50f028a96656f724020b52592ef9f4de6a8144713af427aa605c21c8a685a2.png">
</td>
<td>
Throne of the Tides<br>(Completed 2023/08/27)
</td>
<td>
Passive. Start of Combat: Summon a / Tentacle with Taunt. (Gains +1/+1
after you sell a minion!)<br>Finish in 1st place as Ozumat
</td>
</tr>
<tr>
<td>
<b>Patches the Pirate</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/8ff3c758832f1ba6473046d775b1e9c2f4e159194cdbdee3c0151ea1b0685e29.png">
</td>
<td>
Who's In Charrrrge?<br>(Completed 2024/03/03)
</td>
<td>
Get a Pirate. After you buy a Pirate, your next Hero Power costs (1)
less.<br>Finish in 1st place as Patches the Pirate
</td>
</tr>
<tr>
<td>
<b>Patchwerk</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/35aca69a6618c085bf4bf8a3f6ff8b41154a30f377c256a50634053e72cf02a2.png">
</td>
<td>
Werkin' It<br>(Completed 2023/06/10)
</td>
<td>
Passive Start with 60 Health instead of 30.<br>Finish in 1st place as
Patchwerk
</td>
</tr>
<tr>
<td>
<b>Professor Putricide</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/6b13a07802d9632a024835b0687c01dd51cedc309ff9c3fc4ad49b1dd10d7bbb.png">
</td>
<td>
Plague Perfected<br>(Completed 2023/08/07)
</td>
<td>
Craft a custom Undead. ( Creations left!)<br>Finish in 1st place as
Professor Putricide
</td>
</tr>
<tr>
<td>
<b>Pyramad</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/487af9778a9118e33f66671889fdbc755f1e95754cf8b33193d70d5bd071259e.png">
</td>
<td>
Brick House<br>(Completed 2023/09/15)
</td>
<td>
Give a minion + Health. (Gains +1 Health each turn you don't use
this!)<br>Finish in 1st place as Pyramad
</td>
</tr>
<tr>
<td>
<b>Queen Azshara</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/351a78bb20006f04421dc0da1f2bd4273dbc2b66e7365bd883efd751d87f8d2c.png">
</td>
<td>
The Water's Fine<br>(Completed 2023/07/29)
</td>
<td>
Passive. When your warband reaches 25 total Attack, begin your Naga
Conquest. Passive. When your warband reaches 25 total Attack, begin your
Naga Conquest. ( left!)<br>Finish in 1st place as Queen Azshara
</td>
</tr>
<tr>
<td>
<b>Queen Wagtoggle</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/0b0fbdc1cc12d3e681b6c536418f36b4d54418aed245d0bc6bff60a2c565c4f3.png">
</td>
<td>
Me Win Candle!<br>(Completed 2023/11/05)
</td>
<td>
Passive. Start of Combat: Give a friendly minion of each type stats
equal to their Tavern Tiers.<br>Finish in 1st place as Queen Wagtoggle
</td>
</tr>
<tr>
<td>
<b>Ragnaros</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d2eb043afaec5a9543a9012820668e927684cb2bbb908291dd7f692f2c64148c.png">
</td>
<td>
By fire be Purged!<br>(Completed 2023/06/12)
</td>
<td>
Passive After you kill 25 enemy minions, get Sulfuras. (
left!)<br>Finish in 1st place as Ragnaros
</td>
</tr>
<tr>
<td>
<b>Rakanishu</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2bdb3634ac6e7554e01f7750afebbc5401640b3ee9df44531d2522ac1da67d74.png">
</td>
<td>
Hot! Hot! Hot!<br>(Completed 2023/08/13)
</td>
<td>
Give a minion stats equal to your Tavern Tier.<br>Finish in 1st place as
Rakanishu
</td>
</tr>
<tr>
<td>
<b>Reno Jackson</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ef2c29f1dacc4df898394cc80fd7ded5d0c2f3c3bf13cb9c7fff556bba5485dc.png">
</td>
<td>
Aren't I Amazing?<br>(Completed 2023/09/08)
</td>
<td>
Make a friendly minion Golden. (Once per game.)<br>Finish in 1st place
as Reno Jackson
</td>
</tr>
<tr>
<td>
<b>Rock Master Voone</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/036db476361284d598f486f0a6f070c543f08b803955d1bc97efa1175acecb50.png">
</td>
<td>
Ready to Rock!<br>(Completed 2023/06/16)
</td>
<td>
Passive. At the end of every 3 turns, get a plain copy of the left-most
card in your hand. ( (turn, turns) left!) <br>Finish in 1st place as
Rock Master Voone
</td>
</tr>
<tr>
<td>
<b>Rokara</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f390ceaa1a0434817f9262aa0e573509a53798fe5bf721b062f01c85a1fa3a02.png">
</td>
<td>
Not Too Young To Fight<br>(Completed 2023/07/01)
</td>
<td>
Passive. After a friendly minion kills an enemy, give it +1 Attack
permanently.<br>Finish in 1st place as Rokara
</td>
</tr>
<tr>
<td>
<b>Scabbs Cutterbutter</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/42b6ce697b70a7174441ab6f8e954c74e65262e197f2657647b982f7f896a3cd.png">
</td>
<td>
You made this? I made this!<br>(Completed 2023/05/30)
</td>
<td>
Discover a plain copy of a minion from your next opponent's
warband.<br>Finish in 1st place as Scabbs Cutterbutter
</td>
</tr>
<tr>
<td>
<b>Shudderwock</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/bfc5bab0926fcbbdf2f1d167e8022f7edc8047993064edf1ecadc8aa09e5aad4.png">
</td>
<td>
O Frabjous Day!<br>(Completed 2023/11/04)
</td>
<td>
Trigger a friendly minion's Battlecry.<br>Finish in 1st place as
Shudderwock
</td>
</tr>
<tr>
<td>
<b>Silas Darkmoon</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f6c8a988042d2838d33c981b3ad8c52068bfee24b5bad28d0fdbdb614202a9c6.png">
</td>
<td>
Greatest Show On Azeroth<br>(Completed 2023/06/25)
</td>
<td>
Passive. Darkmoon Tickets are in the Tavern! Get 3 to Discover a minion
from your Tavern Tier.<br>Finish in 1st place as Silas Darkmoon
</td>
</tr>
<tr>
<td>
<b>Sindragosa</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ce6fca97b7ee6fbfd31aeee774f15010ed4ad27dbae46833c2397acd184bd014.png">
</td>
<td>
Icy Wings of Northrend<br>(Completed 2023/11/05)
</td>
<td>
Freeze a minion in Bob's Tavern. Passive: At the end of your turn,
Frozen minions get +2/+1.<br>Finish in 1st place as Sindragosa
</td>
</tr>
<tr>
<td>
<b>Sir Finley Mrrgglton</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3017d5bc0d7918641ceaf1a0f98286170dd76f51640cdc0e46ea2d1d60ee81af.png">
</td>
<td>
How About a New Adventure?<br>(Completed 2023/08/31)
</td>
<td>
Passive At the start of the game, Discover a Hero Power.<br>Finish in
1st place as Sir Finley Mrrgglton
</td>
</tr>
<tr>
<td rowspan="3">
<b>Sire Denatureius</b>
</td>
<td rowspan="3">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/12c1f9737729cc2f32d414bbfd7ece06833cfa96ff030406bd1db1a78be9d7fc.png">
</td>
<td>
I AM Revendreth<br>(Completed 2023/05/28)
</td>
<td>
Passive. At the start of the game, choose one of two Quests.<br>Finish
in 1st place as Sire Denatureius
</td>
</tr>
<tr>
<td>
Case Closed<br>(Completed 2023/07/14)
</td>
<td>
Complete 10 different Quests
</td>
</tr>
<tr>
<td>
Handsomely Rewarded<br>(Completed 2023/07/14)
</td>
<td>
Earn 15 different Quest Rewards.
</td>
</tr>
<tr>
<td rowspan="2">
<b>Skycap’n Kragg</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/257b9cf7c78d018570ab0fcf4501719bc58019afd4f0d531f050a903e4f8ec61.png">
</td>
<td>
I'm the Cap'n Now<br>(Completed 2023/07/11)
</td>
<td>
Gain Gold. Increases each turn. (Once per game.)<br>Finish in 1st place
as Skycap’n Kragg
</td>
</tr>
<tr>
<td>
Rainy Day Fund<br>(Completed 2023/06/08)
</td>
<td>
Upgrade your Tavern Tier twice in one turn as Skycap’n Kragg
</td>
</tr>
<tr>
<td>
<b>Snake Eyes</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ab242e1c6ed6993963d7362ceefa0ec34e183a9241653f73f6b5b43525172f9a.png">
</td>
<td>
This is how I roll<br>(Completed 2023/12/09)
</td>
<td>
Roll a 6-sided die. Gain that much Gold. (Cannot be used again for that
many turns!)<br>Finish in 1st place as Snake Eyes
</td>
</tr>
<tr>
<td>
<b>Sneed</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/feb36c75696b071a11ba92b88b707cc257e78000e34d79ffb0fd6408af212984.png">
</td>
<td>
Mind if I Roll Sneed?<br>(Completed 2023/0622)
</td>
<td>
Give a minion "Deathrattle: Summon a random minion from a Tavern Tier
lower."<br>Finish in 1st place as Sneed
</td>
</tr>
<tr>
<td>
<b>Sylvanas Windrunner</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dbe5dc9a94396a2e96ceca5d9ef7db743f269a1563538ed9541ff83a370de47d.png">
</td>
<td>
Let None Survive!<br>(Completed 2023/09/24)
</td>
<td>
Give +2/+1 to your minions that died last combat.<br>Finish in 1st place
as Sylvanas Windrunner
</td>
</tr>
<tr>
<td>
<b>Tae'thelan Bloodwatcher</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/172f7600ab278bde0612c1e675a2d9fd81b80a5799e6b141acfbb4c407a67af7.png">
</td>
<td>
Rewarding Research<br>(Completed 2023/12/20)
</td>
<td>
Passive Every third Tavern spell you buy costs (0). <br>Finish in 1st
place as Tae'thelan Bloodwatcher
</td>
</tr>
<tr>
<td>
<b>Tamsin Roame</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/24a33af73786549cfc465e34350bacaf03bcfef217e59460ba667d3ac8a0f4c7.png">
</td>
<td>
A-Roame-atherapy<br>(Completed 2023/10/22)
</td>
<td>
Start of Combat: Destroy your lowest Health minion. Give its stats to
your other minions.<br>Finish in 1st place as Tamsin Roame
</td>
</tr>
<tr>
<td>
<b>Tavish Stormpike</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d82e87fb69bd11aec529281ccc903b4ea9e87f29e3ffdf7a54a4a57e972954d0.png">
</td>
<td>
360 No Scope!<br>(Completed 2023/05/28)
</td>
<td>
Take aim! Start of Combat: Deal damage to your target. (Doubles every
turns!) Take aim! Start of Combat: Deal damage to your target. (Doubles
in (turn, turns)!)<br>Finish in 1st place as Tavish Stormpike
</td>
</tr>
<tr>
<td>
<b>Teron Gorefiend</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/ef5a9e031e2cdcdda9fa55124294247b3502d97e888f3c752d8cddf548f46d18.png">
</td>
<td>
That was Goreious!<br>(Completed 2023/11/17)
</td>
<td>
Choose a friendly minion. Start of Combat: Destroy it. Once you have
space, resummon an exact copy.<br>Finish in 1st place as Teron Gorefiend
</td>
</tr>
<tr>
<td>
<b>Tess Greymane</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/17a12a8a7335de3807f74c8eb00fdb30e12c518b5f916eaa7507072a86087580.png">
</td>
<td>
Couldn't Do It Without You<br>(Completed 2023/11/18)
</td>
<td>
Refresh Bob's Tavern with your last opponent's warband.<br>Finish in 1st
place as Tess Greymane
</td>
</tr>
<tr>
<td rowspan="2">
<b>The Curator</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2198929a67a341765cd3b60141f40522c6e51e1ef47d6e29c56c8ed7abad7a2b.png">
</td>
<td>
Menagerie Manager<br>(Completed 2023/09/24)
</td>
<td>
Passive. Start the game with a 1/1 Amalgam with Venomous and all minion
types.<br>Finish in 1st place as The Curator
</td>
</tr>
<tr>
<td>
LOST "DOG". CALL CURATOR<br>(Completed 2023/09/21)
</td>
<td>
End your turn with the Amalgam in play while not playing as The Curator
</td>
</tr>
<tr>
<td rowspan="2">
<b>The Great Akazamzarak</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/19e004e0d5733af2e9e6a698f6e7606d465fb8917c9f20b55b4e4318f1b2eb7c.png">
</td>
<td>
The Magician's Code<br>(Completed 2023/09/22)
</td>
<td>
Discover a Secret. Put it into the battlefield.<br>Finish in 1st place
as The Great Akazamzarak
</td>
</tr>
<tr>
<td>
Comeback King<br>(Completed 2023/09/22)
</td>
<td>
Finish in 1st place after having your Ice Block triggered as The Great
Akazamzarak
</td>
</tr>
<tr>
<td>
<b>The Jailer</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dd00d910a25ecacf576f4a9eef0aabef472ebb063d99af7771ac5355c6a4cfa4.png">
</td>
<td>
Master of Maw<br>(Completed 2023/11/13)
</td>
<td>
Give a minion + /+ . Upgrades after nine friendly minions die. ( left!)
Give a minion + /+ . Upgrades after nine friendly minions die. (
left!)<br>Finish in 1st place as The Jailer
</td>
</tr>
<tr>
<td>
<b>The Lich King</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/d5ecee3ec0546d517e6ac31803f76126787e4d7a4c64ab0160db9d0b4f64e61e.png">
</td>
<td>
There Must Always Be a Lich King<br>(Completed 2023/07/16)
</td>
<td>
Give a minion Reborn until next turn.<br>Finish in 1st place as The Lich
King
</td>
</tr>
<tr>
<td>
<b>The Nameless One</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/b1c0549d7a441bc3753353748eb06cdb93be99ccfda77be36741b635885f41e0.png">
</td>
<td>
No Ones a Winner<br>(Completed 2024/04/27)
</td>
<td>
Passive. Start of Game: Copy your teammate's Hero Power.<br>Finish in
1st place as The Nameless One
</td>
</tr>
<tr>
<td>
<b>The Rat King</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/13de6035422fd8200c3ec5c095a1a75d92767b395e6be340633d641c84e85fc1.png">
</td>
<td>
There Must Always Be a Rat King<br>(Completed 2023/11/09)
</td>
<td>
Discover a minion of a specific minion type. Swaps type each
turn.<br>Finish in 1st place as The Rat King
</td>
</tr>
<tr>
<td>
<b>Thorim, Stormlord<b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/7ade2fe78e8170fe89a0329f3989f7797702fd5b046f6a9c1f8a6d9d00fa349b.png">
</td>
<td>
A Tier above the rest<br>(Completed 2023/01/29)
</td>
<td>
Meetings always reach quorum when they're led by Thorim. Passive. At the
start of the game, Discover a Tier 7 minion to get after you spend 65
Gold.<br>Finish in 1st place as Thorim, Stormlord
</td>
</tr>
<tr>
<td rowspan="2">
<b>Tickatus</b>
</td>
<td rowspan="2">
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f9de22070f0cf2698497bfed225dc408d2dfa18d4a56f390497cbdb561fc0b2a.png">
</td>
<td>
Ticketmaster<br>(Completed 2023/07/21)
</td>
<td>
Passive Every 4 turns, Discover a Darkmoon Prize. ( (turn, turns)
left!)<br>Finish in 1st place as Tickatus
</td>
</tr>
<tr>
<td>
And YOU get Prize Cards!<br>(Completed 2023/08/23)
</td>
<td>
Play 16 different Darkmoon Prizes.
</td>
</tr>
<tr>
<td>
<b>Trade Prince Gallywix</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/1dc20a3d033232bea3a95e1b53659be58fc32a34cb6973da45c45e2288fba644.png">
</td>
<td>
Profit Prophet<br>(Completed 2023/11/26)
</td>
<td>
Passive After you sell a minion, get 1 extra Gold next turn.<br>Finish
in 1st place as Trade Prince Gallywix
</td>
</tr>
<tr>
<td>
<b>Vanndar Stormpike</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f8065b689b8dfde3b8c25f19f4cb6619241e00b78c3b8ce9f549318c55ad0390.png">
</td>
<td>
Stormpike Stardom<br>(Completed 2023/11/11)
</td>
<td>
Passive Avenge (2): Give your minions +1 Health permanently.<br>Finish
in 1st place as Vanndar Stormpike
</td>
</tr>
<tr>
<td>
<b>Varden Dawngrasp</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/dd3acdbb568b6a9ad80181048b2151d287e8b751d30435bb3f301b96d08aa4db.png">
</td>
<td>
Chill Out<br>(Completed 2023/08/27)
</td>
<td>
Passive. After Bob's Tavern is Refreshed, copy his highest Tier minion
and Freeze them both.<br>Finish in 1st place as Varden Dawngrasp
</td>
</tr>
<tr>
<td>
<b>Vol’jin</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/684eb855c8eb91247a4e4e968ea63f94d320d0563ac64bac76d357abbae7080e.png">
</td>
<td>
Da Voodoo Shuffle<br>(Completed 2024/02/03)
</td>
<td>
Choose two minions. Swap their stats.<br>Finish in 1st place as Vol’jin
</td>
</tr>
<tr>
<td>
<b>Xyrella</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/427f8059ebf91ccd337c4e9b92ea8fda0059c8e87f9e431bbb6b8987671f7c1d.png">
</td>
<td>
Brightest Kid Around<br>(Completed 2023/07/09)
</td>
<td>
Choose a minion in Bob's Tavern to add to your hand. Set its stats to
2.<br>Finish in 1st place as Xyrella
</td>
</tr>
<tr>
<td>
<b>Yogg-Saron, Hope’s End</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/f1dc1cda3b54e88565c0e68cf7b6daede62e526b13ac92e52259bba6be961818.png">
</td>
<td>
Madness in the Tavern<br>(Completed 2023/11/05)
</td>
<td>
Add a random minion in Bob's Tavern to your hand. Give it
+1/+1.<br>Finish in 1st place as Yogg-Saron. Hope’s End
</td>
</tr>
<tr>
<td>
<b>Ysera</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/3958796b08f99af07900ad211c849806c3e3303885f5834ab0029ad8a02fed32.png">
</td>
<td>
A Dream Come True<br>(Completed 2024/05/01)
</td>
<td>
Passive Bob always offers an extra Dragon whenever the Tavern is
Refreshed.<br>Finish in 1st place as Ysera
</td>
</tr>
<tr>
<td>
<b>Y'Shaarj</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/9662e21c32847be6f13d7d54010ed8e33d84dfe70aefc7bda6ddd2d3b2d09bbd.png">
</td>
<td>
Rage Your Dream<br>(Completed 2023/07/16)
</td>
<td>
Start of Combat: Summon and get a random minion from your Tavern
Tier.<br>Finish in 1st place as Y'Shaarj
</td>
</tr>
<tr>
<td>
<b>Zephrys the Great</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/8a3ce8409e37fa1f637fbe1889b4fd47bb99b989c905af6660b2b2671a6fed1a.png">
</td>
<td>
Phenomental Cosmic Powers<br>(Completed 2023/05/27)
</td>
<td>
If you have two copies of a minion, find the third. ( (Wish, Wishes)
left!)<br>Finish in 1st place as Zephrys the Great
</td>
</tr>
<tr>
<td>
<b>Zerek, Master Cloner</b>
</td>
<td>
<img src="https://d15f34w2p8l1cc.cloudfront.net/hearthstone/2ac9daa1401f8ff0daa5a6a6beba6ed21742d60f1671741ad35f5ed234f374cf.png">
</td>
<td>
Master Cloner<br>(Completed 2024/12/30)
</td>
<td>
Summon an exact copy of a friendly minion. (Once per game.)<br>Finish in
1st place as Zerek, Master Cloner
</td>
</tr>
</table>
]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>Hearthstone</tag>
        <tag>Battlegrounds</tag>
        <tag>game</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimize sgemm on RISC-V platform</title>
    <url>/2024/02/26/103_sgemm_riscv_en/</url>
    <content><![CDATA[<p>This project records the process of optimizing SGEMM
(single-precision floating point General Matrix Multiplication) on the
riscv platform.</p>
<p><img src=/images/sgemm/data/result_all.png width=70% /></p>
<span id="more"></span>
<p>This project records the process of optimizing SGEMM
(single-precision floating point General Matrix Multiplication) on the
riscv platform.</p>
<p>General Matrix Multiplication (GEMM) is one of the core computing
units in deep learning frameworks, widely used for implementing
operators such as Convolution, Full Connection, and Matmul.</p>
<p>I conducted experiments and explorations on the <strong>Allwinner
Nezha D1</strong> development board. <code>Versions 0 through 5</code>
were implemented using the C language, while
<code>versions 6 through 9</code> partially utilized assembly language,
involving the <code>RISC-V V</code> extension instructions.</p>
<p>Note: Unlike some other GEMM optimization projects, this project uses
<strong>row-major order</strong> matrices exclusively. Because I prefer
<strong>row-major order</strong>!</p>
<h1 id="prerequisite-knowledge">Prerequisite Knowledge</h1>
<p><img src=/images/sgemm/pics/riscv.gif width=50% /></p>
<p><a href="https://riscv.org/">RISC-V</a> is an open standard
Instruction Set Architecture (ISA) enabling a new era of processor
innovation through open collaboration.</p>
<hr />
<p><img src=/images/sgemm/pics/GEMM.png width=70% /></p>
<p><a
href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">GEMM</a>
General matrix multiply, one of the Basic Linear Algebra
Subprograms.</p>
<hr />
<p><a href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a> stands for
Floating-point Operations Per Second, also known as peak speed per
second. It refers to the number of floating-point operations executed
per second. One <strong>gigaflop (GFLOPS)</strong> equals ten to the
power of nine (10^9) floating-point operations per second.</p>
<p>The computational complexity of matrix multiplication is calculated
as <code>2 * M * N * K</code>. The ratio of computational complexity to
time taken gives the GFLOPS of the current GEMM version. - Multiplying
by 2 is because each operation involves one multiplication and one
addition.</p>
<h1 id="prepare">Prepare</h1>
<p>The related code is located in <code>./prepare/</code>.</p>
<h2 id="test-cross-compilation">Test Cross-Compilation</h2>
<p>I use the <strong>Allwinner Nezha D1</strong> development board, and
downloaded the cross-compilation link from <a
href="https://xuantie.t-head.cn/community/download?id=4090445921563774976">here</a>.</p>
<p>For detailed instructions, please refer to the <a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/blob/main/prepare/README.md">readme</a>.</p>
<h2 id="memory-bandwidth-test">Memory Bandwidth Test</h2>
<p>I conducted memory bandwidth tests on the development board using the
following projects:</p>
<ul>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/2.memcpy_bandwidth_test">2.memcpy_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/3.flw_bandwidth_test">3.flw_bandwidth_test</a></li>
<li><a
href="https://github.com/Zhao-Dongyu/sgemm_riscv/tree/main/prepare/4.vlw_bandwidth_test">4.vlw_bandwidth_test</a></li>
</ul>
<p><img src=/images/sgemm/pics/memory_bandwidth_test.png width=70% /></p>
<h2 id="roofline-model">Roofline Model</h2>
<p><a href="https://en.wikipedia.org/wiki/Roofline_model">Roofline</a>
proposes a method for quantitative analysis using "Operational
Intensity" and provides a formula for the theoretical performance limit
achievable on computational platforms.</p>
<p>According to <a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL
Public Course | RISC-V Technical Analysis</a>:</p>
<ul>
<li>The computing power of D1 can reach <code>4 GFlops</code> (<span
class="citation" data-cites="1GHz">@1GHz</span>).</li>
<li>Memory: <code>2.727 GB/s</code> (DDR3 792 MHz).
<ul>
<li>Although I measured the highest as <code>2.592 GB/s</code>, there
may be some problems somewhere?</li>
<li>Let's trust Sensetime for now, temporarily accept its value.</li>
</ul></li>
</ul>
<p><img src=/images/sgemm/pics/roofline.png width=70% /></p>
<h1 id="sgemm-optimization">SGEMM Optimization</h1>
<p>Related code is located in <code>./sgemm/</code>.</p>
<h2 id="usage">Usage</h2>
<p>Take <code>step0</code> as an example, you need to edit the Makefile
first to configure your cross-compilation chain.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> sgemm/step0/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb push test_bl_sgemm_step0.x ./.</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">adb shell <span class="string">&#x27;./test_bl_sgemm_step0.x&#x27;</span></span></span><br></pre></td></tr></table></figure>
<h2 id="version-0-naive-version">Version 0: Naive Version</h2>
<p>This version seems to be <strong>the most intuitive</strong> to me,
after all, this is how I learned, understood, and computed matrix
multiplication: &gt; Multiply one row of A by one column of B to get one
element of C.</p>
<p><img src=/images/sgemm/data/result_0.png width=70% /></p>
<p><img src=/images/sgemm/pics/step0.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;          <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;      <span class="comment">// Start 0-st loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>I think <code>version 0</code> very well explains the formula <span
class="math inline">\(C_{mn} = \sum_{k=1}^{K} A_{mk}B_{kn}\)</span>.</p>
<p>However, this version has obvious shortcomings: on a platform with a
theoretical computing power of <code>4 GFLOPS</code>, it only achieves a
maximum computational performance of <code>0.03 GFLOPS</code>. This is
because <strong>the access to matrix B has a very low cache hit rate,
i.e., "poor spatial locality"</strong>. Throughout the calculation, it
is equivalent to accessing matrix B many, many times.</p>
<p>It is advisable to access the elements of multi-dimensional arrays in
sequential order. This can improve the spatial locality of memory access
and make it more friendly to the cache.</p>
<p>Furthermore, it can be observed that with the increase in size, the
performance fluctuates significantly. Analysis of the data shows that
when m=n=k is 128 164 192 228 256 288 320 352 384, the performance is
poor. These numbers differ by 32, and 32 * 4 (sizeof(float)) = 128
B.</p>
<p>It is speculated that the performance fluctuation is related to
cacheline and hardware prefetching -- cacheline = 64B, after cache miss,
hardware prefetching, i.e., HWPrefetcher, reads one more cacheline.</p>
<h2 id="version-1-loop-interchange-version">Version 1: Loop Interchange
Version</h2>
<p>Reusing data in the cache is the most basic and efficient use of
cache. For nested loops, <code>loop interchange</code>,
<code>loop reversal</code>, <code>loop fusion</code>,
<code>loop distribution</code>, <code>loop tiling</code>,
<code>loop unrolling and jam</code>, etc., can be used to improve
program performance.</p>
<p>Selecting an appropriate loop transformation method can both maintain
the semantics of the program and improve its performance.</p>
<p><img src=/images/sgemm/data/result_1.png width=70% /></p>
<p><img src=/images/sgemm/pics/step1.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i ++ ) &#123;              <span class="comment">// Start 2-th loop</span></span><br><span class="line">    <span class="keyword">for</span> ( p = <span class="number">0</span>; p &lt; k; p ++ ) &#123;          <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j ++ ) &#123;      <span class="comment">// Start 0-nd loop</span></span><br><span class="line">            C( i, j ) += A( i, p ) * B( p, j );</span><br><span class="line">        &#125;                                 <span class="comment">// End   0-th loop</span></span><br><span class="line">    &#125;                                     <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                         <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>Compared with <code>version 0</code>, <code>version 1</code> has
better spatial locality for the operation on matrix B, and the
performance has been greatly improved (especially for larger sizes,
while for m = n = k &lt;= 68, the efficiency of version 0 is
higher).</p>
<p>Adjusting the order of m, n, and k does not affect the result (i.e.,
maintaining the semantics of the program), but it can affect the
performance. Testing the performance of different loop orders (using the
Allwinner Nezha D1 platform, with m=n=k=512 as an example)</p>
<table>
<thead>
<tr class="header">
<th>Loop Order</th>
<th>GFLOPS</th>
<th>Analysis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MNK</td>
<td>0.012</td>
<td>High cache miss for accessing B</td>
</tr>
<tr class="even">
<td>MKN</td>
<td>0.180</td>
<td></td>
</tr>
<tr class="odd">
<td>NMK</td>
<td>0.012</td>
<td>High cache miss for accessing B</td>
</tr>
<tr class="even">
<td>NKM</td>
<td>0.009</td>
<td>High cache miss for accessing A</td>
</tr>
<tr class="odd">
<td>KMN</td>
<td>0.165</td>
<td></td>
</tr>
<tr class="even">
<td>KNM</td>
<td>0.009</td>
<td>High cache miss for accessing A</td>
</tr>
</tbody>
</table>
<p>However, the hardware utilization of <code>version 1</code> is still
very low, and further optimizations are needed.</p>
<h2 id="version-2-blocking-version">Version 2: Blocking Version</h2>
<p><img src=/images/sgemm/data/result_2.png width=70% /></p>
<p><img src=/images/sgemm/pics/step2.gif width=70% /></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;          <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;      <span class="comment">// Start 1-st loop</span></span><br><span class="line">        AddDot_4x4_opt( k, &amp;A( i, <span class="number">0</span> ), lda, &amp;B( <span class="number">0</span>, j ), ldb, &amp;C( i, j ), ldc );</span><br><span class="line">    &#125;                                          <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                              <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p><img src=/images/sgemm/pics/gemm_block.png width=70% /></p>
<p>To avoid unnecessary cache swapping, blocking processing is
performed. <a href="https://zhuanlan.zhihu.com/p/342923482">Discussing
Why Blocking Matrix Optimization Works</a> is a good read, I recommend
learning from it.</p>
<p>After performing block operations in <code>version 2</code>, the
performance is still not satisfactory. This is because, although this
version superficially implements blocking logic, there are still some
small tricks in the calculation within the block that have not been
applied.</p>
<h2 id="version-3-blocked-optimization-version">Version 3: Blocked
Optimization Version</h2>
<p><img src=/images/sgemm/data/result_3.png width=70% /></p>
<p><img src=/images/sgemm/pics/step3.gif width=70% /></p>
<p><code>AddDot_4x4_opt</code> has been added.</p>
<p>Several tricks are mentioned in<a
href="https://github.com/flame/blislab/blob/master/tutorial.pdf">BLISlab-tutorial</a>:</p>
<ul>
<li>2.4.2 Loop unrolling
<ul>
<li>Updating loop index i and the pointer cp every time through the
inner loop creates considerable overhead. For this reason, a compiler
will perform loop unrolling.</li>
</ul></li>
<li>2.4.3 Register variables
<ul>
<li>Notice that computation can only happen if data is stored in
registers. A compiler will automatically transform code so that the
intermediate steps that place certain data in registers is
inserted.</li>
</ul></li>
</ul>
<p>After using these tricks, this version has significantly improved
performance!</p>
<p>However, for larger matrix sizes, the performance of this version is
still relatively low. Upon investigation, for example, after accessing
B[0,0], B[0,1], B[0,2], B[0,3], when accessing B[1,0], when the size is
large, there must be a <code>cache miss</code>. Therefore, it would be
great if the data could be rearranged in advance.</p>
<h2 id="version-4-b-prepack-version">Version 4: B prepack Version</h2>
<p><img src=/images/sgemm/data/result_4.png width=70% /></p>
<p><img src=/images/sgemm/pics/step4.gif width=70% /></p>
<p>I assume matrix B is <strong>parameter</strong>, so we can perform
the <code>pack</code> operation in advance. Version 4
<code>prepack</code> matrix B, leading to further performance
improvement!</p>
<p>The reason for the performance improvement is evident: there is a
significant reduction in <code>cache misses</code> when accessing matrix
B. This is the first time I deeply realized the importance of
<code>prepacking neural network weights</code> before model
inference.</p>
<p>It can be seen that when the size is relatively large, the
performance still declines. This should be due to a high number of cache
misses when accessing matrix A. Should we pack A?</p>
<p>I assume matrix A is <strong>input</strong>, so packing A cannot be
done in advance and must be included in the overall timing. Is it
necessary?</p>
<h2 id="version-5-a-pack-b-prepack-version">Version 5: A pack &amp; B
prepack Version</h2>
<p><img src=/images/sgemm/data/result_5.png width=70% /></p>
<p><img src=/images/sgemm/pics/step5.gif width=70% /></p>
<p>Based on Version 4, Version 5 performs packing on matrix A.</p>
<p>Here, since matrix A is assumed to be an input, packing A needs to be
performed during computation, and this time consumption needs to be
included in the timing.</p>
<p>The results are still pleasing, especially with large matrix sizes,
achieving further performance improvements.</p>
<p>I initially approached this experiment with a trial-and-error
mindset, considering the additional read of A and writing of packA. It
seems the main challenge ahead lies in combating cache misses.</p>
<p>The current optimization direction has reached its limit. It's worth
trying to do some <code>preload</code> during the calculation
process.</p>
<p>Next, we'll move to assembly, work on vector calculations, and
implement <code>preload</code> in assembly.</p>
<h2 id="version-6-assembly-version">Version 6: Assembly Version</h2>
<p><img src=/images/sgemm/data/result_6.png width=70% /></p>
<p>Brief explanation: A is not packed, but B is prepacked with 16
numbers.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;       <span class="comment">// Start 2-nd loop</span></span><br><span class="line">    <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">    <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">    <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;   <span class="comment">// Start 1-st loop</span></span><br><span class="line">        <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">        <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>Regarding the use of rvv instructions, I believe <code>vsetvli</code>
is essential, and <code>vfmacc.vf</code> is the mainstay.</p>
<p>I have learned a lot from <a
href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL Course | RISC-V
Technical Analysis</a>. They are truly professional! I recommend
learning theoretical guidance and knowledge points from them, paying
tribute to OpenPPL!</p>
<p>As for assembly operators, there are many details in assembly, and I
strongly complain: <strong>writing assembly is really annoying!
Especially the debugging process, it's torturous.</strong> The last time
I wrote assembly was during my undergraduate classes. Picking it up
again brings some novelty and excitement, and being able to control the
execution of operators at a very fine granularity gives a great sense of
accomplishment.</p>
<p>Regarding how the assembly files are implemented specifically, I
believe the fastest way is to look at the assembly code. I won't explain
it further here.</p>
<p>It should be noted that this version's performance is very poor. Why
is that? It's another issue of <strong>loop order</strong>.</p>
<h2 id="version-7-assembly-version-with-loop-order-swapped">Version 7:
Assembly Version with Loop Order Swapped</h2>
<p><img src=/images/sgemm/data/result_7.png width=70% /></p>
<p>Brief explanation: A is not packed, but B is prepacked with 16
numbers.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; n; j += DGEMM_NR ) &#123;       <span class="comment">// Start 2-st loop</span></span><br><span class="line">    <span class="type">int</span> nb = DGEMM_NR;</span><br><span class="line">    <span class="keyword">if</span>((n - j) &lt; DGEMM_NR) nb = n - j; </span><br><span class="line">    <span class="keyword">for</span> ( i = <span class="number">0</span>; i &lt; m; i += DGEMM_MR ) &#123;   <span class="comment">// Start 1-nd loop</span></span><br><span class="line">        <span class="type">int</span> mb = DGEMM_MR;</span><br><span class="line">        <span class="keyword">if</span>((m - i) &lt; DGEMM_MR) mb = m - i; </span><br><span class="line">        RvvSgemm4x16(   nb,                 <span class="comment">// nr &lt;= 16, a0</span></span><br><span class="line">                        mb,                 <span class="comment">// mr &lt;= 4,  a1</span></span><br><span class="line">                        k,                  <span class="comment">// astride = k*sizeof(float), a2</span></span><br><span class="line">                        &amp;A[i * k],          <span class="comment">// mr * k,   a3</span></span><br><span class="line">                        &amp;packB[j * k],      <span class="comment">// k * 16,   a4</span></span><br><span class="line">                        &amp;C( i, j ),         <span class="comment">// mr * nr,  a5</span></span><br><span class="line">                        n * <span class="keyword">sizeof</span>(<span class="type">float</span>),  <span class="comment">// Len(N) * sizeof(float), a6</span></span><br><span class="line">                        bias</span><br><span class="line">                    );</span><br><span class="line">    &#125;                                       <span class="comment">// End   1-st loop</span></span><br><span class="line">&#125;                                           <span class="comment">// End   2-nd loop</span></span><br></pre></td></tr></table></figure>
<p>Reversing the order of loops, starting with the n-direction followed
by the m-direction, significantly improves performance.</p>
<p>However, the performance of large-sized matrices is still not very
good. The root cause remains in <code>memory access</code>. The
computation of large-sized matrices in the <code>roofline model</code>
is considered <code>compute-bound</code>, where ideally the
<code>compute time</code> and <code>memory access time</code> should
overlap as much as possible. Currently, a significant amount of time is
spent on memory access (mostly due to <code>cache miss</code>!).</p>
<h2 id="version-8-assembly-version-with-preload">Version 8: Assembly
Version with Preload</h2>
<p><img src=/images/sgemm/data/result_8.png width=70% /></p>
<p>Brief Explanation: Matrix A is not <code>packed</code>, while Matrix
B undergoes <code>prepackaging</code> of 16 elements and includes a
<code>preload</code> operation.</p>
<p>The performance is explosive! It reaches a maximum of
<code>2.212 GFLOPS</code>.</p>
<p>Core operations:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfmacc.vf v16,  ft0, v0</span><br><span class="line">vlw.v v4, (bp0)         # b0&#x27;-&gt;v4</span><br><span class="line">flw fs4, 384(bp0)       # pre-load B</span><br><span class="line">addi bp0,bp0,64</span><br><span class="line">vfmacc.vf v20,  ft1, v0</span><br></pre></td></tr></table></figure>
<p>Inserting some <code>load</code> operations between
<code>vfmacc.vf</code> instructions preloads the data that will be used
later into the <code>cache</code>, significantly reducing
<code>cache miss</code>.</p>
<p>Initially, I was puzzled—how can the <code>compute time</code> and
<code>memory access time</code> overlap when the code seems to execute
sequentially? It wasn't until later that I understood the essence here,
which lies in the principle of <code>cacheline</code>. Indeed,
foundational knowledge is crucial!</p>
<h2 id="version-9-assembly-version-with-a-packed">Version 9: Assembly
Version with A Packed</h2>
<p><img src=/images/sgemm/data/result_9.png width=70% /></p>
<p>Based on previous experience, an attempt was made to
<code>pack</code> Matrix A, but surprisingly, the results were not very
good. A brief analysis suggests that the preload for Matrix A in this
version of the assembly code might not be optimized.</p>
<p>In the previous version, although A wasn't packed, there was preload
for A's 4 rows, which also addressed the pain point of cache miss for
Matrix A.</p>
<h1 id="conclusion">Conclusion</h1>
<p>To continue optimizing this operator, there is still much to be done,
such as rearranging pipelines in assembly.</p>
<p>As mentioned in the OpenPPL Course | RISC-V Technical Analysis, using
<code>vf</code> instructions can achieve 80% of the theoretical peak,
which is 4*80%, <code>3.2 GFLOPs</code>. I currently have only
<code>2.121 GFLOPs</code>, indicating there's still a lot of room for
optimization theoretically.</p>
<p>Furthermore, the RVV currently uses version 0.7.1, and it seems
there's still much work to be done in RVV instruction optimization, such
as the serious efficiency problem encountered with <code>vlw</code>.</p>
<p>In conclusion, working on these tasks has allowed me to learn a lot
from many experts, and I'm very grateful. I also hope this article can
help more people.</p>
<h1 id="acknowledgement">Acknowledgement</h1>
<ul>
<li><p><a href="https://github.com/flame/blislab">BLISlab: A Sandbox for
Optimizing GEMM</a></p>
<p>This project introduced me to how to optimize GEMM</p></li>
<li><p><a
href="https://github.com/surez-ok/blislab_riscv">riscv平台优化矩阵乘(基于blislab优化实践)</a></p>
<p>I conduct experiments and exploration based on this project</p></li>
<li><p>Thanks to Mr. Ding for your guidance.</p></li>
</ul>
<h1 id="references">References</h1>
<p><a href="https://zhuanlan.zhihu.com/p/474684731">OpenPPL 公开课 |
RISC-V 技术解析</a></p>
<p><a href="http://riscvbook.com/">RISC-V-Reader</a></p>
<p><a
href="https://github.com/riscv/riscv-v-spec/releases">riscv-v-spec-0.7.1</a></p>
<p><a
href="https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf">Anatomy
of High-Performance Matrix Multiplication</a></p>
<p><a
href="https://developer.huawei.com/consumer/cn/forum/topic/0201105374686101528">编译器优化丨Cache优化</a></p>
<p><a
href="https://www.leiphone.com/category/yanxishe/Puevv3ZWxn0heoEv.html">OpenBLAS项目与矩阵乘法优化
| AI 研习社</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34204282">Roofline
Model与深度学习模型的性能分析</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/342923482">浅谈分块矩阵优化方法为什么有用</a></p>
<h1 id="thanks-for-your-support">Thanks for your support!</h1>
<p><a
href="https://buymeacoffee.com/zhaodongyu"><strong>Buymeacoffee</strong></a>
<a href="https://paypal.me/ZhaoDongyu"><strong>Paypal</strong></a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>RISCV</tag>
        <tag>GEMM</tag>
        <tag>HPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Marlin fp8</title>
    <url>/2024/10/18/105_marlin_fp8/</url>
    <content><![CDATA[<p><a href="https://docs.vllm.ai/en/stable/quantization/fp8.html">vLLM
支持 FP8(W8A8)</a>了，开始学习fp8 ，写一个
FP8(W4A8)算子，减少一些IO吞吐，看看能不能提升性能。</p>
<span id="more"></span>
<p>官方文档</p>
<p><a
href="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html">Using
FP8 with Transformer Engine</a></p>
<p>视频 <a
href="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/">FP8
for Deep Learning</a> 含有pdf</p>
<p>有价值的文章<a
href="https://zhuanlan.zhihu.com/p/714933476">大模型量化技术原理：FP8</a></p>
<h1 id="fp8-的好处">FP8 的好处</h1>
<ul>
<li>Accelerates math-intensive operations（加速计算密集型操作）
<ul>
<li>FP8 Tensor Cores are 2X faster than 16-bit Tensor Cores</li>
</ul></li>
<li>Accelerates memory-intensive operations（加速内存密集型操作）
<ul>
<li>Reduces memory traffic, since 8-bits requires half number of bytes
to access memory</li>
</ul></li>
</ul>
<p><img src="/images/marlin_fp8/3.png" width="70%"></p>
<p>Note: Smaller GEMMs will achieve lower perf due to GPU
underutilization, among other reasons</p>
<p><img src="/images/marlin_fp8/mn-footprint.svg" width="70%"></p>
<p>Performance improves as the K dimension increases, even when M=N is
relatively large, as setup and tear-down overheads for the computation
are amortized better when the dot product is longer. <a
href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">dl-performance-matrix-multiplication</a></p>
<h1 id="fp8-数据类型">FP8 数据类型</h1>
<p>事实上，FP8(8-bit floating point) 数据类型分两种：</p>
<ul>
<li>E4M3
<ul>
<li>it consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa.
(1 个符号位、4 个指数位和 3 位尾数)</li>
<li>It can store values up to +/-448 and nan.（存储高至 +/-448 和 nan
的值，注意这个为了扩大动态范围，没有 inf）</li>
</ul></li>
<li>E5M2
<ul>
<li>it consists of 1 sign bit, 5 exponent bits and 2 bits of mantissa.
(1 个符号位、5 个指数位和 2 位尾数)</li>
<li>It can store values up to +/-57344, +/- inf and nan. (存储高至
+/-57344、+/- inf 和 nan 的值)</li>
<li>The tradeoff of the increased dynamic range is lower precision of
the stored values.(增加动态范围的代价是存储值的精度较低)</li>
</ul></li>
</ul>
<p><img src="/images/marlin_fp8/5.png" width="70%"></p>
<p><img src="/images/marlin_fp8/fp8_formats.png" width="70%">
浮点数据类型的结构。显示的所有值都是 0.3952 的最接近表示。</p>
<p>在训练神经网络期间，可以使用这两种类型。</p>
<ul>
<li>通常，前向激活和权重需要<strong>更高的精度</strong>，因此
<code>E4M3</code> 数据类型最适合在前向传递期间使用。(2
位尾数对某些网络而言不够)</li>
<li>然而，在后向传递中，流经网络的梯度通常不太容易受到精度损失的影响，但需要<strong>更高的动态范围</strong>。因此，最好使用
<code>E5M2</code> 数据格式存储它们。</li>
<li>H100 TensorCores
支持将这些类型的任意组合作为输入，使我们能够使用其首选精度存储每个张量。</li>
</ul>
<p><img src="/images/marlin_fp8/4.png" width="70%">
<img src="/images/marlin_fp8/2.png" width="70%"></p>
<ul>
<li>E4M3 动态范围：2 的 18 次方</li>
<li>E5M2 动态范围：2 的 32 次方</li>
</ul>
<p>可以得出结论：A number of networks match higher-precision accuracy
with only E4M3。目标很明确了，我就使用 <code>E4M3</code>
的数据类型做推理。</p>
<h1 id="e4m3-表示范围"><code>E4M3</code> 表示范围</h1>
<p>参考论文 <a href="https://arxiv.org/pdf/2209.05433">FP8 FORMATS FOR
DEEP LEARNING</a></p>
<p><img src="/images/marlin_fp8/Details of FP8 Binary Formats.png" width="80%"></p>
<p><code>E4M3</code>指数偏移量：7。实际的指数 e = E - 7 。</p>
<ol type="1">
<li>最小非零正数</li>
</ol>
<ul>
<li><p>当指数位 <code>E = 1</code> 时（即存储的指数是
<code>0001</code>），实际指数 <code>e = 1 - 7 = -6</code> 。</p></li>
<li><p>尾数 <code>M = 0</code> 时，隐含的有效位为
<code>1</code>，所以最小的非零正数是 <code>1.0 * 2^&#123;-6&#125; = 2^&#123;-6&#125;</code>
，即：<code>2^&#123;-6&#125; = 0.015625</code></p></li>
<li><p>次正规数（Subnormal numbers）：当 <code>E = 0</code>
时，指数固定为 <code>e = -6</code> ，但<strong>尾数不再隐含有效位
1</strong>，因此最小的次正规数为 <code>0.125 * 2^&#123;-6&#125;</code>
即：<code>0.125 * 2^&#123;-6&#125; = 0.001953125</code></p>
<p>在正规数中，尾数有隐含的 1.，而次正规数没有隐含的
1.，因此可以表示更小的数。
次正规数的引入是为了避免在计算非常小的数时直接舍入为
0，保持一定的精度。</p></li>
</ul>
<ol start="2" type="1">
<li>最大值</li>
</ol>
<ul>
<li><p>FP8 (E4M3) 最大指数 <code>E = 15</code> （即存储的指数是
1111，本来这种是inf的），表示指数 <code>e = 8</code>
，最大值是：<code>1.75 * 2^8 = 448</code></p>
<p>因此，FP8 E4M3 能表示的最大值为 ±448。</p></li>
</ul>
<ol start="3" type="1">
<li>特殊值</li>
</ol>
<ul>
<li>NaN（非数字）：当指数 E = 15 且尾数 M ≠ 0 时，表示 NaN（Not a
Number）。</li>
<li>没有无穷大 (Inf)：E4M3 格式无法表示正负无穷大，当指数位 E = 1111
且尾数为 0 时，不表示无穷大，而是最大值 448。</li>
</ul>
<p>综上，FP8 (E4M3) 的数值表示范围</p>
<ul>
<li>最小次正规数： 0.001953125 。</li>
<li>最小非零正规数： 0.015625 。</li>
<li>最大值：±448。</li>
<li>NaN：当指数全为 1 且尾数不为 0 时表示 NaN（非数字）。</li>
</ul>
<h1 id="tensor-cores">Tensor Cores</h1>
<p>在Tensor Cores中使用FP8数据类型进行矩阵乘法运算</p>
<p><img src="/images/marlin_fp8/0.png" width="80%"></p>
<ul>
<li>相比于16位的数据类型,2倍的FLOP（浮点运算次数）</li>
<li>FFMA（Fuse Multiply-Add，融合乘加）操作可以达到
32倍的计算效率，在使用2:4稀疏性（sparsity）时，这个倍率可以提升到
64倍</li>
<li>支持所有FP8输入类型的组合</li>
<li>操作之后的输出可以是 FP16（16位浮点数）或者 FP32（32位浮点数）</li>
<li>支持在 8位和32位/16位数据类型 之间的转换。</li>
</ul>
<h1 id="fp8-for-dl">FP8 for DL</h1>
<p>What’s needed for inference numerics</p>
<ul>
<li>Per-tensor scaling</li>
<li>Mixed precision</li>
</ul>
<p>What’s needed for performance (speed):</p>
<ul>
<li>FP8 Tensor Cores, for faster math</li>
<li>FP8 I/O, to reduce bandwidth pressure</li>
</ul>
<h2 id="per-tensor-scaling">Per-tensor scaling</h2>
<p>FP8 的动态范围往往比较窄，这导致无法单纯用 FP8
表示神经网络中所有的数值。</p>
<p>FP8 往往对 给定张量
具有足够的范围，但并不总是对所有张量都具有足够的范围，因此每个张量都进行缩放。</p>
<p>张量值以更高精度计算，在转换为 FP8 之前进行 scale。核心思想就是使用
scale 将每个 tensor 的数值范围适应 FP8 的表示范围。</p>
<p><img src="/images/marlin_fp8/1.png" width="60%"></p>
<p>就像上图，紫色的曲线代表原始tensor的表示范围，几乎有一半都在 FP8
的表示范围以外，导致小于 2^(-6) 的数都变成了0。</p>
<p>使用 scale因子 S，使得数据分布向右移动，这条蓝色虚线曲线就全部在 FP8
的表示范围内。</p>
<h1 id="整体目标">整体目标</h1>
<p><img src="/images/marlin_fp8/7.png" width="60%"></p>
<p><a href="https://github.com/mit-han-lab/qserve">QServe: W4A8KV4
Quantization and System Co-design for Efficient LLM
Serving</a>论文中这个图比较明显的展示出了不同数据类型（FP16、INT8、INT4等）在不同计算密度下的性能表现，即A100的roofline模型。</p>
<ul>
<li>在计算密度较低的区域是 W4A16 / W4A8(int8) Sweet Region</li>
<li>在计算密度较高的区域是 W8A8 Sweet Region</li>
</ul>
<p>目标就是
W4A8(fp8)，即能有好的推理性能，又能有比W4A8(int8)更好的精度性能。</p>
<ul>
<li>2024.10.21</li>
</ul>
<p>在<a
href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md">quantization-in-TRT-LLM</a></p>
<p><a
href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/blob/main/llm_ptq/README.md#model-support-list">Model
Support List</a></p>
<h1 id="实验">实验</h1>
<p><img src="/images/marlin_fp8/6.png" width="90%"></p>
<p>实验平台：<a
href="https://images.nvidia.com/content/Solutions/data-center/vgpu-L40-datasheet.pdf">NVIDIA
L40</a> 是基于 Ada Lovelace 架构的一款高端数据中心和工作站级 GPU。</p>
<p>Nvidia L40 GPU硬件参数如下，可以发现FP8 Tensor Core和INT8 Tensor
Core在各自数据类型上的计算能力是相同的</p>
<p><img src="/images/marlin_fp8/8.png" width="50%"></p>
<p>思路：巧用符号位！</p>
<p>一个INT4数字只能精确表示从-8到7的16个整数。</p>
<p>2024.10.24 在程序员节这天想出了int4转fp8的好方法！</p>
<p>问题卡在如何实现类似于 <code>__hsub2</code> 的 fp8
的向量化减法函数</p>
<p>在<a
href="https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH__FP8X4__E4M3__STRUCT.html">C++
struct for handling vector type of four fp8 values of e4m3
kind.</a>也没有找到相关函数。</p>
<p>去论坛问了一下：<a
href="https://forums.developer.nvidia.com/t/handling-vector-type-of-four-fp8-values-arithmetic-functions/311095">Handling
vector type of four fp8 values Arithmetic Functions</a></p>
<p>学习CUTLASS <a
href="https://www.bilibili.com/video/BV1Qk4y1n7Nd/?vd_source=a706cf2a9a33f4c298a6ef4764cea661">半小时快速入门CUTLASS</a></p>
<p><a
href="https://cdnapi-ev.kaltura.com/p/2935771/sp/293577100/playManifest/entryId/1_f4ooowud/format/applehttp/protocol/https/session.m3u8?uiConfId=46302491">视频下载链接</a></p>
<p>CUTLASS中include/cutlass/arch/mma_sm89.h中</p>
<p>Matrix Multiply 16832 - Float {E4M3, E5M2}, FP32 accumulation</p>
<p>Matrix multiply-add operation - F32 = fe4m3 * fe4m3 + F32</p>
<p>在L40
运行cutlass，<code>./exampless/58_ada_fp8_gemm/58_ada_fp8_gemm</code></p>
<pre><code>Running GEMM with staged accumulation (OpMultiplyAdd)
=====================
Skipped reference check
Problem size: 1024x1024x1024
Runtime (ms): 0.0117968
GFLOPs/sec:   182040
Passed

Running GEMM with fast accumulation (OpMultiplyAddFastAccum)
=====================
Skipped reference check
Problem size: 1024x1024x1024
Runtime (ms): 0.0110768
GFLOPs/sec:   193872
Passed</code></pre>
<p>L40最大是 362</p>
<p>QQQ的思路是</p>
<p>W4A8版本的Kernel输入输出与W4A16版本的输入输出有一些区别：</p>
<p>A: 数据类型为int8_t，W4A16版本为fp16。</p>
<p>C:
是额外添加的专门用来做global_reduce的buffer。W4A16版本以输出C矩阵作为buffer来进行global_reduce。由于int8
Tensor
Core的计算结果是int32_t，与输出结果(fp16)数据类型不同，因此额外开辟了一块全局显存来做global_reduce。</p>
<p>D：输出结果。</p>
<p>s1: activation的per_channel scale。</p>
<p>s2: 权重的per_channel scale。</p>
<p>s3: 权重的per_group scale。</p>
<p>当权重做per_channel量化时，s3为空。当权重做per_group量化时，权重有两个scale：s2(per_channel)和s3(per_group)。这和量化算法相关</p>
<p>而这里A搞改成fp8，C不用改</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/714731771">基于NVIDIA
L20集群上FP8精度的训练及推理的实践分享</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>GEMM</tag>
        <tag>HPC</tag>
        <tag>CUDA</tag>
        <tag>MARLIN</tag>
      </tags>
  </entry>
  <entry>
    <title>万物皆可RSS</title>
    <url>/2023/06/29/67_RSS/</url>
    <content><![CDATA[<p>万物皆可RSS！</p>
<span id="more"></span>
<p><a href="https://sspai.com/post/60781">指路文章</a></p>
<p><a
href="https://www.bilibili.com/video/BV1oU4y1s7o8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=a706cf2a9a33f4c298a6ef4764cea661">入坑视频教程</a></p>
<p><a
href="https://www.runningcheese.com/rss-subscriptions">入坑文章指南</a></p>
<blockquote>
<p>RSS，英文全称Really Simple
Syndication，中文译作简易信息聚合（也叫聚合内容），是一种基于XML标准，在互联网上被广泛采用的内容包装盒投递协议。它能够描述和同步网站内容，而RSS订阅这种形式则可以更快地获取最新信息。</p>
</blockquote>
<ul>
<li><p>碎片化阅读</p>
<blockquote>
<p>碎片化阅读是指一种通过手机、电子书、计算机等进行的不完整的、断断续续的阅读模式。</p>
</blockquote></li>
<li><p>信息茧房 &gt;
信息茧房概念是由哈佛大学法学院教授、奥巴马总统的法律顾问凯斯·桑斯坦在其2006年出版的著作《信息乌托邦——众人如何生产知识》中提出的。通过对互联网的考察，桑斯坦指出，在信息传播中，因公众自身的信息需求并非全方位的，公众只注意自己选择的东西和使自己愉悦的通讯领域，久而久之，会将自身桎梏于像蚕茧一般的“茧房”中。</p></li>
</ul>
<p>众所周知，我们身处一个信息爆炸的网络大数据时代，再加上数据埋点、人群画像等精准营销手段，往往会产生这样的一个结果：<strong>我们经常会被迫接收到来自服务商提供的，经由大数据算法检验认为的“所谓匹配我们”的信息。</strong></p>
<p>而这些“精准匹配的个性化”信息会不断缩小我们的认知范围，久而久之，渐渐我们会失去对信息的敏感，处于一种“<strong>别人推荐什么，你就看什么</strong>”的状态中。这是很可怕的！！</p>
<p>我们每个人都是独立的个体，应当是“自由”的。这份自由中自然包含着“思想自由”：个人拥有或思考独立于他人之外的观点、事实、思想之自由。</p>
<p>--摘自<a href="https://sspai.com/post/60781">指路文章</a></p>
<h1 id="rsshub-feedly-reeder搭配">RSSHub + Feedly + Reeder搭配</h1>
<p>RSSHub是一个开源、简单易用、易于扩展的 RSS
生成器，可以给任何奇奇怪怪的内容生成 RSS
订阅源。目前借助于开源社区的力量快速发展中，已经适配数百家网站的上千项内容。</p>
<ul>
<li>RSSHub可以为几乎所有网站提供RSS订阅源，比如知乎、虎扑、博客、微信公众号、B站、起点等媒体站点。甚至即使该站点不提供RSS订阅服务，我们也可以基于它来制作一个RSS订阅源，可谓非常强大</li>
</ul>
<p>Feedly是一个RSS聚合器应用程序，支持各种网页浏览器和运行iOS或Android的移动设备，也是一个基于云端的服务。其从各种在线资源聚合用户自定的新闻订阅源，并可与他人分享。</p>
<ul>
<li>选择Feedly则是因为其提供了文件夹分类功能，且也拥有数量不菲的订阅源，在对RSS订阅源的管理上十分方便，同时恰好也在Reeder支持的第三方订阅服务列表中。</li>
</ul>
<p>Reeder是一款评价极高的iOS端RSS阅读应用。其支持与Feedbin、Feedly、Inoreader等RSS第三方服务云端同步，甚至也支持与FreshRSS、Reader、Fever等自建RSS服务同步数据。</p>
<ul>
<li>使用Reeder则仅仅是因为它是iOS上广受好评的RSS阅读端APP，而我购买后体验的效果也很满意。</li>
</ul>
<p>--摘自<a href="https://sspai.com/post/60781">指路文章</a></p>
<p>我绕了一大圈，看各种资料，最终的选择也恰巧是如此。</p>
<h1 id="我的一些操作">我的一些操作：</h1>
<h2 id="mac安装reeder4">1、mac安装Reeder4</h2>
<p>程序体积比我想象的要小</p>
<p>新建了个账户，icloud的feeds。</p>
<p>订阅了一下自己的博客，阅读体验还可以。</p>
<h2 id="iphone安装reeder5">2、iphone安装Reeder5</h2>
<p>重新注册了个美区账号，<a
href="https://mp.weixin.qq.com/s/s9fMsBzVu4nXBbBuYym9mA">步骤</a></p>
<p>使用礼品卡的方式下载了Reeder5</p>
<p>新建了个账户，icloud的feeds，和电脑的订阅同步上了～</p>
<h2 id="浏览器安装rss插件">3、浏览器安装<a
href="https://greasyfork.org/zh-CN/scripts/373252-rss-show-site-all-rss">RSS+插件</a></h2>
<p>安装后，在网页的右下角会有一个“蓝色圆点”，显示当前网页有多少个可用
RSS 源，如果没有，则不会显示。</p>
<p>有了它，我们就再也不需要到处找 RSS 订阅源。</p>
<p>只需要和平时上网一样，遇到喜欢的 RSS 源，一键添加即可。</p>
<h2 id="注册feedly">4、注册feedly</h2>
<p>https://feedly.com/</p>
<p>但是，<a href="https://www.v2ex.com/t/797931">Reeder 4 无法添加
feedly</a></p>
<p>决定买Reeder5，既然拥有国外信用卡是早晚的事，那就早点做吧。</p>
<h2 id="注册币安">5、注册币安</h2>
<p><a href="https://www.binance.com/zh-CN">币安</a></p>
<p><a
href="https://www.youtube.com/watch?v=sWm0RgSapp8">币安使用方式</a></p>
<p>欧易似乎更好用，但我还是先入为主了。</p>
<h2 id="注册onekey-visa卡">6、注册onekey visa卡</h2>
<p>在<a
href="https://card.onekey.so/?i=S1RXT8">这里</a>注册美国虚拟信用卡</p>
<p><a href="https://www.youtube.com/watch?v=nIfFC7hFZ90">教程</a></p>
<p>我是用币安向onekey提现USDT实现激活的，期间扣除了1USDT的手续费。以及onekey有1$的开卡费和一点手续费。</p>
<h2 id="添加feedly账户">7、添加feedly账户</h2>
<p>下载Reeder5之后，就可以绑定Feedly账户了。</p>
<hr />
<p>目前我是<code>RSS+插件</code>的重度使用者</p>
<hr />
<h1 id="rss真香"><strong>RSS真香！</strong></h1>
]]></content>
      <tags>
        <tag>RSS</tag>
      </tags>
  </entry>
  <entry>
    <title>LaTeX学习</title>
    <url>/2023/05/30/69_LaTeX/</url>
    <content><![CDATA[<p>今天看的论文的排版真的太好看了！开始学习<code>LaTeX</code>，整个过程记录一下。</p>
<span id="more"></span>
<p><code>mactex</code>下载地址：<a
href="https://www.tug.org/mactex/">https://www.tug.org/mactex/</a></p>
<pre><code>MacTeX.pkg.
The Size is 5.2G - 15 March 2023
The MD5 sum is 64416e6da409609c48970e23906e7642
The SHA512 sum is 0ff569be6af3e658b4a60e2b89bf10c41b9bae38bdaee60021c69aea238d2436886c0883954ab0a64394d9a6e080958b2fc82f91ac5c4dfe9e1de32df46efd49</code></pre>
<p><a
href="https://www.tug.org/mactex/READ_ME_FIRST.pdf">READ_ME_FIRST</a>给了我很大帮助。</p>
<ul>
<li>2023.05.31</li>
</ul>
<p>看了<a href="https://www.doc88.com/p-9773781953864.html">LaTeX Notes
v1.20</a>，<a
href="https://www.dralpha.com/">作者</a>很有意思，也给我打开了<code>LaTeX</code>大门。</p>
<p>新的版本位于<a
href="https://github.com/huangxg/lnotes">https://github.com/huangxg/lnotes</a>,<a
href="https://github.com/huangxg/lnotes/blob/master/lnotes2.pdf">PDF文档</a>。</p>
<p>在<a
href="https://zhuanlan.zhihu.com/p/521649367">知乎</a>看到一篇很好的文章</p>
<p>【更新中】</p>
]]></content>
      <tags>
        <tag>LaTeX</tag>
        <tag>study</tag>
      </tags>
  </entry>
  <entry>
    <title>flatc</title>
    <url>/2023/06/28/71_flatc/</url>
    <content><![CDATA[<p>记录一下flatbuffer的学习</p>
<span id="more"></span>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/google/flatbuffers.git</span><br><span class="line"></span><br><span class="line">cmake -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=Release</span><br><span class="line"></span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<p><a
href="https://stackoverflow.com/questions/24275803/how-to-build-flatbuffers-on-osx">reference</a></p>
<p>./flatc -t schema.fbs -- test.tflite</p>
<p>./flatc -t ~/Projects/tensorflow/tensorflow/lite/schema/schema.fbs --
~/Downloads/encoder.tflite</p>
<p>参考资料：</p>
<p><a
href="https://www.jianshu.com/p/523968b82e74?tdsourcetag=s_pcqq_aiomsg">iOS中FlatBuffers的使用</a></p>
]]></content>
  </entry>
  <entry>
    <title>Marlin代码解读</title>
    <url>/2024/09/26/104_marlin/</url>
    <content><![CDATA[<p>Marlin Kernel是<a href="https://github.com/IST-DASLab">IST-DASLab</a>
开发的GPTQ量化模型高性能 FP16(activation) x INT4(weight)
GEMM算子实现，在现有W4A16 GEMM Kernel中，Marlin Kernel性能是最好的。</p>
<p>作为一个不会cuda的小白，研究完<a
href="https://github.com/IST-DASLab/marlin"><code>marlin</code></a>算子之后神清气爽，</p>
<p>【长文预警 &amp; 多图预警】</p>
<span id="more"></span>
<h1 id="准备工作">准备工作</h1>
<h2 id="分析工具">分析工具</h2>
<p>先把工具学习了，学习 <code>nsight-compute</code>。 NVIDIA Nsight
Compute 是一款适用于 CUDA
应用程序的交互式内核分析器。它通过用户界面和命令行工具提供详细的性能指标和
API 调试。</p>
<p><a
href="https://docs.nvidia.com/nsight-compute/NsightCompute/index.html">Nsight
Compute</a></p>
<p><a
href="https://www.bilibili.com/video/BV13w411o7cu/?vd_source=a706cf2a9a33f4c298a6ef4764cea661">【CUDA进阶】深入理解
Nsight System 和 Nsight Compute</a></p>
<h2 id="搭建环境">搭建环境</h2>
<ul>
<li><code>git clone https://github.com/IST-DASLab/marlin.git</code></li>
<li><code>export TORCH_CUDA_ARCH_LIST="8.6"</code>
(这里要根据实际的版本来)</li>
<li><code>pip install .</code></li>
<li><code>python test.py</code></li>
</ul>
<hr />
<p>我是先读了<a
href="https://arxiv.org/pdf/2408.11743">论文</a>，看了一些大佬的分析文章，比如
<a href="https://zhuanlan.zhihu.com/p/707470647">Marlin
W4A16&amp;W4A8代码走读</a>、<a
href="https://zhuanlan.zhihu.com/p/698715169">fp16*int4计算kernel--MARLIN代码走读</a>
、 <a href="https://zhuanlan.zhihu.com/p/716412368">MARLIN:
Mixed-Precision Auto-Regressive Parallel Inference on Large Language
Models论文解读</a>等，以及<a
href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md">NVIDIA
官方指导文档</a>， 然后开始进行代码的详细分析，慢慢学会了 cuda 和 marlin
。</p>
<hr />
<h1 id="开始分析">开始分析</h1>
<p>在 <strong>test.py</strong> 中起一个 m = 128, sms = 5 的
<code>marlin.mul</code></p>
<p>即 <code>self.run_problem(128, 768, 256, -1, -1)</code>。</p>
<p>⚠️注意：本文全部使用这个case的数值进行带入。即</p>
<ul>
<li>m = 128</li>
<li>k = 256</li>
<li>n = 768</li>
</ul>
<p>这个算子里面的变量不要老是按照拿到的数进行理解，要结合代码进行理解。</p>
<p>接下来进入 <strong>marlin_cuda_kernel.cu</strong> 文件的
<code>marlin_cuda</code> 函数逐步分析。</p>
<h1 id="int-marlin_cuda函数">int marlin_cuda()函数</h1>
<h2 id="变量分析">变量分析</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> tot_m = prob_m;</span><br><span class="line"><span class="type">int</span> tot_m_blocks = ceildiv(tot_m, <span class="number">16</span>);</span><br><span class="line"><span class="type">int</span> pad = <span class="number">16</span> * tot_m_blocks - tot_m;</span><br></pre></td></tr></table></figure>
<p>由于传入的<code>prob_m = 128</code>，则
<code>tot_m = 128</code>，将其除以16并进行上取整，得<code>tot_m_blocks = 8</code>，这里是整除，没有进行pad，所以<code>pad</code>计算出来也就是0。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (sms == <span class="number">-1</span>)</span><br><span class="line">  cudaDeviceGetAttribute(&amp;sms, cudaDevAttrMultiProcessorCount, dev);</span><br></pre></td></tr></table></figure>
<p>这里由于指定了<code>sms = 5</code>，所以不会走后面的代码，否则会获取指定
CUDA 设备（dev）的多处理器数量。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (thread_k == <span class="number">-1</span> || thread_n == <span class="number">-1</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (prob_m &lt;= <span class="number">16</span>) &#123;</span><br><span class="line">    <span class="comment">// For small batchizes, better partioning is slightly more important than better compute utilization</span></span><br><span class="line">    thread_k = <span class="number">128</span>;</span><br><span class="line">    thread_n = <span class="number">128</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    thread_k = <span class="number">64</span>;</span><br><span class="line">    thread_n = <span class="number">256</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如注释所说：对于小批量处理来说，<strong>较好的分区(better
partioning)</strong> 比 <strong>较好的计算资源利用(better compute
utilization)</strong> 稍微更重要。​在目前的例子中，prob_m =
128，所以得到的分区是</p>
<ul>
<li><code>thread_k = 64</code></li>
<li><code>thread_n = 256</code></li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> thread_k_blocks = thread_k / <span class="number">16</span>;</span><br><span class="line"><span class="type">int</span> thread_n_blocks = thread_n / <span class="number">16</span>;</span><br><span class="line"><span class="type">int</span> group_blocks = (groupsize == <span class="number">-1</span>) ? <span class="number">-1</span> : groupsize / <span class="number">16</span>;</span><br><span class="line"><span class="type">int</span> blocks = sms;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>thread_k_blocks = 64/16 = 4</code></li>
<li><code>thread_n_blocks = 256/16 = 16</code></li>
</ul>
<blockquote>
<p>为什么是除以16呢？因为Marlin Kernel使用的Tensor Core指令为
<code>m16n8k16</code>
size的MMA指令，所以一次MMA指令执行的矩阵size为m16n8k16。而Marlin
Kernel在设计的时候，以n方向的2次MMA计算的矩阵作为一个基本的sub_tile，即sub_tile的尺寸为<code>m16n16k16</code>。——源自<a
href="https://zhuanlan.zhihu.com/p/707470647">Marlin
W4A16&amp;W4A8代码走读</a></p>
</blockquote>
<h2 id="主循环">主循环</h2>
<p>接下来进入沿着m方向的循环</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; tot_m_blocks; i += <span class="number">4</span>) &#123;</span><br><span class="line">  <span class="type">int</span> thread_m_blocks = tot_m_blocks - i;</span><br><span class="line">  prob_m = tot_m - <span class="number">16</span> * i;</span><br><span class="line">  <span class="type">int</span> par = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> (thread_m_blocks &gt; <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="comment">// Note that parallel &gt; 1 currently only works for inputs without any padding</span></span><br><span class="line">    par = (<span class="number">16</span> * thread_m_blocks - pad) / <span class="number">64</span>;</span><br><span class="line">    <span class="keyword">if</span> (par &gt; max_par)</span><br><span class="line">      par = max_par;</span><br><span class="line">    prob_m = <span class="number">64</span> * par;</span><br><span class="line">    i += <span class="number">4</span> * (par - <span class="number">1</span>);</span><br><span class="line">    thread_m_blocks = <span class="number">4</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>之前算过 tot_m_blocks=8，则</p>
<ul>
<li><code>i = 0</code></li>
<li><code>thread_m_blocks = 8</code></li>
<li><code>prob_m = 128 - 16 * 0 = 128</code></li>
</ul>
<p>由于 <code>thread_m_blocks &gt; 4</code>，则
<code>par = (16 * 8 - 0) / 64 = 2</code>，这里可以看出来，<strong>m方向最大的基本执行单元是
64</strong>。</p>
<p><code>prob_m = 64 * par = 128</code>, 这里是做了 pad 后的
prob_m。</p>
<p>然后<code>i += 4 * (2 - 1)</code>，此时 i = 4。</p>
<pre><code>我认为这里的(par - 1)的原因就是因为 for 循环里对 i 也有一个增加操作，
感觉二者结合起来看比较容易理解
（或者我认为改成i += 4 * par, for循环里不做i增操作更好理解一些）。</code></pre>
<p>此时，得到<code>thread_m_blocks = 4</code>。</p>
<p><code>i</code>进入for循环的<code>i+=4</code>，不再满足循环条件，循环结束。</p>
<pre><code>这里的par是2，如果par超过了max_par，那么应该就会多循环几次了。</code></pre>
<p>这一段的作用就是确定了m方向上的blocks = 4 和 parallel = 2。</p>
<p>目前的结果是</p>
<ul>
<li>thread_m_blocks = 4</li>
<li>thread_n_blocks = 16</li>
<li>thread_k_blocks = 4</li>
<li>group_blocks = -1</li>
</ul>
<p>于是进入<code>CALL_IF(4,16,4,-1)</code></p>
<h2 id="参数整理">参数整理</h2>
<p>各个参数都确定下来了，先画个图整理一下：</p>
<p><img src="/images/marlin/0_gemm.png" width="95%"></p>
<p>以M=128, K=256, N=768为例，每个小方格的大小是64x64，即 tile
的大小。</p>
<h1 id="global-void-marlin函数"><strong>global</strong> void
Marlin()函数</h1>
<h2 id="变量分析-1">变量分析</h2>
<p>开始核心代码 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> parallel = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (prob_m &gt; <span class="number">16</span> * thread_m_blocks) &#123;</span><br><span class="line">  parallel = prob_m / (<span class="number">16</span> * thread_m_blocks);</span><br><span class="line">  prob_m = <span class="number">16</span> * thread_m_blocks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
这里是对prob_m进行了限制，如前文所说，<strong>m方向最大的基本执行单元是64</strong>，对于较大的
GEMM，并行运行多个batch大小为 64 的版本。</p>
<p>所以这几句的结果是</p>
<ul>
<li>thread_m_blocks = 4</li>
<li>parallel = 2</li>
<li>prob_m = 64</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> k_tiles = prob_k / <span class="number">16</span> / thread_k_blocks;</span><br><span class="line"><span class="type">int</span> n_tiles = prob_n / <span class="number">16</span> / thread_n_blocks;</span><br><span class="line"><span class="type">int</span> iters = ceildiv(k_tiles * n_tiles * parallel, gridDim.x);</span><br></pre></td></tr></table></figure>
<p>得到k_tiles = 4, n_tiles = 3, 所以目前一共有 k_tiles * n_tiles *
parallel =
24个tile。而SM=5，因此，每个block需要计算的tile数量为5（24除以5后上取整，最后一个block只需要计算4个tile），即iters
= 5.</p>
<h2 id="条带分区">条带分区</h2>
<p>在讲接下来的部分前先讲一下marlin里面的stripe概念。</p>
<p><img src="/images/marlin/1_stripe.png" width="50%"></p>
<p><a href="https://www.arxiv.org/pdf/2408.11743">图片来源</a></p>
<p>条带分区（Striped
Partitioning）是一种在并行计算中常用的技术，特别是在大型矩阵乘法计算中，通过该技术可以<strong>提高负载均衡并最小化计算过程中的开销</strong>。</p>
<p>MARLIN内核中，条带分区是指由多个SM（流式多处理器）处理不同的矩阵“条带”，这意味着<strong>每个SM负责处理多块矩阵数据</strong>，这种分区方法保证了工作负载在处理器之间的<strong>均匀分布</strong>。</p>
<p>条带分区的核心思想是：</p>
<ul>
<li>工作均衡分配：通过跨列或跨行分配条带，可以确保处理器均匀分配任务，防止部分处理器过载而其他处理器闲置。</li>
<li>减少全局同步开销：由于条带分区将任务均匀分配给各个处理器，减少了全局同步的需求，降低了并行计算中的通信开销。</li>
<li>提高缓存和内存效率：通过分割矩阵数据到条带，系统能够更有效地使用GPU缓存和内存带宽，从而最大化内存加载的吞吐量，并提高整体计算效率。</li>
</ul>
<p>这种方法有助于在不同的GPU架构中优化计算性能，特别是在大规模深度学习模型的推理任务中。</p>
<h2 id="本示例下的条带分区">本示例下的条带分区</h2>
<p>前文讲过，每个block需要计算的tile数量为5。这里对B画个图表示一下：</p>
<p><img src="/images/marlin/2.png" width="95%"></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> slice_row = (iters * blockIdx.x) % k_tiles;</span><br><span class="line"><span class="type">int</span> slice_col_par = (iters * blockIdx.x) / k_tiles;</span><br></pre></td></tr></table></figure>
<p>这两句有点抽象，结合上面的图，这里的计算其实是确定了每个block起始位置的纵坐标和横坐标。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>block 0</th>
<th>block 1</th>
<th>block 2</th>
<th>block 3</th>
<th>block 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>slice_row</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>0</td>
</tr>
<tr class="even">
<td>slice_col_par</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>5</td>
</tr>
</tbody>
</table>
<h2 id="指针推进">指针推进</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// We can easily implement parallel problem execution by just remapping indices and advancing global pointers</span></span><br><span class="line"><span class="keyword">if</span> (slice_col_par &gt;= n_tiles) &#123;</span><br><span class="line">  A += (slice_col_par / n_tiles) * <span class="number">16</span> * thread_m_blocks * prob_k / <span class="number">8</span>;</span><br><span class="line">  C += (slice_col_par / n_tiles) * <span class="number">16</span> * thread_m_blocks * prob_n / <span class="number">8</span>;</span><br><span class="line">  locks += (slice_col_par / n_tiles) * n_tiles;</span><br><span class="line">  slice_col = slice_col_par % n_tiles;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只需重新映射索引(remapping indices)和推进全局指针(advancing global
pointers)，就可以轻松实现并行问题执行。</p>
<p>比如这里的 <code>n_tiles = 3</code>，那对于 block 3 和 block 4
而言，要处理的就是第二个parallel，所以要推进一下A和C的指针。</p>
<h1 id="init_slice函数">init_slice()函数</h1>
<p>接下来看比较容易迷惑的部分，如果不想细看这部分代码，可以直接看结论
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Compute all information about the current slice which is required for synchronization.</span></span><br><span class="line"><span class="keyword">auto</span> init_slice = [&amp;] () &#123;</span><br><span class="line">  slice_iters = iters * (blockIdx.x + <span class="number">1</span>) - (k_tiles * slice_col_par + slice_row);</span><br><span class="line">  <span class="keyword">if</span> (slice_iters &lt; <span class="number">0</span> || slice_col_par &gt;= n_tiles * parallel)</span><br><span class="line">    slice_iters = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (slice_iters == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span> (slice_row + slice_iters &gt; k_tiles) </span><br><span class="line">    slice_iters = k_tiles - slice_row;</span><br><span class="line">  slice_count = <span class="number">1</span>;</span><br><span class="line">  slice_idx = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> col_first = iters * ceildiv(k_tiles * slice_col_par, iters);</span><br><span class="line">  <span class="keyword">if</span> (col_first &lt;= k_tiles * (slice_col_par + <span class="number">1</span>)) &#123;</span><br><span class="line">    <span class="type">int</span> col_off = col_first - k_tiles * slice_col_par;</span><br><span class="line">    slice_count = ceildiv(k_tiles - col_off, iters);</span><br><span class="line">    <span class="keyword">if</span> (col_off &gt; <span class="number">0</span>)</span><br><span class="line">      slice_count++;</span><br><span class="line">    <span class="type">int</span> delta_first = iters * blockIdx.x - col_first;</span><br><span class="line">    <span class="keyword">if</span> (delta_first &lt; <span class="number">0</span> || (col_off == <span class="number">0</span> &amp;&amp; delta_first == <span class="number">0</span>))</span><br><span class="line">      slice_idx = slice_count - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      slice_idx = slice_count - <span class="number">1</span> - delta_first / iters;</span><br><span class="line">      <span class="keyword">if</span> (col_off &gt; <span class="number">0</span>)</span><br><span class="line">        slice_idx--;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (slice_col == n_tiles) &#123;</span><br><span class="line">    A += <span class="number">16</span> * thread_m_blocks * prob_k / <span class="number">8</span>;</span><br><span class="line">    C += <span class="number">16</span> * thread_m_blocks * prob_n / <span class="number">8</span>;</span><br><span class="line">    locks += n_tiles;</span><br><span class="line">    slice_col = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p><img src="/images/marlin/3.png" width="95%"></p>
<p><code>slice_iters</code></p>
<ul>
<li>当前slice中的线程块(threadblock)块数</li>
<li>block 0有4个，要迭代4次，block 1有3个，要迭代3次，依此类比</li>
</ul>
<p><code>slice_count</code></p>
<ul>
<li>当前slice中活跃线程块(active threadblock)的总数</li>
<li>第一个slice有1个（只有block 0），第二个slice有2个（有block 0和block
1）</li>
</ul>
<p><code>slice_idx</code></p>
<ul>
<li>当前slice中的线程块索引(index of threadblock)</li>
<li>这个索引从下到上编号。以第二个slice为例，block 1的idx是0，block
0的idx是1。</li>
</ul>
<h1 id="a_sh_wr_iters-变量">a_sh_wr_iters 变量</h1>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> a_gl_stride = prob_k / <span class="number">8</span>; <span class="comment">// stride of the A matrix in global memory</span></span><br><span class="line"><span class="comment">// We typically use `constexpr` to indicate that this value is a compile-time constant</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_stride = <span class="number">16</span> * thread_k_blocks / <span class="number">8</span>; <span class="comment">// stride of an A matrix tile in shared memory</span></span><br><span class="line">constexpr <span class="type">int</span> a_gl_rd_delta_o = <span class="number">16</span> * thread_k_blocks / <span class="number">8</span>; <span class="comment">// delta between subsequent A tiles in global memory</span></span><br><span class="line"><span class="type">int</span> a_gl_rd_delta_i = a_gl_stride * (threads / a_gl_rd_delta_o); <span class="comment">// between subsequent accesses within a tile</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_wr_delta = a_sh_stride * (threads / a_gl_rd_delta_o); <span class="comment">// between shared memory writes</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_rd_delta_o = <span class="number">2</span> * ((threads / <span class="number">32</span>) / (thread_n_blocks / <span class="number">4</span>)); <span class="comment">// between shared memory tile reads</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_rd_delta_i = a_sh_stride * <span class="number">16</span>; <span class="comment">// within a shared memory tile</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_stage = a_sh_stride * (<span class="number">16</span> * thread_m_blocks); <span class="comment">// overall size of a tile</span></span><br><span class="line">constexpr <span class="type">int</span> a_sh_wr_iters = ceildiv(a_sh_stage, a_sh_wr_delta); <span class="comment">// number of shared write iterations for a tile</span></span><br></pre></td></tr></table></figure>
<p>接下来<strong>着重</strong>看一下 <code>a_sh_wr_iters</code>
这个变量：</p>
<p>从 Global mem 加载数据到 Shared mem 的时候，每一个 thread 会读取一个
int4（即4个int，128 bits），一个 thread blocks
256个线程需要分多次才能将完整的 tile 数据块读取完毕。</p>
<pre><code>为什么用int4？
因为 kernel 使用的读取全局显存数据的`cp.async.cg.shared.global`指令最大处理长度是128 bits。</code></pre>
<p>在本文中，读取A矩阵tile需要的次数</p>
<p><code>a_sh_wr_iters = ceildiv(a_sh_stage, a_sh_wr_delta)</code>,</p>
<p>表示A的一个 tile 的 shared write 迭代次数。 其中，</p>
<ul>
<li>a_sh_stage = 512，表示A的一个 tile 的整体尺寸
<ul>
<li>A矩阵一个 tile
为<code>[16 x thread_m_blocks, 16 x thread_k_blocks]</code>，所以大小是
64 * 64 / 8</li>
<li>除以8是因为A的指针是int4类型，4个int32_t, 128
bit，对应8个fp16，</li>
</ul></li>
<li>a_sh_wr_delta = 256，表示between shared memory writes。
<ul>
<li>a_sh_wr_delta = a_sh_stride * (threads / a_gl_rd_delta_o);</li>
<li>8 * (256 / 8) = 256</li>
</ul></li>
</ul>
<p><img src="/images/marlin/4.png" width="95%"></p>
<p>也就意味着，一个 thread blocks
256个线程能够读取256个<code>int4</code>，一个 tile 有 512
个<code>int4</code>，读取A矩阵 tile 要循环两次。在后面的
<code>fetch_to_shared</code> 函数里，一次是 fetch A 的一个
tile，这个地方会体现出循环了两次。</p>
<h1 id="a_sh_wr_pred-变量">a_sh_wr_pred 变量</h1>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Precompute which thread should not read memory in which iterations; this is needed if there are more threads than</span></span><br><span class="line"><span class="comment">// required for a certain tilesize or when the batchsize is not a multiple of 16.</span></span><br><span class="line"><span class="type">bool</span> a_sh_wr_pred[a_sh_wr_iters];</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a_sh_wr_iters; i++)</span><br><span class="line">  a_sh_wr_pred[i] = a_sh_wr_delta * i + a_sh_wr &lt; a_sh_stride * prob_m;</span><br><span class="line"><span class="type">bool</span> s_sh_wr_pred = threadIdx.x &lt; s_sh_stride;</span><br></pre></td></tr></table></figure>
<p>在矩阵运算中预先计算出哪些线程在特定的迭代过程中不应该从内存中读取数据。这是为了优化计算资源的使用，特别是在以下两种情况下：</p>
<ul>
<li>线程数超过所需的 tile
大小时：即当前的任务需要的线程数比实际提供的线程数要少，这可能会导致一些线程不需要参与内存读取。</li>
<li>batchsize 不是 16
的倍数时：当批处理大小无法整齐地划分时，可能会出现某些线程无需读取数据的情况。</li>
</ul>
<p><code>#pragma unroll</code>：这个编译器指令提示编译器将循环展开（unroll），以减少循环控制的开销，优化性能。通常用于
GPU 编程中的小规模循环，因为展开循环可以减少分支跳转。</p>
<h1 id="a矩阵的load">A矩阵的load</h1>
<h2 id="关于-bank-conflict-的背景知识">关于 bank conflict
的背景知识</h2>
<p>Shared memory 是片上存储器，因此与 local memory 或 global memory
相比更高的带宽和更低的延迟。前提是线程之间没有 bank conflicts。</p>
<p>为了实现高带宽，共享内存被划分为大小相等的内存模块，称为
Banks，可以同时访问。因此，任何由 n
个地址组成的内存读取或写入请求都可以同时提供服务，从而产生比单个模块带宽高
n 倍的总带宽。</p>
<p>但是，如果warp内多个线程的内存请求的两个地址位于同一Bank中，则存在bank
conflict，并且必须序列化访问。也就是说，<strong>Bank conflict 是在一个
warp 内，有2个或者以上的线程访问了同一个 bank
上不同地址的内存。</strong></p>
<p>shared memory被分为 32 bank，每个bank的位宽是 4 byte。最大
transaction 大小为 128 bit。如果每个线程请求 16 bit，那么 warp
宽度将为每次请求（warp 宽度）的 512 bit。</p>
<p>当GPU每个线程访存大于 4 bytes，即每个 warp 大于
<code>4 * 32 = 128</code> bytes 时,GPU 不会发出单个transaction，GPU
会将其分成 4 个 transactions（在这种情况下：T0-T7
组成一个transaction，T8-T15
是一个transaction，依此类推），每个transaction的宽度为 128 bit。</p>
<p>需要注意的是，bank conflicts 的确定是按 <strong>transaction</strong>
进行的，而不是按request、warp 或instruction进行的。</p>
<p>因此，每个 wrap 则会分割成多个 transaction 去执行，每个 transaction
保证线程内的访存不落在同一bank即可，所以当我们用最大访存指令时，需要保证1/4个连续线程不会存在地址重叠。</p>
<p>这也就是这个说法：</p>
<p>an access to Shared Memory will be conflict-free if the following
conditions are satisfied across each warp:</p>
<ul>
<li>{T0, T1, .., T7} do not access the same 128-bit bank</li>
<li>{T8, T9, .., T15} do not access the same 128-bit bank</li>
<li>{T16, T17, .., T23} do not access the same 128-bit bank</li>
<li>{T24, T25, .., T31} do not access the same 128-bit bank</li>
</ul>
<h2 id="a矩阵的解决方式">A矩阵的解决方式</h2>
<p>在marlin中，为了提高 load
效率，一般会使用<strong>向量化</strong>的读取命令，一次读取 128
bit，也就是 16 byte，对应<strong>4个bank</strong>。那么 8
个线程就可以一次完成 32 个bank 的load，所以问题简化为研究
<code>T0 - T7</code> or <code>T8 - T15</code> or <code>T16 - T23</code>
or <code>T24 - T32</code> 这 8 个线程内没有 bank 冲突。</p>
<p>如上文所讲，bank conflict 是针对单次 memory transaction
而言的。如果单次 memory transaction 需要访问的 128 bytes 中有多个 word
属于同一个 bank，就产生了 bank conflict。</p>
<p>对于A矩阵这种<strong>激活值矩阵</strong>而言，没有办法提前pack，因此
ldmatrix 指令<strong>读取</strong>的时候，会发生bank
conflict。为了解决这个问题，需要对A矩阵的存储地址进行转换。对于ij位置可以通过转存到i(i
⊕ j)位置来避免冲突， 其中⊕是异或计算。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> transform_a = [&amp;] (<span class="type">int</span> i) &#123;</span><br><span class="line">  <span class="type">int</span> row = i / a_gl_rd_delta_o;</span><br><span class="line">  <span class="keyword">return</span> a_gl_rd_delta_o * row + (i % a_gl_rd_delta_o) ^ row;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// Since the computation of this remapping is non-trivial and, due to our main loop unrolls, all shared memory </span></span><br><span class="line"><span class="comment">// accesses are static, we simply precompute both transformed reads and writes.</span></span><br><span class="line"><span class="type">int</span> a_sh_wr_trans[a_sh_wr_iters];</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a_sh_wr_iters; i++)</span><br><span class="line">  a_sh_wr_trans[i] = transform_a(a_sh_wr_delta * i + a_sh_wr);</span><br></pre></td></tr></table></figure>
<p>由于重映射计算较为复杂，且主循环进行了展开（unroll），所有的共享内存访问都是静态的，因此预先计算好重映射的读取和写入索引可以提高性能。</p>
<h2 id="swizzle-的实现">Swizzle 的实现</h2>
<p>A矩阵的swizzle实现是<code>transform_a</code>这个函数，其实现很简单</p>
<ul>
<li><code>row = i / a_gl_rd_delta_o</code> 算出来是在第几行</li>
<li><code>i % a_gl_rd_delta_o</code> 算出来是在第几列</li>
<li><code>(i % a_gl_rd_delta_o) ^ row</code>
进行异或，修改了列的位置</li>
<li><code>a_gl_rd_delta_o * row + (i % a_gl_rd_delta_o) ^ row</code>加上整体偏移</li>
</ul>
<p><code>a_sh_wr_trans</code>这个数组记录了进行swizzle的相关映射。</p>
<p>做了个动图看一下，就非常明晰了：</p>
<p><img src="/images/marlin/5.gif" width="95%"></p>
<p>从 global memory 到 shared memory
搬运的代码实现是在<code>fetch_to_shared</code>函数中 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">int4* sh_a_stage = sh_a + a_sh_stage * pipe;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a_sh_wr_iters; i++) &#123;</span><br><span class="line">  cp_async4_pred(</span><br><span class="line">    &amp;sh_a_stage[a_sh_wr_trans[i]],</span><br><span class="line">    &amp;A[a_gl_rd_delta_i * i + a_gl_rd + a_gl_rd_delta_o * a_off],</span><br><span class="line">    a_sh_wr_pred[i]</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Predicated asynchronous global-&gt;shared copy; used for inputs A where we apply predication to handle batchsizes that</span></span><br><span class="line"><span class="comment">// are not multiples of 16.</span></span><br><span class="line">__device__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">cp_async4_pred</span><span class="params">(<span class="type">void</span>* smem_ptr, <span class="type">const</span> <span class="type">void</span>* glob_ptr, <span class="type">bool</span> pred = <span class="literal">true</span>)</span> &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> BYTES = <span class="number">16</span>;</span><br><span class="line">  <span class="type">uint32_t</span> smem = static_cast&lt;<span class="type">uint32_t</span>&gt;(__cvta_generic_to_shared(smem_ptr));</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="string">&quot;&#123;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;   .reg .pred p;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;   setp.ne.b32 p, %0, 0;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;   @p cp.async.cg.shared.global [%1], [%2], %3;\n&quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;&#125;\n&quot;</span> :: <span class="string">&quot;r&quot;</span>((<span class="type">int</span>) pred), <span class="string">&quot;r&quot;</span>(smem), <span class="string">&quot;l&quot;</span>(glob_ptr), <span class="string">&quot;n&quot;</span>(BYTES)</span></span><br><span class="line"><span class="params">  )</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 由此可见，copy矩阵A 使用的是异步拷贝指令。</p>
<ul>
<li><p>cp.async
指令用于从全局内存异步拷贝数据到共享内存，不会阻塞其他计算操作。后面在讲流水线的时候还会再提到这块。</p></li>
<li><p>通过 pred
参数，允许控制是否执行这次异步拷贝。它用于处理批处理大小不是 16
的情况，确保在某些条件下可以跳过无效的数据拷贝操作。</p></li>
<li><p>16 字节是 GPU
进行全局到共享内存拷贝的一个高效的最小块，这样能够提升拷贝效率。</p></li>
</ul>
<h2 id="load到寄存器">load到寄存器</h2>
<p>接下来看一下A矩阵是怎么load到寄存器的。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; thread_m_blocks; i++)</span><br><span class="line">  ldsm4(frag_a[k % <span class="number">2</span>][i], &amp;sh_a_stage[a_sh_rd_trans[k % b_sh_wr_iters][i]]);</span><br></pre></td></tr></table></figure>
<p>可以看到，在 shared memory 到 frag 的过程中，用到了
<code>a_sh_rd_trans</code>
进行索引，而这也是通过上述的<code>transform_a</code>函数实现的，相当于往shared
memory写的时候是按照异或规则写的，读的时候也要按照异或规则读出来，这样才能正确地映射。</p>
<hr />
<p>说一下题外话：</p>
<p>其实这里我纠结了很久，关于bank conflict 和
swizzle的内容我能够理解，但是我不能理解 marlin 用 <code>swizzle</code>
实现 <code>bank conflict free</code>: 在 marlin 中，A矩阵是 row-major
排布的，所以我一直疑惑为什么需要进行 swizzle ：这个矩阵在 global memory
本身就是 row-major 排布的，岂不是按照原始顺序 load 到 shared
memory，然后顺序读取的话也不会 bank conflict？</p>
<p>为此我在<a
href="https://www.zhihu.com/question/667972067">cuda的swizzle是怎么实现bank
conflict free的？</a>进行了提问，<a
href="https://www.zhihu.com/question/667972067/answer/3634692524">Arthur</a>的回答我认为很合理，即：</p>
<ul>
<li>swizzle 是在更复杂访问模式下，确保每个线程不会访问到相同的memory
bank。</li>
<li>如果矩阵是row-major的且读取是连续的，那么可能无需swizzle操作就能避免bank
conflict。</li>
<li>但如果存在交错访问或者更复杂的访问模式，则swizzle是有必要的，用以确保bank
conflict free。</li>
</ul>
<p>作为一个cuda小白最开始对这里非常疑惑，这里整理一下看过的一些相关资料：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/671419093">cute 之 Swizzle</a>
- reed</li>
<li><a href="https://zhuanlan.zhihu.com/p/684250988">cute
Swizzle细谈</a> - 进击的Killua</li>
<li><a href="https://zhuanlan.zhihu.com/p/706796240">cute swizzle</a> -
shengying.wei</li>
<li><a
href="https://zhuanlan.zhihu.com/p/659142274">CUDA编程概念:什么是bank
conflict？</a> - likewind1993</li>
<li><a
href="https://zhuanlan.zhihu.com/p/681966685">[深入分析CUTLASS系列] 0x03
cutlass 源码分析(二) --- bank conflict free 的shared memory layout
(附tvm等价pass)</a> - JoeNomad</li>
<li><a href="https://zhuanlan.zhihu.com/p/690052715">搞懂 CUDA Shared
Memory 上的 bank conflicts 和向量化指令（LDS.128 /
float4）的访存特点</a> - Alan 小分享</li>
<li><a
href="https://forums.developer.nvidia.com/t/how-to-understand-the-bank-conflict-of-shared-mem/260900">How
to understand the bank conflict of shared_mem</a></li>
</ul>
<hr />
<p>后来发现，如果不做 swizzle 其实是会发生 <code>bank conflict</code>
的，有一个细节：</p>
<p>在计算<code>a_sh_rd_trans</code>的时候 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; b_sh_wr_iters; i++) &#123;</span><br><span class="line">  <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; thread_m_blocks; j++)</span><br><span class="line">    a_sh_rd_trans[i][j] = transform_a(a_sh_rd_delta_o * i + a_sh_rd_delta_i * j + a_sh_rd); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>要注意输入<code>a_sh_rd</code>变量：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Shared read index.</span></span><br><span class="line"><span class="type">int</span> a_sh_rd = a_sh_stride * ((threadIdx.x % <span class="number">32</span>) % <span class="number">16</span>) + (threadIdx.x % <span class="number">32</span>) / <span class="number">16</span>;</span><br><span class="line">a_sh_rd += <span class="number">2</span> * ((threadIdx.x / <span class="number">32</span>) / (thread_n_blocks / <span class="number">4</span>));</span><br></pre></td></tr></table></figure>
<p>这里估计是最难懂的一点，<a
href="https://zhuanlan.zhihu.com/p/698715169">fp16*int4计算kernel--MARLIN代码走读</a>
中解释了这一点：</p>
<p>第一行结合关于ldmatrix的说明，在 thread 0-32 分别获取对应 ldmatrix
需要的地址。这里可以看出确实是需要做swizzle操作。</p>
<p>第二行的 (thread_n_blocks / 4) 的逻辑：</p>
<ul>
<li><p>256个线程，至少需要 b_sh_wr_iters
次后处理完一行，那就需要一次读取
<code>thread_k_blocks/b_sh_wr_iters</code> 个 SUB_TILE。</p></li>
<li><p><code>thread_k_blocks/b_sh_wr_iters = threads/8/thread_n_blocks</code></p></li>
<li><p>为了能够利用资源，如果不重复读取，就会处理<code>threads/32</code>个SUB_TILE。</p></li>
<li><p>但是我们可以重复读取A矩阵来进行乘法后再归约，那么就有</p></li>
<li><p><code>threads/32/x = threads/8/thread_n_blocks</code></p></li>
<li><p>因此 <code>x = thread_n_blocks/4</code></p></li>
<li><p>这就是<code>thread_n_blocks/4</code>的由来。</p></li>
<li><p>因此A矩阵，在LDMATRIX后，有重复矩阵。</p></li>
</ul>
<p>如果仔细分析的话，可以这么理解：</p>
<ul>
<li>实际上的GLOBAL IO没有变化，还是GLOBAL-&gt;SHARED的大小</li>
<li>运算并没有变多，B矩阵一直在变化，所以不会有重复计算。</li>
</ul>
<p>简单来说就是为了<strong>配平IO和MMA操作</strong>，做的妖操作。<a
href="https://zhuanlan.zhihu.com/p/698715169">fp16*int4计算kernel--MARLIN代码走读</a></p>
<hr />
<p>这里画了一个图展示Global memory -&gt; Shared Memory -&gt; Frag
的过程：</p>
<p><img src="/images/marlin/6.png" width="95%"></p>
<p>需要注意的是，<code>ldmatrix</code> 指令从 shared mem
加载数据到寄存器时会自动处理，使得寄存器中的数据满足这种特定的
layout。</p>
<h2 id="ldmatrix函数">ldmatrix函数</h2>
<p><a href="https://zhuanlan.zhihu.com/p/702818267">CUTLASS CuTe
GEMM细节分析（一）——ldmatrix的选择</a>
一文中指出了ldmatrix在加载地址上的灵活性，也就是ldmatrix并不要求这8个行在Shared
Memory上连续存储，但每个行内部必须是连续存储的。这一点解疑了我关于A整体不连续是怎么ldmatrix的这个疑问。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Instruction for loading a full 16x16 matrix fragment of operand A from shared memory, directly in tensor core layout.</span></span><br><span class="line">__device__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">ldsm4</span><span class="params">(FragA&amp; frag_a, <span class="type">const</span> <span class="type">void</span>* smem_ptr)</span> &#123;</span><br><span class="line">  <span class="type">uint32_t</span>* a = reinterpret_cast&lt;<span class="type">uint32_t</span>*&gt;(&amp;frag_a);</span><br><span class="line">  <span class="type">uint32_t</span> smem = static_cast&lt;<span class="type">uint32_t</span>&gt;(__cvta_generic_to_shared(smem_ptr));</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="string">&quot;ldmatrix.sync.aligned.m8n8.x4.shared.b16 &#123;%0,%1,%2,%3&#125;, [%4];\n&quot;</span></span></span><br><span class="line"><span class="params">    : <span class="string">&quot;=r&quot;</span>(a[<span class="number">0</span>]), <span class="string">&quot;=r&quot;</span>(a[<span class="number">1</span>]), <span class="string">&quot;=r&quot;</span>(a[<span class="number">2</span>]), <span class="string">&quot;=r&quot;</span>(a[<span class="number">3</span>]) : <span class="string">&quot;r&quot;</span>(smem)</span></span><br><span class="line"><span class="params">  )</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/#mma-ldmatrix-fragments">Warp-level
matrix load instruction: ldmatrix</a></p>
<p>这里使用了<code>ldmatrix.sync.aligned.m8n8.x4</code>,第二、三、四个矩阵的元素按照上表的布局加载到每个线程的后续目标寄存器中。</p>
<p><img src="/images/marlin/mma-ldmatrix-fragments.png" width="95%"></p>
<p>这样，A矩阵就被按照特定的 layout 加载到了寄存器中。而这个 layout 又是
mma 指令所需要的：</p>
<p>在 <a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/#matrix-fragments-for-mma-m16n8k16-with-floating-point-type">matrix-fragments-for-mma-m16n8k16-with-floating-point-type</a>中可以看到
Multiplicand A 的 The layout of the fragments held by different
threads:</p>
<p><img src="/images/marlin/mma-16816-A-f16.png" width="95%"></p>
<h1 id="b矩阵的load">B矩阵的load</h1>
<p>B矩阵的load相对简单，没有做swizzle。因为是顺序 fetch 到 SMEM，再顺序
fetch 到 frag，B矩阵经过重排，使得MMA计算需要的[8, 16]
tile全部处于一行，所以也不会有 bank conflict。</p>
<h2 id="矩阵排布">矩阵排布</h2>
<p>需要关注的是在算子之前的矩阵的排布，也就是 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">perm = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">    perm1 = []</span><br><span class="line">    col = i // <span class="number">4</span></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> [</span><br><span class="line">            <span class="number">2</span> * (i % <span class="number">4</span>),</span><br><span class="line">            <span class="number">2</span> * (i % <span class="number">4</span>) + <span class="number">1</span>,</span><br><span class="line">            <span class="number">2</span> * (i % <span class="number">4</span> + <span class="number">4</span>),</span><br><span class="line">            <span class="number">2</span> * (i % <span class="number">4</span> + <span class="number">4</span>) + <span class="number">1</span></span><br><span class="line">        ]:</span><br><span class="line">            perm1.append(<span class="number">16</span> * row + col + <span class="number">8</span> * block)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        perm.extend([p + <span class="number">256</span> * j <span class="keyword">for</span> p <span class="keyword">in</span> perm1])</span><br><span class="line"></span><br><span class="line">perm = np.array(perm)</span><br><span class="line">interleave = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line">perm = perm.reshape((-<span class="number">1</span>, <span class="number">8</span>))[:, interleave].ravel()</span><br></pre></td></tr></table></figure>
这个过程，实现了Multiplicand B 的 The layout of the fragments held by
different threads：</p>
<p><img src="/images/marlin/mma-16816-B-f16.png" width="95%"></p>
<p>当然，后面还是做了一个 interleave 操作的，所以 B
的真正排布是在这个基础上做了 interleave
的，但是在进行dequant的时候又反interleave
了，所以这里的分析就不管interleave了，更直观一些。</p>
<p><img src="/images/marlin/9.png" width="95%"></p>
<h1 id="matmul">matmul</h1>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Execute the actual tensor core matmul of a sub-tile. </span></span><br><span class="line"><span class="keyword">auto</span> matmul = [&amp;] (<span class="type">int</span> k) &#123;</span><br><span class="line">  <span class="comment">// We have the m dimension as the inner loop in order to encourage overlapping dequantization and matmul operations.</span></span><br><span class="line">  <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; j++) &#123;</span><br><span class="line">    <span class="type">int</span> b_quant = frag_b_quant[k % <span class="number">2</span>][j];</span><br><span class="line">    <span class="type">int</span> b_quant_shift = b_quant &gt;&gt; <span class="number">8</span>;</span><br><span class="line">    FragB frag_b0 = dequant(b_quant);</span><br><span class="line">    <span class="comment">// If there are no groups, we can just scale the final output once and can avoid doing so for each weight.</span></span><br><span class="line">    <span class="keyword">if</span> (group_blocks != <span class="number">-1</span>)</span><br><span class="line">      scale(frag_b0, frag_s[k % <span class="number">2</span>][j], <span class="number">0</span>);</span><br><span class="line">    FragB frag_b1 = dequant(b_quant_shift);</span><br><span class="line">    <span class="keyword">if</span> (group_blocks != <span class="number">-1</span>)</span><br><span class="line">      scale(frag_b1, frag_s[k % <span class="number">2</span>][j], <span class="number">1</span>);</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; thread_m_blocks; i++) &#123;</span><br><span class="line">      mma(frag_a[k % <span class="number">2</span>][i], frag_b0, frag_c[i][j][<span class="number">0</span>]);</span><br><span class="line">      mma(frag_a[k % <span class="number">2</span>][i], frag_b1, frag_c[i][j][<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>关于dequant这个函数，我之前在 <a
href="https://zhuanlan.zhihu.com/p/710203211">WeightonlyGEMM:
dequantize_s4_to_fp16x2代码解析</a> 详细分析过。思路是一样的，区别就是
marlin 想要 <code>signed int4</code> 输出，于是将 <code>-8</code> 这个
symmetric zero point 直接融合到 <code>SUB</code> 和 <code>ADD</code> 的
magic number 中。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__device__ <span class="keyword">inline</span> FragB <span class="title function_">dequant</span><span class="params">(<span class="type">int</span> q)</span> &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> LO = <span class="number">0x000f000f</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> HI = <span class="number">0x00f000f0</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> EX = <span class="number">0x64006400</span>;</span><br><span class="line">  <span class="comment">// Guarantee that the `(a &amp; b) | c` operations are LOP3s.</span></span><br><span class="line">  <span class="type">int</span> lo = lop3&lt;(<span class="number">0xf0</span> &amp; <span class="number">0xcc</span>) | <span class="number">0xaa</span>&gt;(q, LO, EX);</span><br><span class="line">  <span class="type">int</span> hi = lop3&lt;(<span class="number">0xf0</span> &amp; <span class="number">0xcc</span>) | <span class="number">0xaa</span>&gt;(q, HI, EX);</span><br><span class="line">  <span class="comment">// We want signed int4 outputs, hence we fuse the `-8` symmetric zero point directly into `SUB` and `ADD`.</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> SUB = <span class="number">0x64086408</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> MUL = <span class="number">0x2c002c00</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> ADD = <span class="number">0xd480d480</span>;</span><br><span class="line">  FragB frag_b;</span><br><span class="line">  frag_b[<span class="number">0</span>] = __hsub2(</span><br><span class="line">    *reinterpret_cast&lt;half2*&gt;(&amp;lo),</span><br><span class="line">    *reinterpret_cast&lt;<span class="type">const</span> half2*&gt;(&amp;SUB)</span><br><span class="line">  );</span><br><span class="line">  frag_b[<span class="number">1</span>] = __hfma2(</span><br><span class="line">    *reinterpret_cast&lt;half2*&gt;(&amp;hi),</span><br><span class="line">    *reinterpret_cast&lt;<span class="type">const</span> half2*&gt;(&amp;MUL), *reinterpret_cast&lt;<span class="type">const</span> half2*&gt;(&amp;ADD)</span><br><span class="line">  );</span><br><span class="line">  <span class="keyword">return</span> frag_b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里仅以 e0 和 e1 为例：</p>
<p><img src="/images/marlin/7.png" width="95%"></p>
<p>进行 <code>lop3</code> 操作后，提出来 e0 和 e1
元素，但此时的结果是加了 2^10 的，因此要减去 1024，以及减 8 这个zero
point，<code>SUB = 0x64086408</code> 中的 <code>6408</code> 对应的就是
1032。</p>
<p>frag_b_quant 类型是 <code>Vec&lt;int, 4&gt;</code>，这里的 j
循环是对这4个int的循环。</p>
<p>FragB 类型是 <code>Vec&lt;half2, 2&gt;</code>，也就是说
<code>dequant</code> 函数计算后得到的是2个 half2.</p>
<p>对应关系是这样的：</p>
<p><img src="/images/marlin/8.png" width="95%"></p>
<p>对B进行dequant后，就进行了 mma 操作。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; thread_m_blocks; i++) &#123;</span><br><span class="line">  mma(frag_a[k % <span class="number">2</span>][i], frag_b0, frag_c[i][j][<span class="number">0</span>]);</span><br><span class="line">  mma(frag_a[k % <span class="number">2</span>][i], frag_b1, frag_c[i][j][<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/images/marlin/10.jpg" width="40%"></p>
<p><a href="https://zhuanlan.zhihu.com/p/702818267">图片来源</a></p>
<p>在m维度的循环是内循环，这样可以复用 b 的 dequant 结果。</p>
<p>一次matmul调用，外循环迭代4次，即在B矩阵的n方向计算4个16x16的B矩阵小块。内循环迭代thread_m_blocks次，即在A矩阵的m方向计算thread_m_blocks个16x16小块。每次内循环调用两次mma指令，完成一个
m16n16k16 的子块计算。</p>
<p>为什么每个matmul要计算4个B矩阵的 16x16 子块呢？这是因为从 shared mem
加载数据到 reg 的时候，每个线程读取 128 bit，每个 warp 个 32
线程总共读取 128x32 bits数据，而4个16x16
B矩阵块的数据量正好就是(16x16/8)x4x32
bits。这样可以使用所有的线程去加载计算所需要的数据。</p>
<h2 id="mma-指令">mma 指令</h2>
<p>其中 mma 是用的 <a
href="https://docs.nvidia.com/cuda/parallel-thread-execution/#multiply-and-accumulate-instruction-mma">mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32</a></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// m16n8k16 tensor core mma instruction with fp16 inputs and fp32 output/accumulation.</span></span><br><span class="line">__device__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">mma</span><span class="params">(<span class="type">const</span> FragA&amp; a_frag, <span class="type">const</span> FragB&amp; frag_b, FragC&amp; frag_c)</span> &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span>* a = reinterpret_cast&lt;<span class="type">const</span> <span class="type">uint32_t</span>*&gt;(&amp;a_frag);</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span>* b = reinterpret_cast&lt;<span class="type">const</span> <span class="type">uint32_t</span>*&gt;(&amp;frag_b);</span><br><span class="line">  <span class="type">float</span>* c = reinterpret_cast&lt;<span class="type">float</span>*&gt;(&amp;frag_c);</span><br><span class="line">  <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="string">&quot;mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 &quot;</span></span></span><br><span class="line"><span class="params">    <span class="string">&quot;&#123;%0,%1,%2,%3&#125;, &#123;%4,%5,%6,%7&#125;, &#123;%8,%9&#125;, &#123;%10,%11,%12,%13&#125;;\n&quot;</span></span></span><br><span class="line"><span class="params">    : <span class="string">&quot;=f&quot;</span>(c[<span class="number">0</span>]), <span class="string">&quot;=f&quot;</span>(c[<span class="number">1</span>]), <span class="string">&quot;=f&quot;</span>(c[<span class="number">2</span>]), <span class="string">&quot;=f&quot;</span>(c[<span class="number">3</span>])</span></span><br><span class="line"><span class="params">    :  <span class="string">&quot;r&quot;</span>(a[<span class="number">0</span>]),  <span class="string">&quot;r&quot;</span>(a[<span class="number">1</span>]),  <span class="string">&quot;r&quot;</span>(a[<span class="number">2</span>]),  <span class="string">&quot;r&quot;</span>(a[<span class="number">3</span>]),  <span class="string">&quot;r&quot;</span>(b[<span class="number">0</span>]),  <span class="string">&quot;r&quot;</span>(b[<span class="number">1</span>]),</span></span><br><span class="line"><span class="params">       <span class="string">&quot;f&quot;</span>(c[<span class="number">0</span>]),  <span class="string">&quot;f&quot;</span>(c[<span class="number">1</span>]),  <span class="string">&quot;f&quot;</span>(c[<span class="number">2</span>]),  <span class="string">&quot;f&quot;</span>(c[<span class="number">3</span>])</span></span><br><span class="line"><span class="params">  )</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>mma() 是张量核心的矩阵乘法函数，执行 A × B 的操作，并将结果累积到
frag_c 中。由于量化后的数据拆分为两部分（frag_b0 和
frag_b1），因此执行了两次乘法，每次使用不同的子块进行运算。</p>
<p>在mma的计算过程中SM就已经做了tile层面的b_sh_wr_iters 的归约操作。</p>
<h1 id="a矩阵-和-b矩阵-的参数分析">A矩阵 和 B矩阵 的参数分析</h1>
<p>在m = 128，k = 256，n = 768的情况下：</p>
<h2 id="a-矩阵">A 矩阵</h2>
<p><img src="/images/marlin/a_gl.png" width="95%"> <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">a_gl_stride = prob_k / <span class="number">8</span>; <span class="comment">// stride of the A matrix in global memory 每一个 thread 会读取一个 int4，所以除以 128/16=8</span></span><br><span class="line">a_gl_rd_delta_o = <span class="number">16</span> * thread_k_blocks / <span class="number">8</span>; <span class="comment">// delta between subsequent A tiles in global memory</span></span><br><span class="line">a_gl_rd_delta_i = a_gl_stride * (threads / a_gl_rd_delta_o); <span class="comment">// between subsequent accesses within a tile</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/images/marlin/a_smem.png" width="50%"> <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">a_sh_stride = <span class="number">16</span> * thread_k_blocks / <span class="number">8</span>; <span class="comment">// stride of an A matrix tile in shared memory</span></span><br><span class="line">a_sh_wr_delta = a_sh_stride * (threads / a_gl_rd_delta_o); <span class="comment">// between shared memory writes</span></span><br><span class="line">a_sh_rd_delta_o = <span class="number">2</span> * ((threads / <span class="number">32</span>) / (thread_n_blocks / <span class="number">4</span>)); <span class="comment">// between shared memory tile reads</span></span><br><span class="line">a_sh_rd_delta_i = a_sh_stride * <span class="number">16</span>; <span class="comment">// within a shared memory tile</span></span><br><span class="line">a_sh_stage = a_sh_stride * (<span class="number">16</span> * thread_m_blocks); <span class="comment">// overall size of a tile</span></span><br><span class="line">a_sh_wr_iters = ceildiv(a_sh_stage, a_sh_wr_delta); <span class="comment">// number of shared write iterations for a tile</span></span><br></pre></td></tr></table></figure></p>
<h2 id="b-矩阵">B 矩阵</h2>
<p><img src="/images/marlin/b_gl.png" width="95%"></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">b_gl_stride = <span class="number">16</span> * prob_n / <span class="number">32</span>; </span><br><span class="line"><span class="comment">// B的16在最内维([16*thread_k_blocks, 16*thread_n_blocks]-&gt;[thread_k_blocks, 16*thread_n_blocks*16])，所以会乘以16；</span></span><br><span class="line"><span class="comment">// 每一个 thread 会读取一个 int4，所以除以 128/4=32. </span></span><br><span class="line">b_gl_rd_delta_o = b_gl_stride * thread_k_blocks;</span><br><span class="line">b_gl_rd_delta_i = b_gl_stride * (threads / b_sh_stride);</span><br></pre></td></tr></table></figure>
<p><img src="/images/marlin/b_smem.png" width="95%"></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">b_sh_stride = <span class="number">32</span> * thread_n_blocks / <span class="number">4</span>;</span><br><span class="line">b_sh_wr_delta = threads;</span><br><span class="line">b_sh_rd_delta = threads;</span><br><span class="line">b_sh_stage = b_sh_stride * thread_k_blocks;</span><br><span class="line">b_sh_wr_iters = b_sh_stage / b_sh_wr_delta;</span><br></pre></td></tr></table></figure>
<p><code>b_sh_stride = 32 * thread_n_blocks / 4;</code></p>
<p>这里困扰了我很久，在这里一定要记录一下：</p>
<ul>
<li>32 = 16 * 16 * 4 / 32
<ul>
<li>16 * 16 是warp大小</li>
<li>4是重复4次</li>
<li>32是类型转换 int4-&gt;INT4（128bit/4bit=32）</li>
</ul></li>
<li>4是重复4次</li>
</ul>
<h1 id="reduce">Reduce</h1>
<p><img src="/images/marlin/0_gemm.png" width="95%"></p>
<p><img src="/images/marlin/3.png" width="95%"></p>
<p>按照Marlin Kernel的Tile切分方式，一个C矩阵的Tile可能由多个thread
block参与计算。如上文，C矩阵第一个Tile只有 <code>thread block 0</code>
参与计算，而第二个Tile由 <code>block 0</code> 和 <code>block 1</code>
共同计算，两个 <code>block</code> 分别持有部分结果。</p>
<p>对于第一个 Tile 这种只由一个 block 参与计算的情况，由于一个 thread
block 中不同的 warp 分别持有部分结果，因此，只需要进行 thread block 内的
reduce，把不同 warp 持有的部分结果规约。这部分工作由
<code>thread_block_reduce()</code> 负责。thread_block_reduce() 利用
shared mem交换数据。</p>
<p>而对于第二个 Tile 这种由多个 block 参与计算的情况，则需要在进行
thread block 内的 Reduce 之后，再进行 block 间的 reduce，将多个 block
持有的部分结果进行规约。这部分工作由 <code>global_reduce()</code>
负责。global_reduce()利用 global mem 交换数据。</p>
<h1 id="block-reduce">block reduce</h1>
<h2 id="frag_c-寄存器">frag_c 寄存器</h2>
<p>先看一下 <code>FragC frag_c[thread_m_blocks][4][2];</code></p>
<ul>
<li><code>using FragC = Vec&lt;float, 4&gt;;</code>
mma后，一个线程是4个fp32；</li>
<li><code>thread_m_blocks</code>: 比较好理解，一个 tile 里面在 m
方向计算过 thread_m_blocks 个 16x16 小块；</li>
<li>4: 在n方向计算4个16x16的B矩阵小块；</li>
<li>2: subtile 即 <code>A(16,16) * B(16,16)</code> 分为2次 mma
执行。</li>
</ul>
<p><img src="/images/marlin/11.png" width="95%"></p>
<p>从 <code>frag_c</code> 的角度看第一个 slice 的过程：</p>
<ol type="1">
<li><code>zero_accums</code>函数将 <code>frag_c</code> 清零</li>
<li>循环 <code>slice_iters</code> 次，<code>frag_c</code>
的各个<code>[thread_m_blocks][4][2]</code> 不断在做累加，实现了在 tile
层面的 b_sh_wr_iters 的归约。</li>
<li>当 slice_iters == 0
的时候，也就意味着做完这个block在这个slice里做完mma了，于是要进行
<code>thread_block_reduce</code>，在K维度做
<code>thread_k_blocks/b_sh_wr_iters</code> 的归约。</li>
</ol>
<h2 id="block-reduce-分析">block reduce 分析</h2>
<p>在许多并行计算的情况下，k 维度（内积的长度）可能非常大，因此通过将 k
维度切分为多个块可以增加并行计算的粒度和线程的利用率。</p>
<ul>
<li>k 维度的切分：为了增加并行计算的线程块（warps）的数量，这里选择将 k
维度进行切分。切分后的每个 k 维度的子块可以由不同的 warp
进行并行计算。</li>
<li>增加 warp 数量：通过将 k 维度切分，不仅可以增加参与计算的 warp
数量，还能更好地利用 GPU 的并行计算能力，提高整体吞吐量。</li>
</ul>
<p>为了确保性能，代码选择只切分 k 维度，而保持 n
维度（列数）大小合理。这样可以保证每个线程块（warp）有足够的计算工作，但不会因为过多线程竞争导致
n 维度变得过大，从而降低局部数据重用的效率。</p>
<p>由于不同的 warp 都在并行处理相同的输出位置，因此每个 warp
只计算了部分的结果（partial
sums），这些部分结果需要在最后进行归约（reduction）操作.通过在共享内存中累积每个
warp
的部分和，可以高效地进行归约操作，而不必依赖全局内存。这避免了全局内存带来的延迟，同时利用共享内存实现快速的同步和数据共享。</p>
<p>通过切分 k 维度，这段代码增加了并行度，从而提高了 GPU 的利用率。每个
warp
计算部分和的结果，最终需要通过共享内存进行归约操作，将多个部分和合并为最终结果。这种方式可以有效提升矩阵乘法的性能，同时利用共享内存的高效性来完成必要的同步操作。</p>
<h2 id="代码">代码</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> m_block = <span class="number">0</span>; m_block &lt; thread_m_blocks; m_block++) &#123;</span><br><span class="line">  <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = red_off; i &gt; <span class="number">0</span>; i /= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (i &lt;= red_idx &amp;&amp; red_idx &lt; <span class="number">2</span> * i) &#123;</span><br><span class="line">      <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span> * <span class="number">2</span>; j++) &#123;</span><br><span class="line">        <span class="type">int</span> red_sh_wr = red_sh_delta * j + (red_sh_rd - red_sh_stride * i);</span><br><span class="line">        <span class="keyword">if</span> (i &lt; red_off) &#123;</span><br><span class="line">          <span class="type">float</span>* c_rd = reinterpret_cast&lt;<span class="type">float</span>*&gt;(&amp;sh[red_sh_delta * j + red_sh_rd]); <span class="comment">// 将共享内存的整数地址转换为 float* 指针，从而方便操作浮点数据。</span></span><br><span class="line">          <span class="type">float</span>* c_wr = reinterpret_cast&lt;<span class="type">float</span>*&gt;(&amp;sh[red_sh_wr]);</span><br><span class="line">          <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="number">4</span>; k++)</span><br><span class="line">            reinterpret_cast&lt;FragC*&gt;(frag_c)[<span class="number">4</span> * <span class="number">2</span> * m_block + j][k] += c_rd[k] + c_wr[k]; <span class="comment">//每个线程会将共享内存中的值 c_rd 和 c_wr 读取出来，并将其累加到 frag_c 中。</span></span><br><span class="line">        &#125;</span><br><span class="line">        sh[red_sh_wr] = reinterpret_cast&lt;int4*&gt;(&amp;frag_c)[<span class="number">4</span> * <span class="number">2</span> * m_block + j];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (red_idx == <span class="number">0</span>) &#123; <span class="comment">//当前线程是最后的归约线程，它负责将共享内存中最终的归约结果写回到 frag_c 中。</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span> * <span class="number">2</span>; i++) &#123;</span><br><span class="line">      <span class="type">float</span>* c_rd = reinterpret_cast&lt;<span class="type">float</span>*&gt;(&amp;sh[red_sh_delta * i + red_sh_rd]);</span><br><span class="line">      <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; j++) <span class="comment">// 循环遍历每个剩余的数据块，并将结果累加。</span></span><br><span class="line">        reinterpret_cast&lt;FragC*&gt;(frag_c)[<span class="number">4</span> * <span class="number">2</span> * m_block + i][j] += c_rd[j];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  __syncthreads(); <span class="comment">//线程同步，避免不同线程之间的数据竞争问题。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码实现的是
并行的对数级共享内存归约（reduction）操作，通过减少内存读写操作来提高归约的性能。对数级归约是指通过每次减少一半的数据规模来加速归约操作，时间复杂度为
O(log(n))，这里通过逐步合并部分结果实现归约。</p>
<p>在并行计算中，不仅需要在每个 warp
内部进行局部归约，还需要在计算的最后阶段对多个 warp
产生的中间结果进行合并，得到最终的结果。</p>
<h2 id="数据流向">数据流向</h2>
<p>由于每个线程的寄存器是私有的，其他线程无法访问，因此线程之间若需要共享数据或进行同步处理，必须通过共享内存来进行通信和数据交换。</p>
<p>所以block reduce只需要进行 <code>REG-&gt;SHARED-&gt;REG</code>
的转化，不涉及全局内存读取。而且这一过程避免了任何不必要的读取或写入迭代，例如，对于两个
warp，我们仅通过 warp 1 写入一次，仅通过 warp 0 读取一次。</p>
<h1 id="global-reduce">global reduce</h1>
<p>通过条状分区，同一列的数据会尽量集中在较少的线程块上处理，从而减少归约操作的频率。尽可能减少全局归约是优化并行程序性能的重要手段之一。</p>
<p>最终的归约操作是在 L2 缓存 中串行完成的。L2
缓存速度比全局内存快，但比共享内存慢。使用 L2
缓存进行归约操作具有以下优点：</p>
<ul>
<li>L2 缓存的优势：相比全局内存，L2
缓存的延迟较低，访问速度更快。在全局归约过程中，使用 L2
缓存可以加快数据的合并速度，减少内存带宽的消耗。</li>
<li>串行归约：归约操作是串行进行的，这可能是因为最终的输出数据较小，串行执行归约的开销很低。并且，串行操作可以简化编程复杂度，不需要额外的并行归约算法。</li>
</ul>
<p><img src="/images/marlin/12.png" width="95%"></p>
<p>看一下第二个 slice 的过程：</p>
<ul>
<li>第一次 <code>start_pipes</code> 后，block1 进行了 3
次迭代(slice_iters)，在最后一次做完mma后进行了
<code>thread_block_reduce</code></li>
<li>第二次 <code>start_pipes</code> 后，block0 进行了 1
次迭代(slice_iters)，在做完mma后进行了
<code>thread_block_reduce</code></li>
<li>注意此时 <code>last = slice_idx == slice_count - 1</code>
为true（要注意slice_idx是numbered bottom to top）</li>
</ul>
<p>这个slice的slice_count是大于1的，所以会进行global_reduce
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (slice_count &gt; <span class="number">1</span>) &#123; <span class="comment">// only globally reduce if there is more than one block in a slice</span></span><br><span class="line">  barrier_acquire(&amp;locks[slice_col], slice_idx);</span><br><span class="line">  global_reduce(slice_idx == <span class="number">0</span>, last);</span><br><span class="line">  barrier_release(&amp;locks[slice_col], last);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>global_reduce的工作原理其实是按照 <code>slice_idx</code>
的顺序依次进行规约：</p>
<p>1）第一个slice(slice_idx==0)，将自己持有的部分结果写到全局显存。</p>
<p>2）后续非最后一个slice从全局显存读取已经规约的结果，与自己持有的结果相加，结果写回全局显存。</p>
<p>3）最后一个slice从全局显存读取已经规约的结果与自己持有的结果相加。</p>
<h2 id="global_reduce-函数"><code>global_reduce</code> 函数</h2>
<p><img src="/images/marlin/13.png" width="60%"></p>
<p>对照这个流程图，对于 <code>slice_idx = 0</code> 的 block
而言，是first，不是last，所以是将结果转成 half 类型后写到 global memory
里面；</p>
<p>对于 <code>slice_idx = 1</code> 的 block
而言，不是first，不是last，所以是先将 global memory 的结果fetch到 shared
memory，转成 float 后进行 reduce，结果转成 half 类型后写到 global
memory里面；</p>
<p>对于 <code>slice_idx = 2</code> 的 block
而言，不是first，是last，所以是先将 global memory 的结果fetch到 shared
memory，转成 float 后进行 reduce，结果还在寄存器里面</p>
<p>按照我的理解，<code>slice_idx = 0</code> 的 block 和
<code>slice_idx = 1</code> 的 block 是在一个 pipe里面，那么怎么保证
<code>slice_idx = 1</code> 的 block 在<code>slice_idx = 0</code> 的
block
存完之后再操作呢？这种按slice顺序进行reduce的行为由<code>barrier_acquire</code>
和 <code>barrier_release</code> 函数来保证的。</p>
<h2 id="barrier_acquire-函数"><code>barrier_acquire</code> 函数</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Wait until barrier reaches `count`, then lock for current threadblock.</span></span><br><span class="line">__device__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">barrier_acquire</span><span class="params">(<span class="type">int</span>* lock, <span class="type">int</span> count)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="type">int</span> state = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">      <span class="comment">// Guarantee that subsequent writes by this threadblock will be visible globally.</span></span><br><span class="line">      <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(<span class="string">&quot;ld.global.acquire.gpu.b32 %0, [%1];\n&quot;</span> : <span class="string">&quot;=r&quot;</span>(state) : <span class="string">&quot;l&quot;</span>(lock))</span>;</span><br><span class="line">    <span class="keyword">while</span> (state != count);</span><br><span class="line">  &#125;</span><br><span class="line">  __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只有线程块中的第一个线程（threadIdx.x == 0）会执行对 lock
的检查。这是为了避免同一线程块中的所有线程都同时访问全局内存，造成资源竞争。</p>
<p><code>asm volatile ("ld.global.acquire.gpu.b32 %0, [%1];\n" : "=r"(state) : "l"(lock));</code>
这条内联汇编指令从全局内存中读取锁的值 lock，并将其存入 state。它使用了
CUDA 的 acquire
语义，这保证了在该点之前的所有读写操作对其他线程是可见的。</p>
<p>如果当前锁的值与 count
不一致，线程块中的第一个线程会一直循环，直到锁值等于
count。这意味着线程块会等待，直到其他线程块执行了对应的操作，修改了 lock
的值。</p>
<p>后面有一个线程同步，确保整个线程块中的所有线程在 lock
达到指定值之前不会继续执行后续代码。</p>
<p>这个函数整体是用于等待锁达到指定值，让线程块中的所有线程在达到barrier前等待。这是一个同步点，确保后续操作不会与前面已经发生的操作重叠。</p>
<h2 id="barrier_release-函数"><code>barrier_release</code> 函数</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Release barrier and increment visitation count.</span></span><br><span class="line">__device__ <span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">barrier_release</span><span class="params">(<span class="type">int</span>* lock, <span class="type">bool</span> reset = <span class="literal">false</span>)</span> &#123;</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (reset) &#123;</span><br><span class="line">      lock[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> val = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Make sure that all writes since acquiring this barrier are visible globally, while releasing the barrier. </span></span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(<span class="string">&quot;fence.acq_rel.gpu;\n&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(<span class="string">&quot;red.relaxed.gpu.global.add.s32 [%0], %1;\n&quot;</span> : : <span class="string">&quot;l&quot;</span>(lock), <span class="string">&quot;r&quot;</span>(val))</span>; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在释放barrier前，先进行线程同步，确保线程块内的所有线程都已经完成了各自的工作，达到了barrier点。
仍然是线程块内的第一个线程执行锁的更新操作，避免多线程对同一锁进行竞争访问。
如果 reset 为 true，则将 lock 重置为 0
并立即返回，不再执行后续的释放操作。这用于在这个slice的最后一次global
reduce
<code>asm volatile ("fence.acq_rel.gpu;\n");</code>：这条指令是一个“获取-释放（acquire-release）”内存屏障，它确保在屏障释放前，当前线程块对共享数据的所有写操作对其他线程块是可见的。</p>
<p><code>asm volatile ("red.relaxed.gpu.global.add.s32 [%0], %1;\n" : : "l"(lock), "r"(val));</code>：这里使用了
add.s32（32位整数加法）操作，通过原子操作将 lock 值加
1，表示当前线程块已经达到了屏障。其他等待这个锁的线程块就可以继续运行了。</p>
<p>这种加法操作使用了
<code>red.relaxed.gpu.global.add</code>，它是一个relaxed操作，不需要严格的同步模型，因此有助于性能优化。</p>
<p>这个函数整体是用于释放锁，并将锁的值加
1，表示当前线程块已经完成了当前阶段的工作。如果是在这个slice的最后一次global
reduce，则重置锁，以便重新初始化同步机制。</p>
<p>这两个函数共同实现了线程块之间的同步机制，确保数据在不同线程块之间传递时的一致性，同时最大化性能优化。</p>
<p>workspace：是用来做global_reduce的标志位，shape为[n / 128 *
max_par]，由于n维度最小切分粒度为128，因此C矩阵最多被切分为n / 128 *
max_par个slice，每个slice需要一个标志位。</p>
<h1 id="write_result">write_result</h1>
<p>global_reduce规约完不同thread
block的结果之后，C矩阵Tile的结果规约在最后一个slice的寄存器中了，因此，最后一个slice负责把结果写回全局显存，这部分工作由write_result()负责。</p>
<p>write_result()先将结果写回shared mem，再从shared
mem写回全局显存。同时，如果使用per_channel量化，则在这里会进行结果的scale操作。</p>
<p>因为共享内存访问比全局内存要快得多，所以先在共享内存中对结果进行重新排序，</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> write = [&amp;] (<span class="type">int</span> idx, <span class="type">float</span> c0, <span class="type">float</span> c1, FragS&amp; s) &#123;</span><br><span class="line">  half2 res = __halves2half2(__float2half(c0), __float2half(c1));</span><br><span class="line">  <span class="keyword">if</span> (group_blocks == <span class="number">-1</span>) <span class="comment">// for per-column quantization we finally apply the scale here</span></span><br><span class="line">    res = __hmul2(res, s[<span class="number">0</span>]);</span><br><span class="line">  ((half2*) sh)[idx] = res;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><code>res = __halves2half2(__float2half(c0), __float2half(c1))</code>
将两个单精度浮点数压缩成一个半精度浮点数对</p>
<p>如果
<code>group_blocks == -1</code>，说明需要进行按列量化（per-column
quantization）。</p>
<p>最后，重排后的 half2 结果被写入共享内存 sh 的指定位置 idx。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ceildiv(<span class="number">16</span> * thread_m_blocks, threads / (<span class="number">2</span> * thread_n_blocks)); i++) &#123;</span><br><span class="line">  <span class="keyword">if</span> (c_gl_wr &lt; c_gl_wr_end) &#123;</span><br><span class="line">    C[c_gl_wr] = sh[c_sh_rd];</span><br><span class="line">    c_gl_wr += c_gl_wr_delta;</span><br><span class="line">    c_sh_rd += c_sh_rd_delta;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>共享内存中的数据被整理后，通过循环依次写入全局内存。线程逐步读取共享内存中的数据（sh），并写入全局内存的对应位置（C）。</p>
<h1 id="任务调度">任务调度</h1>
<p>https://zhuanlan.zhihu.com/p/716412368</p>
<p>相比于CNN的卷积，LLM的矩阵乘有这样的特性：IO比乘法运算更重要，因此需要更加精致的优化。TILE的核心是在IO读取和计算之间的一种平衡，这种调度目前IO处理非常有优势。</p>
<p>疑惑的点：</p>
<p>矩阵A为什么是竖的 block reduce具体是哪些在reduce</p>
<h1 id="参考资料">参考资料：</h1>
<p>https://arxiv.org/pdf/2408.11743</p>
<p>进击的Killua：<a
href="https://zhuanlan.zhihu.com/p/716412368">MARLIN: Mixed-Precision
Auto-Regressive Parallel Inference on Large Language
Models论文解读</a></p>
<p>suluner：<a href="https://zhuanlan.zhihu.com/p/707470647">Marlin
W4A16&amp;W4A8代码走读</a></p>
<p>cutlass/media/docs/implicit_gemm_convolution.md at main ·
NVIDIA/cutlass</p>
<p>reed：<a href="https://zhuanlan.zhihu.com/p/665082713">cute 之
GEMM流水线</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/698715169">fp16*int4计算kernel--MARLIN代码走读</a></p>
<p><a
href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf">Optimizing
Parallel Reduction in CUDA</a></p>
<h1 id="ncu-rep">ncu-rep</h1>
<p>输入 m = 128, k = 256, n = 768</p>
<ul>
<li>0_base.ncu-rep: 原始版本，不限制SM数量</li>
<li>1_base_sm5.ncu-sep: 限制SM数量为5</li>
</ul>
<p><code>ncu --set full --target-processes all --kernel-name 'Marlin' --export base.ncu-rep python test.py</code></p>
<p>最开始设置 SM 为5，分析报告指出：</p>
<p>The grid for this launch is configured to execute only 5 blocks,
which is less than the GPU's 82 multiprocessors. This can underutilize
some multiprocessors. If you do not intend to execute this kernel
concurrently with other workloads, consider reducing the block size to
have at least one block per multiprocessor or increase the size of the
grid to fully utilize the available hardware resources. See the 
Hardware Model description for more details on launch
configurations.</p>
<p>于是解除对SM的限制，最终得到的报告如下：</p>
<p><img src="/images/marlin/ncu0.png" width="100%">
<img src="/images/marlin/ncu1.png" width="100%">
<img src="/images/marlin/ncu2.png" width="100%">
<img src="/images/marlin/ncu3.png" width="100%">
<img src="/images/marlin/ncu4.png" width="100%">
<img src="/images/marlin/ncu5.png" width="100%"></p>
<p>https://zhuanlan.zhihu.com/p/714731771</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>GEMM</tag>
        <tag>HPC</tag>
        <tag>CUDA</tag>
        <tag>MARLIN</tag>
      </tags>
  </entry>
  <entry>
    <title>git</title>
    <url>/2023/06/19/73_git/</url>
    <content><![CDATA[<p>日常的一些git操作其实已经非常熟悉了，比较欠缺的是团队协作遇到的一些问题。</p>
<p>在此记录一下需要经常查阅的情景。</p>
<span id="more"></span>
<h1
id="git将当前修改的内容提交到新的分支上">Git：将当前修改的内容提交到新的分支上</h1>
<h2 id="前言">前言：</h2>
<p>有时候在参加一个项目时，执行clone得到master分支，
一开始只是想看看源码或者忘记了自己没有新建分支，结果后面自己根据需求添加了代码<code>添加后没有执行commit</code>,
但是此时的修改都在master分支，
提交必然是不可以的，还是要新建分支<code>所有修改都要在新建分支上进行</code>，最后在分支执行通过后，才能合并到master分支。
那么，这时候如何力挽狂澜，如何在保存这些修改的前提下，新建分支并提交呢？</p>
<h2 id="操作思路">操作思路：</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//步骤1：在当前的master分支上的修改暂存起来</span></span><br><span class="line">git stash</span><br><span class="line"><span class="comment">//步骤2：暂存修改后，在本地新建分支（new_branch为新分支的名字）</span></span><br><span class="line">git checkout -b new_branch</span><br><span class="line"><span class="comment">//步骤3：将暂存的修改放到新建分支中</span></span><br><span class="line">git stash pop</span><br><span class="line"><span class="comment">//步骤4：使用TortoiseGit进行commit，比如add、modify、delete...</span></span><br><span class="line"><span class="comment">//步骤5：将提交的内容push到远程服务器</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure>
<p>————————————————</p>
<p>版权声明：本文为CSDN博主「Oruizn」的原创文章，遵循CC 4.0
BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Oruizn/article/details/111294375</p>
<h1
id="git创建本地分支并关联远程分支">git创建本地分支并关联远程分支</h1>
<h2 id="创建本地分支">1. 创建本地分支</h2>
<p><code>git branch 分支名</code></p>
<p>例如：<code>git branch dev</code>，这条命令是基于当前分支创建的本地分支，假设当前分支是master(远程分支)，则是基于master分支创建的本地分支dev。</p>
<h2 id="切换到本地分支">2. 切换到本地分支</h2>
<p><code>git checkout 分支名</code></p>
<p>例如：<code>git checkout dev</code>，这条命令表示从当前master分支切换到dev分支。
## 3. 创建本地分支并切换</p>
<p><code>git checkout -b 分支名</code></p>
<p>例如：<code>git checkout -b dev</code>，这条命令把创建本地分支和切换到该分支的功能结合起来了，即基于当前分支master创建本地分支dev并切换到该分支下。</p>
<h2 id="提交本地分支到远程仓库">4. 提交本地分支到远程仓库</h2>
<p><code>git push origin 本地分支名</code></p>
<p>例如：<code>git push origin dev</code>，这条命令表示把本地dev分支提交到远程仓库，即创建了远程分支dev。</p>
<blockquote>
<p>注：要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你创建的本地分支不会因为你的写入操作而被自动同步到你引入的远程服务器上，你需要明确地执行推送分支的操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。</p>
</blockquote>
<h2 id="新建本地分支与远程分支关联">5.新建本地分支与远程分支关联</h2>
<p><code>git branch –set-upstream 本地新建分支名 origin/远程分支名</code></p>
<p>或者</p>
<p><code>git branch –set-upstream-to=origin/远程分支名</code></p>
<p>例如：<code>git branch –set-upstream-to=origin/dev</code>，把本地dev分支和远程dev分支相关联。</p>
<blockquote>
<p>注：本地新建分支，
push到远程服务器上之后，使用<code>git pull</code>或者<code>git pull</code>
拉取或提交数据时会报错，必须使用命令：<code>git pull origin dev</code>（指定远程分支）；如果想直接使用<code>git pull</code>或<code>git push</code>拉去提交数据就必须创建本地分支与远程分支的关联。</p>
</blockquote>
<p>————————————————</p>
<p>版权声明：本文为CSDN博主「EntyIU」的原创文章，遵循CC 4.0
BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_59244784/article/details/129170582</p>
<hr />
<p>推git遇到了一个错误：</p>
<p><code>remote: fatal: pack exceeds maximum allowed size</code>。</p>
<p>估计以后也会再次遇到，记录一下解决步骤。</p>
<!-- more -->
<h2 id="解决方案">解决方案</h2>
<p>第一反应当然是google一下，顺利找到<a
href="https://stackoverflow.com/questions/67086183/git-push-remote-fatal-pack-exceeds-maximum-allowed-size">解决方案</a></p>
<p>First, you can use <code>git-sizer</code> to get an idea of what is
taking too much space in your current local repository (that you fail to
push)</p>
<ul>
<li><p>if it is because of a commit too big, you can:</p>
<ul>
<li><code>git reset @~</code> to cancel that commit</li>
<li>remake several smaller commits</li>
<li>try and push again</li>
</ul></li>
<li><p>if it is because of a file too big, you can try and activate
<code>Git LFS</code>, but that is limited by quota and going above might
include a non-free service.</p></li>
</ul>
<p>More generally, a "large project" might need to be split up into
several Git repositories.</p>
<h2 id="实践操作">实践操作</h2>
<p>1、安装git-sizer</p>
<p>https://formulae.brew.sh/formula/git-sizer</p>
<p><code>brew install git-sizer</code></p>
<p>2、分析文件大小</p>
<p>在git项目内执行<code>git-sizer</code></p>
<pre><code>Processing blobs: 6673                        
Processing trees: 1053                        
Processing commits: 2                        
Matching commits to trees: 2                        
Processing annotated tags: 0                        
Processing references: 3                        
| Name                         | Value     | Level of concern               |
| ---------------------------- | --------- | ------------------------------ |
| Biggest objects              |           |                                |
| * Blobs                      |           |                                |
|   * Maximum size         [1] |  60.9 MiB | ******                         |
|                              |           |                                |
| Biggest checkouts            |           |                                |
| * Maximum path depth     [2] |    10     | *                              |
| * Maximum path length    [2] |   146 B   | *                              |
| * Total size of files    [2] |  1.40 GiB | *                              |

[1]  ***********(delete)*********** (refs/heads/main:toolchain/***********(delete)***********)
[2]  ***********(delete)*********** (refs/heads/main^&#123;tree&#125;)</code></pre>
<p>好像也没有太大的文件，最大的也就是64MB。那应该是整个commit太大了。</p>
<p>于是我拆开多个分别push，解决了问题。</p>
<h2 id="lfs">LFS</h2>
<p>下次遇到不得不使用LFS的时候再来补充后续的博客</p>
<p><a href="https://www.jianshu.com/p/493b81544f80">Git
LFS的使用</a></p>
<p>brew install git-lfs</p>
<p>git lfs install</p>
]]></content>
      <tags>
        <tag>git</tag>
        <tag>branch</tag>
      </tags>
  </entry>
  <entry>
    <title>gdb</title>
    <url>/2023/06/19/79_gdb/</url>
    <content><![CDATA[<p>今天遇到段错误，旁观大佬一番操作定位解决。</p>
<p>大大提高开发效率的关键几步，值得记录一下。</p>
<span id="more"></span>
<p><code>gdb --args 运行命令</code></p>
<p><code>r</code></p>
<p><code>bt</code></p>
<p><code>p</code></p>
<h2 id="gdb调试传参args">gdb调试传参args</h2>
<p>假设我们的可执行程序是worker,gdb时需要传递参数arg1和arg2，有3种方式：</p>
<ul>
<li><p>方式1</p>
<p><code>gdb --args worker arg1 arg2</code></p></li>
<li><p>方式2: 进入gdb界面，然后设置参数</p>
<p><code>gdb worker</code></p>
<p><code>set args arg1 arg2</code></p>
<p><code>show args</code></p></li>
<li><p>方式3: 在run指令后面直接跟参数</p>
<p><code>gdb worker</code></p>
<p><code>r arg1 arg2</code></p></li>
</ul>
<h2 id="命令">命令</h2>
<p>run/<code>r</code> 运行</p>
<p>continue/c 继续运行</p>
<p>next/n 单步运行</p>
<p>quit/ctrl+d 退出GDB</p>
<p>print/<code>p</code> var 打印变量的值</p>
<p>print/p &amp;var 打印变量地址</p>
<p>printf/p *addr 打印地址的值</p>
<p><code>bt</code> 查看函数back trace</p>
]]></content>
      <tags>
        <tag>study</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title>Final Cut study notes</title>
    <url>/2023/07/08/75_Final-cut-study-notes/</url>
    <content><![CDATA[<p>学习Final Cut Pro的笔记，提高生产力。</p>
<span id="more"></span>
<h1 id="配置">配置</h1>
<hr />
<h1 id="插件">插件</h1>
<h2 id="motion-vfx"><a href="https://www.motionvfx.com/">MOTION
VFX</a></h2>
<p>一个插件集合的网站，巨好用！</p>
<h3 id="mcamrig摄像机运动"><a
href="https://www.motionvfx.com/store,mcamrig,p2219.html?category=63&amp;irgwc=1&amp;irgwc=1">mCamRig</a>摄像机运动</h3>
<ul>
<li>对出教程的UP主特别有用</li>
<li>可以快速制作缩放、旋转，添加镜头模糊、镜头运动效果</li>
<li>用法：
<ul>
<li>字幕里找到插件，拉到对应的片段上</li>
</ul></li>
</ul>
<h3 id="mvertical手机视频背景虚化"><a
href="https://www.motionvfx.com/store,mvertical,p2176.html?irgwc=1">mVertical</a>手机视频背景虚化</h3>
<ul>
<li>横版视频插入竖版视频时，填充两边的黑色</li>
<li>实现背景重叠的背景虚化</li>
<li>小技巧
<ul>
<li>选择片段，按一下X，就自动选择这个片段长度</li>
<li>在字幕里找到mVertical，按Q，就可以整段附着到这个片段</li>
</ul></li>
</ul>
<h2 id="commandpost最强辅助"><a
href="https://commandpost.io">Commandpost</a>最强辅助</h2>
<h1 id="清理缓存">清理缓存</h1>
<p>资源库 右键 显示包内容</p>
<h1 id="高效技巧">高效技巧</h1>
<p>添加视频时，进行了部分选择，标记 -&gt; 清除所选范围（快捷键：Option +
X 取消选择）</p>
<p>添加字幕：按enter会换行，按commond+enter退出编辑状态</p>
<p>把当前片段拉长（某一帧拉长） - 编辑 -&gt; 添加静帧 - 快捷键：shift +
H，片段是完整的没有被切割</p>
<p>固定音乐位置 - 第一帧最前面加一个空隙 Edit -&gt; Insert Generator
-&gt; Gap(option+W) - 选择Gap，control+D，输入1（1帧时长） -
把音乐放最前面 - 删除的时候，Option + commond + Delete</p>
<p>调整变速，不影响整体时长 - 右键， Lift from storyline - Overwrite to
Primary Storyline</p>
<p>添加的时候只要要画面，不要声音</p>
<p>Final cut pro 1. 资源管理 - 关系： 资源库 -&gt; 事件 -&gt; 项目 -
导入素材 - 按住 command， 手动点击素材，批量选择</p>
<p>2.添加 素材到时间线，基本操作 - 1. 片段选择 - 选择入点： I - 选择出点
： O - 添加到时间线： E - 取消选择 - Option + X - 2. 预览素材 -
顺序播放： L - 加速： 连续按2,3次L - 暂停 ：K - 倒序播放 ：J -
3.添加字幕 - 添加到选定时间点： Q - 4. 在2个素材之间快速插入： W - 5.
进阶学习 - 个人收藏 - 关键词 - 3.剪辑 - 1. 切割工具 - B - 2. 延长 缩短
选择的视频 - 逗号 和 句号 - 3. 切割并选中 - command + B - 4.
范围选择工具 - 按住R不松 - 5. 删除 播放头开始 到 选中位置 - Option + [ -
6. 删除 选中位置 到 素材结束位置 - Option + ] - 7. 删除 主时间线视频，
同时不删除连接的视频 ， 且保留 B-roll - Option + command + delete - 8.
速度条<br />
- command + R - 9. 速度条 分段 - shift + B - 10. 完整 显示时间线 - shift
+ Z - 11. 吸附 开关 - N - 12. 打上标记 - M - 13. 预览 开关 - S - 14.
音频 - 音量调节 - Control + or Control - - 15. 字幕 - 添加字幕 到 浏览头
右边 - Q - 添加字幕 到 浏览头 左边 - shift + Q - 直接添加 基本字幕 -
control + T - 16. 转场 - 添加默认转场 - command + T - 17. 添加 静止帧 -
Shift + H - 18. 添加 缝隙 - Option + W - -&gt; Control + D （设置帧数）
- 19. 从故事情节中 提取 - 不影响 时间线 option + command + 向上</p>
]]></content>
      <tags>
        <tag>finalcut</tag>
        <tag>notes</tag>
      </tags>
  </entry>
  <entry>
    <title>Roofline model学习</title>
    <url>/2023/06/05/77_Roofline/</url>
    <content><![CDATA[<blockquote>
<p>给定特定的计算平台，我们的算法实现是不是已经榨干硬件平台的最大能力，还有没有进一步改善的空间？这些问题寻寻觅觅答案，真像雾里看花，我们渴望有一双慧眼，帮我们穿透迷津。</p>
</blockquote>
<p>学习Roofline，有一种理论指导实践的感觉。</p>
<span id="more"></span>
<p>阅读论文：<a
href="https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf">Roofline:
An Insightful Visual Performance Model for Floating-Point Programs and
Multicore Architectures</a></p>
<p>博客：<a
href="https://blog.csdn.net/21cnbao/article/details/116213606?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168593229216800222880877%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=168593229216800222880877&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-116213606-null-null.142%5Ev88%5Einsert_down1,239%5Ev2%5Einsert_chatgpt&amp;utm_term=Roofline%3A%20An%20Insightful%20Visual%20Performance%20Model&amp;spm=1018.2226.3001.4187">Roofline模型初步</a></p>
<p>知乎：<a href="https://zhuanlan.zhihu.com/p/34204282">Roofline
Model与深度学习模型的性能分析</a></p>
<p>我的总结：</p>
<p>【更新中】</p>
]]></content>
      <tags>
        <tag>study</tag>
        <tag>Roofline</tag>
      </tags>
  </entry>
  <entry>
    <title>vtype_register</title>
    <url>/2023/06/14/81_vtype-register/</url>
    <content><![CDATA[<p>最近搞了搞risc-v v，vtype寄存器可谓是灵魂的存在了。</p>
<p>在此记录一下关于vtype寄存器相关的知识和结论。</p>
<span id="more"></span>
<h2 id="background">Background</h2>
<blockquote>
<p>With a vector length specific (VLS) SIMD instruction set the main
problem is to pick the right vector register size. Of course there is a
trade-off between the amount of data-level parallelism and hardware
costs. Due to Moore's law, vector register sizes can be increased over
time without making the CPU chip more expensive. Also, some users are
interested in powerful CPUs with wider vector registers while the
average user is fine with averagely sized register. Thus, there is no
one right vector register size.</p>
</blockquote>
<p>对于向量长度特定 (VLS) SIMD
指令集，主要问题是选择正确的向量寄存器大小。
当然，在数据级并行性和硬件成本之间存在权衡。
由于<code>摩尔定律</code>，<code>矢量寄存器的大小</code>可以随着时间的推移而增加，而不会增加
CPU 芯片的成本。 此外，一些用户对具有<code>更宽</code>向量寄存器的强大
CPU 感兴趣，而普通用户则对<code>平均大小</code>的寄存器感兴趣。
因此，<code>没有一个正确的向量寄存器大小</code>。</p>
<blockquote>
<p>The solution to all this is to design a variable length vector
instruction set. In that way the instructions are then agnostic to the
vector register size of a concrete CPU implementation. Thus, the binary
code is portable between low, middle and high-end CPUs, and
automatically makes use of wider registers in newer CPUs.</p>
</blockquote>
<p>解决方案是<code>设计一个可变长度的向量指令集</code>。
这样，指令就与具体 CPU 实现的向量寄存器大小无关。
因此，二进制代码可在低端、中端和高端 CPU 之间移植，并自动在较新的 CPU
中使用更宽的寄存器。</p>
<p>个人感觉，RISC-V这个后起之秀就胜在不用顾及历史积累，可以很轻易的从头开始。</p>
<h2 id="design-challenges-opcode-space">Design Challenges: Opcode
Space</h2>
<p>Community wanted to stay with 32-bit instruction encoding</p>
<ul>
<li>Low-end embedded systems have 32-bit instruction fetch</li>
<li>Harder to support mixed-length instruction streams, 16,32,48, &amp;
64 bits</li>
<li>Static code size matters on embedded platforms</li>
</ul>
<p>But wanted vast array of datatypes and custom datatypes, and large
set of operations</p>
<h2 id="opcode-solution">Opcode Solution</h2>
<p>There are two registers used when operating vectors in RVV</p>
<h3 id="寄存器vtype-vector-type.">寄存器<strong>vtype: Vector
Type.</strong></h3>
<p>Added a control register to hold some information about current
setting of vector unit</p>
<p>vtype describes
<code>the type of vector we are going to operate</code> and includes</p>
<p>vtype fields (total additional 6-7 state bits) - vsew: standard
element width (SEW=8,16,32,…,1024) - Size in bits of the elements being
operated - <code>8 ≤ sew ≤ ELEN</code> - vlmul: vector length multiplier
(LMUL=1,2,4,8) - <span class="math inline">\(lmul = 2^k\)</span> where
-3 ≤ k ≤ 3 (i.e., lmul ∈ {1/8, 1/4, 1/2, 1, 2, 4, 8}) - 寄存器分组
Groups registers to form “longer vector” - Number of registers in each
group is LMUL - 举例：当 LMUL=2时，<code>vadd v2, v4, v6</code> 意味着
<code>(v2,v3) := (v4,v5) + (v6,v7)</code> - vediv: vector element
divider (EDIV=1,2,4,8)</p>
<p>Encoding only occupies 1.5 major opcodes</p>
<p>Full 64-bit instruction encoding also planned - Can view current
32-bit encoding as compressed form of full encoding</p>
<h3
id="寄存器vl-vector-length-not-to-be-confused-with-vlen">寄存器<strong>vl:
Vector Length</strong> (not to be confused with VLEN!)</h3>
<p>vl describes <code>how many elements of the vector</code> (starting
from the element zero) <code>we are going to operate</code></p>
<ul>
<li><p>$ 0 ≤ vl ≤ vlmax(sew, lmul) $</p></li>
<li><p>$ vlmax(sew, lmul) = (VLEN / sew) × lmul $</p></li>
</ul>
<p>Vector length <span class="math inline">\(vl\)</span> set to min(AVL,
VLMAX)</p>
<h2 id="rvv-defines-32-vector-registers-of-size-vlen-bits">RVV defines
32 vector registers of size VLEN bits</h2>
<ul>
<li>v0 to v31</li>
<li>VLEN is a <code>constant parameter</code> chosen by the implementor
and must be a power of two
<ul>
<li>Zv* standard extensions constraint VLEN to be at least 64 or
128</li>
</ul></li>
<li>E.g., VLEN=512 would be equivalent in size to Intel AVX-512</li>
<li>VLEN is not a great name so read it as
“<strong><code>vector register size (in bits)</code></strong>”</li>
</ul>
<h2 id="vectors-in-rvv-are-divided-in-elements.">Vectors in RVV are
divided in elements.</h2>
<ul>
<li>The size of elements in bits is at least 8 bits up to ELEN bits</li>
<li>ELEN is a constant parameter chosen by the implementor</li>
<li>Must be a power of two and 8 ≤ ELEN ≤ VLEN
<ul>
<li>Zv* standard extensions constrain ELEN to be at least 32 or 64</li>
</ul></li>
</ul>
<h2 id="vector-length-control">Vector Length control</h2>
<p>Current maximum vector length is register length in bits divided by
current element width setting:</p>
<p><span class="math display">\[VLMAX = VLEN/SEW\]</span></p>
<ul>
<li>E.g., <code>VLEN = 512b, SEW=32b</code> =&gt;
<code>VLMAX = 16</code></li>
</ul>
<p>Current active vector length set by vl register</p>
<p><span class="math display">\[0&lt;=vl&lt;=VLMAX\]</span></p>
<h2 id="vsetvlivsetvl-instructions">vsetvli/vsetvl instructions</h2>
<p><span class="math inline">\(vsetvli\)</span> instruction sets both
vector configuration and vector length:</p>
<p><code>vsetvli rd，rs1，vtypei(e8/e16/e32/e64)，multi-num(m1-m8)</code></p>
<p>rd - Returns setting of vector length in scalar register -
返回值，告诉硬件接下去运行函数所需要的操作元素个数的量</p>
<p>rs1 - Application vector length (AVL) - 设置操作元素</p>
<p>vtypei - 每个元素的位宽大小</p>
<p>multi-num - 寄存器的连用数量</p>
<hr />
<p>在 rv64 的编写中，如果 rd
的返回值不容，会导致硬件部分中心进行设置，此步骤极大的占用运行时间，所以<code>在编写的时候需要尽量可能的减少 rd 数值的变换</code>。</p>
<p>rv64 中每个 vector 寄存器总位宽是大于等于 128-bit,
以128为例子，对于如何对 rd
进行数值的变换，公式方面有一个很简单的换算：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rd == rs1 &lt;= (vtypei * multi-num ) ? rs1 : (vtypei * multi-num )</span><br><span class="line">举个例子：</span><br><span class="line"></span><br><span class="line">vsetvli rd, rs1 == 8， vtypei == e8， multi-num == m1</span><br><span class="line">返回值便是 rd == 8 (128 / e8 == 16 &gt;= rs1 =&gt; 8)</span><br><span class="line"></span><br><span class="line">vsetvli rd, rs1 == 32，vtypei == e16， multi-num == m1</span><br><span class="line">返回值便是 rd == 16 (128 / e16 = 16)</span><br></pre></td></tr></table></figure>
<p>如果一段代码中同时存在上述两条指令，那么便会出现硬件重设置情况，对代码整体的性能会产生负面影响。</p>
<hr />
<h2 id="simple-memcpy-example">Simple memcpy example</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># void*memcpy(void*dest,const void*src, size_t n)</span><br><span class="line"># a0=dest, a1=src, a2=n</span><br><span class="line">memcpy:</span><br><span class="line">mv a3, a0 # Copy destination</span><br><span class="line">loop:</span><br><span class="line">vsetvli t0, a2, e8 # Vectors of 8b</span><br><span class="line">vlb.v v0, (a1) # Load bytes</span><br><span class="line">add a1, a1, t0 # Bump pointer</span><br><span class="line">sub a2, a2, t0 # Decrement count</span><br><span class="line">vsb.v v0, (a3) # Store bytes</span><br><span class="line">add a3, a3, t0 # Bump pointer</span><br><span class="line">bnez a2, loop # Any more?</span><br><span class="line">ret # Return</span><br></pre></td></tr></table></figure>
<h2 id="usability-code">usability code</h2>
<p>通过intrinsic的方式，查询<code>vlenb</code>，进而可以兼容不同位宽的设备。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">csrr_vl</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;csrr %0, vl&quot;</span></span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;=r&quot;</span>(a)</span></span><br><span class="line"><span class="params">                 :</span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">csrr_vtype</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;csrr %0, vtype&quot;</span></span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;=r&quot;</span>(a)</span></span><br><span class="line"><span class="params">                 :</span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">int</span> <span class="title function_">csrr_vlenb</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span><span class="params">(<span class="string">&quot;csrr %0, vlenb&quot;</span></span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;=r&quot;</span>(a)</span></span><br><span class="line"><span class="params">                 :</span></span><br><span class="line"><span class="params">                 : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整理的很乱，暂时没时间仔细梳理了。</p>
<h2 id="参考文献">参考文献</h2>
<p><a href="https://gms.tf/riscv-vector.html">Programming with RISC-V
Vector Instructions</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/363901274">RISC-V
小白优化学习</a></p>
<p><a
href="https://riscv.org/wp-content/uploads/2019/06/17.40-Vector_RISCV-20190611-Vectors.pdf">Vector
Extension Update</a></p>
<p><a
href="https://llvm.org/devmtg/2019-04/slides/TechTalk-Kruppe-Espasa-RISC-V_Vectors_and_LLVM.pdf">Adventures
with RISC-V Vectors and LLVM</a></p>
]]></content>
      <tags>
        <tag>risc-v</tag>
        <tag>tech</tag>
      </tags>
  </entry>
  <entry>
    <title>FasterTransformer</title>
    <url>/2023/07/06/83_FasterTransformer/</url>
    <content><![CDATA[<p>记录一下部署大模型的经历</p>
<p>【更新中】</p>
<span id="more"></span>
<p><code>git clone https://github.com/NVIDIA/FasterTransformer.git</code></p>
<p>查看<code>docs/bert_guide.md</code></p>
<p>https://github.com/NVIDIA/nvidia-docker</p>
<p><a href="https://fanjunyu.com/posts/bb9b6d25/">NVIDIA Docker
的安装和使用</a> 好文！</p>
<p>期间需要注意这个问题，于是我最后一句改成了<code>docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi</code></p>
<pre><code>遇到问题
`permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get &quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version&quot;: dial unix /var/run/docker.sock: connect: permission denied`

解决方式：

docker守护进程启动的时候，会默认赋予名字为docker的用户组读写Unix socket的权限，因此只要创建docker用户组，并将当前用户加入到docker用户组中，那么当前用户就有权限访问Unix socket了，进而也就可以执行docker相关命令</code></pre>
<p>事实上，后来我跟着<a
href="https://blog.csdn.net/biejieyu1016/article/details/120874915">这篇教程</a>做的</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker #添加docker用户组</span><br><span class="line">sudo gpasswd -a $USER docker  #将登陆用户加入到docker用户组中</span><br><span class="line">newgrp docker #更新用户组</span><br><span class="line">docker images    #测试docker命令是否可以使用sudo正常使用</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version</span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           24.0.3</span><br><span class="line"> API version:       1.43</span><br><span class="line"> Go version:        go1.20.5</span><br><span class="line"> Git commit:        3713ee1</span><br><span class="line"> Built:             Wed Jul  5 20:44:55 2023</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Context:           default</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          24.0.3</span><br><span class="line">  API version:      1.43 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.20.5</span><br><span class="line">  Git commit:       1d9c861</span><br><span class="line">  Built:            Wed Jul  5 20:44:55 2023</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.6.21</span><br><span class="line">  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.1.7</span><br><span class="line">  GitCommit:        v1.1.7-0-g860f061</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.19.0</span><br><span class="line">  GitCommit:        de40ad0</span><br></pre></td></tr></table></figure>
<p>查看<code>nvidia-smi</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Fri Jul  7 14:04:28 2023       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Quadro P620                    Off | 00000000:01:00.0  On |                  N/A |</span><br><span class="line">| 34%   41C    P8              N/A /  N/A |    191MiB /  2048MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|    0   N/A  N/A      1833      G   /usr/lib/xorg/Xorg                           72MiB |</span><br><span class="line">|    0   N/A  N/A      1949    C+G   ...libexec/gnome-remote-desktop-daemon       38MiB |</span><br><span class="line">|    0   N/A  N/A      1990      G   /usr/bin/gnome-shell                         73MiB |</span><br><span class="line">|    0   N/A  N/A      4784      G   gnome-control-center                          1MiB |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p><a
href="https://hub.docker.com/r/nvidia/cuda/tags?page=1&amp;name=22.04">查找nvidia/cuda镜像</a></p>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>notes</tag>
        <tag>tech</tag>
      </tags>
  </entry>
  <entry>
    <title>踏上ChatGPT旅程</title>
    <url>/2023/05/26/85_ChatGPT/</url>
    <content><![CDATA[<p>开始使用并学习ChatGPT以及相关的<code>LLM(Large Language Model)</code>，不然就被时代淘汰了～</p>
<span id="more"></span>
<h1 id="写在前面">0.写在前面</h1>
<p>从去年11月底ChatGPT发布，到后来爆火，改变互联网，这半年时间我对ChatGPT还是比较冷漠的。</p>
<p>期间我也体验了一下，以及体验了<code>cursor</code>（一个使用chatgpt-4的ai辅助编程工具），但自始至终还是充满了不信任感。所以也一直没注册账号，没想进一步探索。</p>
<p>直到今天，我改变了对它看法，或者说我改变了看它的角度。</p>
<p>以前潜意识里是觉得ChatGPT是智囊、军师、管家之类的身份，觉得人的主观能动性受到限制，所以会抵制。</p>
<p>现在我想把它当工具，来便利我的工作和生活，便没有了心理阻碍。</p>
<p>于是，ChatGPT旅程，出发～</p>
<h1 id="注册chatgpt">1. 注册ChatGPT</h1>
<p>我参考了这篇文章<a
href="https://www.xnbeast.com/create-openai-chatgpt-account/">ChatGPT注册教程攻略：解决手机无法接收验证码问题（亲测有效）</a>
，在<a
href="https://sms-activate.org/">sms-activate</a>购买了临时国外手机号——</p>
<ul>
<li><p>买了个英国的手机号，结果</p>
<p><code>Your account was flagged for potential abuse. If you feel this is an error, please contact us at help.openai.com</code></p></li>
<li><p>换了个美国的也不行</p></li>
<li><p>最后换了个印度尼西亚的手机号，顺利注册，但是进入ChatGPT又显示</p>
<p><code>Sorry, you have been blocked</code></p></li>
</ul>
<p>出师不利啊……突然想起来我买的VPN的维护者说过，这个vpn不搞chatGPT那破玩意儿。于是在tg上找了个<code>@chatgptzs_bot</code>，暂时先用着，也挺好用的。</p>
<p>但是还不甘心，之后我用<code>自由鲸</code>的欧洲节点，开启全局模式，能正常使用ChatGPT了。不过目前不是付费用户，所以没有联网功能和插件功能。</p>
<h1 id="chatgpt使用技巧">2. ChatGPT使用技巧</h1>
<p>昨晚在B站看到一个视频挺好的——</p>
<p><a
href="https://b23.tv/XCclvE1">学完这个视频，简历加一条：熟练掌握ChatGPT解决复杂问题｜ChatGPT使用教程-哔哩哔哩</a></p>
<p>说实话，我也是有一部分被这个视频驱动来搞ChatGPT的。</p>
<h2 id="乔哈里johari沟通视窗">乔哈里(Johari)沟通视窗</h2>
<table>
<thead>
<tr class="header">
<th></th>
<th>你知道</th>
<th>你不知道</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT知道</td>
<td>公开区</td>
<td>盲区</td>
</tr>
<tr class="even">
<td>GPT不知道</td>
<td>隐私区</td>
<td>未知区</td>
</tr>
</tbody>
</table>
<ul>
<li><p>盲区｜问法1:
<code>我想了解xxx，我应该向你问哪些问题？</code></p></li>
<li><p>盲区｜问法2:<code>请给我列出xxx领域/行业相关的，最常用的50个概念，并做简单解释。如果有英文缩写，请给出完整的英文解释。</code></p></li>
<li><p>盲区｜问法3:<code>请详细介绍一下xxx的主要生平事迹。</code>
<code>请详细介绍一下xxx公司的发展历程。</code></p></li>
<li><p>公开区｜检验认知:<code>对于xxx主题/技能，你认为哪些是我必须理解和掌握的核心要点？</code></p></li>
<li><p>公开区｜检验认知:<code>我理解的xxx是这样的，你觉得我的理解对吗？</code></p></li>
<li><p>公开区｜检验认知:<code>我对xxx有一些想法，你能帮我批判性地分析一下这些想法的优点和缺点吗？</code></p></li>
<li><p>公开区｜检验认知:<code>我正在考虑xxx的决定，你能帮我分析一下可能的结果和影响吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我知道xxx的概念，我想知道更多关于xxx的信息。</code></p></li>
<li><p>公开区｜扩充认知:<code>我在xxx问题上遇到困难，你能提供一些可能的解决方案或建议吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我想要深入学习xxx，你能推荐一些进阶的学习资源或学习路径吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我想要在xxx领域有所创新，你能提供一些启发或想法吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我想在xxx领域提升自己，你能根据最新的研究和趋势给我一些建议吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>我正在考虑学习xxx，你能给我一些关于这个领域未来发展的观点吗？</code></p></li>
<li><p>公开区｜扩充认知:<code>（背景信息）我要做关于xxx的研究，我认为原因是，还有其他可能的原因吗？给出一些可能的研究假设。</code></p></li>
<li><p>公开区｜扩充认知:<code>我是一个xx行业新手，马上要采访这个行业的资深大佬，我应该向他请教哪些有价值的问题？</code></p></li>
<li><p>盲区｜问法1: <code>你怎么看待这种现象？</code></p></li>
<li><p>盲区｜问法2: <code>可能的原因有哪些？</code></p></li>
<li><p>盲区｜问法3: <code>这会对xxx产生什么样的影响？</code></p></li>
<li><p>盲区｜问法4: <code>你觉得xxx应该怎么做？</code></p></li>
<li><p>未知区｜问法1:
<code>如果xxx，这对社会会产生什么影响？</code></p></li>
<li><p>未知区｜反向提问:
<code>为了测试我对xxx的了解程度，你会问我什么问题来检验我的水平，最少10个</code></p></li>
<li><p>未知区｜反向提问:
<code>我是xx领域的专家，你会问我哪些问题来检验我的专业水平？最少10个。</code></p></li>
<li><p>未知区｜反向提问:
<code>现在我们玩个“你问我答”的游戏，目的是为了测试我在xxxx方面的专业水平。你负责提问，我负责回答。你要根据我的答案进行反馈、评价、补充。如果我说不知道，你就直接输出正确答案，然后你继续提问。</code></p></li>
<li><p>未知区｜反向提问:
<code>我已经很精通xxx了，我想知道我是否还有需要学习的地方？</code></p></li>
</ul>
<p>1 知道自己要干什么，但是不知道ChatGPT能帮你什么。 -
<code>我要做xxx，你能帮我做什么？</code></p>
<p>2 知道要ChatGPT帮你做什么，但是你不知道给它输入什么具体的信息。 -
<code>我想要你xxx，我应该给你输入什么信息？</code></p>
<p>3 直接给ChatGPT指令。任务目标、背景信息、输出要求都非常清晰。 -
<code>请推荐（书名、电视剧、电影、网站、博主、专家、学习资源） 等等。请翻译.请总结.请润色。</code></p>
<h1 id="llm学习">3. LLM学习</h1>
<p>我：给我推荐一些学习大规模语言模型的资料吧</p>
<blockquote>
<p>当然！以下是一些学习大规模语言模型的资料推荐： 1.
论文和研究文章：深入了解大规模语言模型的原理和技术，可以阅读相关的论文和研究文章，例如《Transformers:
Attention Is All You Need》、《BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding》和《GPT (Generative Pre-trained
Transformer)》。 2.
博客文章和教程：有许多博客文章和教程可供学习，这些文章会解释大规模语言模型的概念、实现和应用。可以查找像Towards
Data Science、Medium和OpenAI Blog等平台上的相关文章。 3. GitHub
上的代码库：学习大规模语言模型的实现和代码细节，可以查看GitHub上的开源代码库，如Hugging
Face的transformers库、OpenAI的GPT代码库等。 4.
视频教程和在线课程：有一些在线课程和视频教程专门介绍大规模语言模型。例如，Coursera
上的 "Natural Language Processing with Sequence Models" 和 YouTube 上的
"The Transformer Explained" 等。 5.
社区和论坛：加入与自然语言处理和大规模语言模型相关的社区和论坛，与其他人交流讨论，获取实践经验和最新动态。</p>
<p>这些资源可以帮助你深入了解大规模语言模型的背后原理、应用和实现。希望对你的学习有所帮助！如需更具体的资源或问题，请随时告诉我。</p>
</blockquote>
<h2 id="学习视频">学习视频</h2>
<p>Google Cloud Tech</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=zizonToFXDs">Introduction
to Large Language Models</a></li>
</ul>
<p>ZOMI</p>
<ul>
<li><p><a
href="https://www.youtube.com/watch?v=XujuQB9AxGA">ChatGPT狂飙原理剖析：GPT系列详解！【ChatGPT】系列第01篇</a></p></li>
<li><p><a
href="https://www.youtube.com/watch?v=qf2zcev7mHQ">ChatGPT狂飙：强化学习RLHF与PPO！【ChatGPT】系列第02篇</a></p></li>
<li><p><a
href="https://www.youtube.com/watch?v=Oy-8BqwXck8&amp;t=1s">ChatGPT狂飙：InstructGPT解析！【ChatGPT】原理第03篇</a></p></li>
<li><p>【ChatGPT狂飙：InstructGPT解析！】思考：</p>
<ul>
<li>大模型通过计算的方式模拟人类的思考，类似于ChatGPT的RLHF技术是否会给世界带来新的技术产业革命？</li>
<li>ChatGPT 使用了 Ray 作为细粒度的并行计算和昇构计算，管理分配
RLIDL模型完成复杂训练任务，方便利用强化学习对环境和计算任务进行控制，这对A框架的分布式能力边界带来哪些新的冲击？</li>
<li>ChatGPT
非常重数据交互，存算一体技术会不会针对ChatGPT等应用的现专用芯片和新的架构?</li>
</ul></li>
</ul>
<p><a
href="https://imzhanghao.com/2023/02/24/chatgpt/">ChatGPT模型的三层理解</a></p>
<p><a
href="https://github.com/dalinvip/Awesome-ChatGPT">Awesome-ChatGPT</a></p>
<p>2023.6.1 学习<a
href="https://www.youtube.com/watch?v=nzqlFIcCSWQ">Transformer论文逐段精读</a></p>
]]></content>
      <tags>
        <tag>LLM</tag>
        <tag>tech</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>Using Ubuntu from scratch</title>
    <url>/2023/06/30/87_Using-Ubuntu-from-scratch/</url>
    <content><![CDATA[<p>原来的Ubuntu系统的用户权限搞崩了，从头开始搞一个～</p>
<span id="more"></span>
<h1 id="系统安装">1.系统安装</h1>
<h2 id="下载">1.1 下载</h2>
<p><a href="http://mirrors.aliyun.com/ubuntu-releases/20.04/">Ubuntu
20.4镜像</a></p>
<h2 id="把iso镜像文件转换成dmg文件">1.2 把iso镜像文件转换成dmg文件</h2>
<p><code>cd Downloads</code></p>
<p><code>hdiutil convert -format UDRW -o ubuntu.iso ubuntu-20.04.6-desktop-amd64.iso</code></p>
<ul>
<li><p><code>-format</code>为生成文件的权限；</p></li>
<li><p><code>UDRW</code> :表示转换成有read/write的权限的镜像。</p></li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Reading Driver Descriptor Map (DDM : 0)…</span><br><span class="line">Reading Ubuntu 20.04.6 LTS amd64         (Apple_ISO : 1)…</span><br><span class="line">Reading Apple (Apple_partition_map : 2)…</span><br><span class="line">Reading Ubuntu 20.04.6 LTS amd64         (Apple_ISO : 3)…</span><br><span class="line">.....................................................</span><br><span class="line">Reading EFI (Apple_HFS : 4)…</span><br><span class="line">.....................................................</span><br><span class="line">Reading Ubuntu 20.04.6 LTS amd64         (Apple_ISO : 5)…</span><br><span class="line">...............................................................................</span><br><span class="line">Elapsed Time:  4.971s</span><br><span class="line">Speed: 834.8MB/s</span><br><span class="line">Savings: 0.0%</span><br><span class="line">created: /Users/zhaodongyu/Downloads/ubuntu.iso.dmg</span><br></pre></td></tr></table></figure>
<h2 id="烧录至优盘">1.3 烧录至优盘</h2>
<p>注意找到U盘的路径，使用<code>diskutil list</code>命令查询。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/dev/disk4 (external, physical):</span><br><span class="line"><span class="meta prompt_">   #</span><span class="language-bash">:                       TYPE NAME                    SIZE       IDENTIFIER</span></span><br><span class="line">   0:     FDisk_partition_scheme                        *16.0 GB    disk4</span><br><span class="line">   1:             Windows_FAT_32 NO NAME                 16.0 GB    disk4s1</span><br></pre></td></tr></table></figure>
<p>找到U盘的路径为/dev/disk4，并使用命令行卸载U盘：</p>
<p><code>diskutil unmountDisk /dev/disk4</code></p>
<p>显示信息：Unmount of all volumes on disk4 was successful</p>
<p>把iso文件写入U盘 <code>mv ubuntu.iso.dmg ubuntu.iso</code></p>
<p><code>sudo dd if=./ubuntu.iso of=/dev/disk4 bs=1m</code></p>
<p>等了几分钟后，显示 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">4149+1 records in</span><br><span class="line">4149+1 records out</span><br><span class="line">4351463424 bytes transferred in 369.833082 secs (11766020 bytes/sec)</span><br></pre></td></tr></table></figure></p>
<p>弹出优盘</p>
<p><code>sudo eject /dev/disk4</code></p>
<h1 id="系统设置">2.系统设置</h1>
<h2 id="开启ssh连接">2.1 开启SSH连接</h2>
<p><a
href="https://blog.csdn.net/jackghq/article/details/54974141">ubuntu开启SSH服务远程登录</a></p>
<p><code>sudo apt-get install openssh-server</code></p>
<p>可以ssh连接了</p>
<h2 id="更新vim">2.2 更新vim</h2>
<ul>
<li><p>卸载旧版的vi：sudo apt-get remove vim-common</p></li>
<li><p>安装新版的vim：sudo apt-get install vim</p></li>
</ul>
<h2 id="挂载硬盘">2.3 挂载硬盘</h2>
<p>查看磁盘信息命令: <code>sudo fdisk -l</code></p>
<p>其中有： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Disk /dev/sdb: 931.51 GiB, 1000204886016 bytes, 1953525168 sectors</span><br><span class="line">Disk model: ST1000DM003-1SB1</span><br><span class="line">Units: sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 4096 bytes</span><br><span class="line">I/O size (minimum/optimal): 4096 bytes / 4096 bytes</span><br></pre></td></tr></table></figure></p>
<p>查看硬盘的UUID命令: <code>sudo blkid</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/dev/sdb: UUID=&quot;2b4d86ff-3fb8-40c4-b5ef-dd840f65f94f&quot; BLOCK_SIZE=&quot;4096&quot; TYPE=&quot;ext4&quot;</span><br></pre></td></tr></table></figure>
<p>mkdir创建挂载点data</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zhaodongyu@portal:/$ sudo mkdir data</span><br><span class="line">zhaodongyu@portal:/$ ls</span><br><span class="line">bin   cdrom  dev  home  lib32  libx32      media  opt   root  sbin  srv       sys  usr</span><br><span class="line">boot  data   etc  lib   lib64  lost+found  mnt    proc  run   snap  swapfile  tmp  var</span><br></pre></td></tr></table></figure>
<p>手动挂载</p>
<p><code>sudo mount /dev/sdb /data</code></p>
<p>永久性挂载分区，修改分区文件，输入命令<code>sudo vi /etc/fstab</code>，增加
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UUID=2b4d86ff-3fb8-40c4-b5ef-dd840f65f94f /data           ext4    defaults          0       0</span><br></pre></td></tr></table></figure></p>
<p>查看挂载好的硬盘信息 <code>df -hT</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/dev/sdb       ext4   916G  207G  663G  24% /data</span><br></pre></td></tr></table></figure>
<h2 id="安装-vs-code">2.4 安装 VS code</h2>
<p>打开 Ubuntu
软件中心，并且搜索<code>Visual Studio Code</code>,然后安装应用</p>
<h2 id="安装make">2.5 安装make</h2>
<p><code>sudo apt install make</code></p>
<h2 id="cmake升级">2.5 cmake升级</h2>
<p><a
href="https://blog.csdn.net/Boys_Wu/article/details/104940575">参考</a></p>
<p>我在<a
href="https://cmake.org/download/">cmake官网</a>下载了<code>cmake-3.27.0-rc4-linux-x86_64.tar.gz</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf cmake-3.27.0-rc4-linux-x86_64.tar.gz</span><br><span class="line"></span><br><span class="line">sudo mv cmake-3.27.0-rc4-linux-x86_64 /opt/cmake-3.27.0</span><br><span class="line"></span><br><span class="line">sudo ln -sf /opt/cmake-3.27.0/bin/* /usr/bin/</span><br><span class="line"></span><br><span class="line">cmake --version</span><br><span class="line">    cmake version 3.27.0-rc4</span><br><span class="line"></span><br><span class="line">sudo vim ~/.bashrc </span><br><span class="line">    export  PATH=$PATH:/opt/cmake-3.27.0/bin</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="gcc降级">2.6 gcc降级</h2>
<p>原来的gcc版本是9.4</p>
<p><code>sudo apt-get install gcc-9</code></p>
<p><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100</code></p>
<p>sudo apt-get install g++-9</p>
<p>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9
100</p>
]]></content>
      <tags>
        <tag>notes</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Crawler</title>
    <url>/2020/05/19/89_Crawler/</url>
    <content><![CDATA[<p>背景：国内表情识别的数据集太少了 &amp;
表情数据集基本都是外国人脸，训出来的模型实际使用效果不好。</p>
<p>于是，在百度上爬了一批数据下来补充到数据集里面。</p>
<p>[爬虫有风险，操作需谨慎]</p>
<span id="more"></span>
<h1 id="写在前面的话">写在前面的话</h1>
<p>一共两个文件</p>
<ul>
<li><p>crawler.py</p>
<p>主程序：实现爬虫功能，爬取的关键词在keywords.txt里面</p></li>
<li><p>keywords.txt</p>
<p>关键词：比如爬开心的图，就可以设置 happy 高兴 高兴表情 高兴的人
高兴脸 开心 开心脸 开心的人 开心表情 欢愉 愉悦 愉悦表情 欢愉表情
幸福表情 幸福的脸 幸福的人 愉悦表情 欢欣表情 欢喜表情 喜悦</p>
<p><strong>每个关键词用换行分隔</strong></p></li>
</ul>
<p>运行python文件后，需要等待一会（正在获取图片的<code>url</code>），然后就显示下载log了。</p>
<p>结果会保存到<code>image</code>文件夹内，后续需要进一步批处理文件以及进行人脸提取、对齐、清洗等工作。</p>
<p>清洗数据集还是相当繁琐的，建议不要浪费时间在数据清洗上，自己想办法找途径处理。</p>
<pre><code>[爬虫有风险，操作需谨慎]

希望你能把这个代码用在正途，造福社会:)

希望你能开源数据集，减少社会整体的成本:)</code></pre>
<h1 id="python代码">python代码</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">download_path = <span class="string">&#x27;./image/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_onepage_urls</span>(<span class="params">onepageurl</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取单个翻页的所有图片的urls+当前翻页的下一翻页的url&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> onepageurl:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;已到最后一页, 结束&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> [], <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        html = requests.get(onepageurl)</span><br><span class="line">        html.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">        html = html.text</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line">        pic_urls = []</span><br><span class="line">        fanye_url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> pic_urls, fanye_url</span><br><span class="line">    pic_urls = re.findall(<span class="string">&#x27;&quot;objURL&quot;:&quot;(.*?)&quot;,&#x27;</span>, html, re.S)</span><br><span class="line">    fanye_urls = re.findall(re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;a href=&quot;(.*)&quot; class=&quot;n&quot;&gt;下一页&lt;/a&gt;&#x27;</span>), html, flags=<span class="number">0</span>)</span><br><span class="line">    fanye_url = <span class="string">&#x27;http://image.baidu.com&#x27;</span> + fanye_urls[<span class="number">0</span>] <span class="keyword">if</span> fanye_urls <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> pic_urls, fanye_url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_pic</span>(<span class="params">pic_urls,label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;给出图片链接列表, 下载所有图片&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#for i, pic_url in tqdm(enumerate(pic_urls)):</span></span><br><span class="line">    <span class="keyword">for</span> i,pic_url <span class="keyword">in</span> <span class="built_in">enumerate</span>(pic_urls):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            pic = requests.get(pic_url, timeout=<span class="number">15</span>)</span><br><span class="line">            string = download_path + label + <span class="string">&#x27;_baidu/&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(string, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(pic.content)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;成功下载第(%s/%s)张图片: %s&#x27;</span> % (<span class="built_in">str</span>(i + <span class="number">1</span>), <span class="built_in">str</span>(<span class="built_in">len</span>(pic_urls)), <span class="built_in">str</span>(pic_url)))</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;下载第%s张图片时失败: %s&#x27;</span> % (<span class="built_in">str</span>(i + <span class="number">1</span>), <span class="built_in">str</span>(pic_url)))</span><br><span class="line">            <span class="comment">#print(e)</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;keywords.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.read().splitlines()</span><br><span class="line">        <span class="built_in">print</span>(lines)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(lines):</span><br><span class="line">        file_path = os.path.join(download_path, line)</span><br><span class="line">        file_path = file_path + <span class="string">&#x27;_baidu&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">            os.makedirs(file_path, <span class="number">0o755</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shutil.rmtree(file_path)</span><br><span class="line">            os.makedirs(file_path, <span class="number">0o755</span>)</span><br><span class="line"></span><br><span class="line">        url_init_first = <span class="string">r&#x27;http://image.baidu.com/search/flip?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fm=result&amp;fr=&amp;sf=1&amp;fmq=1497491098685_R&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;ctd=1497491098685%5E00_1519X735&amp;word=&#x27;</span></span><br><span class="line">        url_init = url_init_first + urllib.parse.quote(line, safe=<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">        all_pic_urls = []</span><br><span class="line">        onepage_urls, fanye_url = get_onepage_urls(url_init)</span><br><span class="line">        all_pic_urls.extend(onepage_urls)</span><br><span class="line"></span><br><span class="line">        fanye_count = <span class="number">0</span>  <span class="comment"># 累计翻页数</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            onepage_urls, fanye_url = get_onepage_urls(fanye_url)</span><br><span class="line">            fanye_count += <span class="number">1</span></span><br><span class="line">            <span class="comment">#print(&#x27;第页&#x27; % str(fanye_count))</span></span><br><span class="line">            <span class="keyword">if</span> fanye_url == <span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> onepage_urls == []:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            all_pic_urls.extend(onepage_urls)</span><br><span class="line"></span><br><span class="line">        down_pic(<span class="built_in">list</span>(<span class="built_in">set</span>(all_pic_urls)),line)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="附keywords.txt">附<code>keywords.txt</code></h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">鄙视脸</span><br><span class="line">鄙视表情</span><br><span class="line">嫌恶</span><br><span class="line">恶心表情</span><br><span class="line">腻烦表情</span><br><span class="line">厌烦表情</span><br><span class="line">憎恶表情</span><br><span class="line">反感表情</span><br><span class="line">不耐烦</span><br><span class="line">不耐烦表情</span><br><span class="line">嫌弃表情</span><br><span class="line">fear</span><br><span class="line">恐惧</span><br><span class="line">恐惧表情</span><br><span class="line">恐惧的人</span><br><span class="line">恐惧脸</span><br><span class="line">害怕</span><br><span class="line">害怕表情</span><br><span class="line">害怕脸</span><br><span class="line">恐慌</span><br><span class="line">恐慌表情</span><br><span class="line">恐慌脸</span><br><span class="line">惊吓</span><br><span class="line">惊怖表情</span><br><span class="line">可骇表情</span><br><span class="line">怯生生表情</span><br><span class="line">胆怯表情</span><br><span class="line">胆怯脸</span><br><span class="line">恐慌的人</span><br><span class="line">惧怕表情</span><br><span class="line">惧怕的人</span><br><span class="line">happy</span><br><span class="line">高兴</span><br><span class="line">高兴表情</span><br><span class="line">高兴的人</span><br><span class="line">高兴脸</span><br><span class="line">开心</span><br><span class="line">开心脸</span><br><span class="line">开心的人</span><br><span class="line">开心表情</span><br><span class="line">欢愉</span><br><span class="line">愉悦</span><br><span class="line">愉悦表情</span><br><span class="line">欢愉表情</span><br><span class="line">幸福表情</span><br><span class="line">幸福的脸</span><br><span class="line">幸福的人</span><br><span class="line">愉悦表情</span><br><span class="line">欢欣表情</span><br><span class="line">欢喜表情</span><br><span class="line">喜悦</span><br><span class="line">sad</span><br><span class="line">悲伤</span><br><span class="line">悲伤表情</span><br><span class="line">悲伤的人</span><br><span class="line">悲伤脸</span><br><span class="line">伤心</span><br><span class="line">伤心表情</span><br><span class="line">难过</span><br><span class="line">难过表情</span><br><span class="line">难过的人</span><br><span class="line">伤心的人</span><br><span class="line">伤感表情</span><br><span class="line">悲痛表情</span><br><span class="line">哀伤</span><br><span class="line">心酸表情</span><br><span class="line">悲戚表情</span><br><span class="line">哀痛的人</span><br><span class="line">悲痛的人</span><br><span class="line">哀伤脸</span><br><span class="line">心酸脸</span><br><span class="line">surprise</span><br><span class="line">惊讶</span><br><span class="line">惊讶表情</span><br><span class="line">惊讶的人</span><br><span class="line">惊讶脸</span><br><span class="line">诧异</span><br><span class="line">惊奇</span><br><span class="line">惊奇表情</span><br><span class="line">惊奇脸</span><br><span class="line">诧异脸</span><br><span class="line">诧异表情</span><br><span class="line">吃惊</span><br><span class="line">吃惊表情</span><br><span class="line">吃惊脸</span><br><span class="line">吃惊的人</span><br><span class="line">惊诧</span><br><span class="line">惊愕</span><br><span class="line">惊诧表情</span><br><span class="line">惊叫表情</span><br><span class="line">惊叫脸</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tech</tag>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title>tflite-micro</title>
    <url>/2023/10/27/91_tflite-micro/</url>
    <content><![CDATA[<p>最近又搞了一次tflite-micro，上一次搞还是在两年前，那时候简直是个工程小白，遇到个问题就卡住了。这一次相对比较顺利，记录一遍过程。</p>
<span id="more"></span>
<p>论文：<a href="https://arxiv.org/abs/2010.08678">TensorFlow Lite
Micro: Embedded Machine Learning on TinyML Systems</a></p>
<h1 id="克隆仓库">克隆仓库</h1>
<p>在tensorflow的<a
href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro">git里面</a>，看到git已经迁移至一个单独的<a
href="https://github.com/tensorflow/tflite-micro">仓库</a>。</p>
<ul>
<li>As of June 25, 2021, the TFLM codebase has moved to a <a
href="https://github.com/tensorflow/tflite-micro">stand-alone github
repository</a>.</li>
</ul>
<p>于是在ubuntu执行命令：</p>
<p><code>git clone https://github.com/tensorflow/tflite-micro.git</code></p>
<p>此时<code>commit 7ac0b662b6402c1fccc8ec8d1db84f0f34dd4a85</code></p>
<h1 id="编译静态库">编译静态库</h1>
<p>求助了ChatGPT 4.0：</p>
<p>「在这个 https://github.com/tensorflow/tflite-micro ，
怎么编译静态库？」</p>
<p>在TensorFlow Lite
Micro的GitHub仓库中，你可以使用以下步骤来编译静态库：</p>
<ol type="1">
<li>首先，你需要克隆仓库：</li>
</ol>
<p><code>git clone https://github.com/tensorflow/tflite-micro.git</code></p>
<ol start="2" type="1">
<li>进入克隆下来的仓库目录：</li>
</ol>
<p><code>cd tflite-micro</code></p>
<ol start="3" type="1">
<li>使用make命令编译静态库：</li>
</ol>
<p><code>make -f tensorflow/lite/micro/tools/make/Makefile TARGET=&lt;TARGET&gt; TARGET_ARCH=&lt;ARCH&gt; microlite</code></p>
<p>在上述命令中，<code>&lt;TARGET&gt;</code>应替换为你的目标平台（如<code>linux</code>），<code>&lt;ARCH&gt;</code>应替换为你的目标架构（如<code>x86_64</code>）。这将在<code>tensorflow/lite/micro/tools/make/gen/&lt;TARGET&gt;_&lt;ARCH&gt;/lib/</code>目录下生成一个名为<code>libtensorflow-microlite.a</code>的静态库。</p>
<p>请注意，你可能需要根据你的具体需求和环境来修改这些步骤。例如，你可能需要安装额外的依赖库，或者修改makefile来包含你需要的特性。</p>
<p>真是太棒了，于是我执行指令：</p>
<p><code>make -f tensorflow/lite/micro/tools/make/Makefile TARGET=linux TARGET_ARCH=x86_64 microlite</code></p>
<p>在 <code>make/download/</code>
目录下，分别下载了<code>flatbuffers</code>、<code>gemmlowp</code>、<code>kissfft</code>、<code>pigweed</code>、<code>ruy</code>。</p>
<p>编译完成后，得到了静态库文件</p>
<p><code>ar: creating gen/linux_x86_64_default/lib/libtensorflow-microlite.a</code></p>
<h1 id="编写-demo-debug">编写 demo &amp; debug</h1>
<p>仿照helloworld工程写了一个小demo</p>
<h2 id="遇到问题">遇到问题：</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Failed to allocate tail memory. Requested: 2760, available 1376, missing: 1384</span><br><span class="line">Failed to allocate memory for context-&gt;eval_tensors, 2760 bytes required</span><br><span class="line">Failed starting model allocation.</span><br></pre></td></tr></table></figure>
<h2 id="解决方法">解决方法：</h2>
<p>先暂时增大ArenaSize，后面再根据实际使用情况调整回来：</p>
<p><code>constexpr int kTensorArenaSize = 300000;</code></p>
<h2 id="遇到问题-1">遇到问题：</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Didn&#x27;t find op for builtin opcode &#x27;LEAKY_RELU&#x27;</span><br><span class="line">Failed to get registration from op code LEAKY_RELU</span><br><span class="line"></span><br><span class="line">Segmentation fault (core dumped)</span><br></pre></td></tr></table></figure>
<h2 id="解决方法-1">解决方法：</h2>
<p>增加算子注册
<code>TF_LITE_ENSURE_STATUS(op_resolver.AddLeakyRelu());</code></p>
<h2 id="遇到问题-2">遇到问题</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Couldn&#x27;t register builtin op #98, resolver size </span><br><span class="line">is too small (1).</span><br></pre></td></tr></table></figure>
<h2 id="解决方法-2">解决方法</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">using HelloWorldOpResolver = tflite::MicroMutableOpResolver&lt;1&gt;;</span><br></pre></td></tr></table></figure>
<p>这里的1代表注册个数，草率了，应该<code>op_resolver</code> add
了几个这里就写几的。</p>
<h2 id="遇到问题-3">遇到问题</h2>
<p>一旦执行到<code>interpreter.input(0)-&gt;data.f[0] = 1.f;</code>就段错误。</p>
<h2 id="解决办法">解决办法</h2>
<p><code>CFLAGS = -DTF_LITE_STATIC_MEMORY</code></p>
<h1 id="进一步减小库体积">进一步减小库体积</h1>
<p>为了压缩体积，<code>BUILD_TYPE</code>使用了<code>release</code>进行编译，这期间会遇到<code>MicroPrintf</code>不支持的问题（release_with_logs是可以的），进行一些注释就可以。</p>
<p>以及进行<code>-Os</code>编译，可以减少很多体积占用。</p>
]]></content>
      <tags>
        <tag>notes</tag>
        <tag>tech</tag>
        <tag>tflite-micro</tag>
        <tag>TinyML</tag>
      </tags>
  </entry>
  <entry>
    <title>ADAS</title>
    <url>/2020/04/23/93_ADAS/</url>
    <content><![CDATA[<p>自动驾驶汽车可以实现<code>驾驶员驾驶</code>、<code>系统辅助驾驶员驾驶</code>以及<code>系统自动驾驶</code>。而无人驾驶汽车是不需要驾驶员的，即便在紧急情况时也能自行处理。</p>
<p>最近调研了一下ADAS，感觉是一个不错的方向。然而个人认为自动驾驶的<code>corner case</code>太多，未来充满挑战。</p>
<span id="more"></span>
<h2 id="自动驾驶汽车">自动驾驶汽车</h2>
<p><code>自动驾驶汽车(Autonomous vehicles; Self-piloting automobile)</code>是一种通过电脑系统实现无人驾驶的智能汽车。</p>
<p>自动驾驶汽车可以实现<code>驾驶员驾驶</code>、<code>系统辅助驾驶员驾驶</code>以及<code>系统自动驾驶</code>。而无人驾驶汽车是不需要驾驶员的，即便在紧急情况时也能自行处理。</p>
<p><img src="/images/adas/1.png" width="80%"></p>
<h2 id="自动驾驶的五个等级">自动驾驶的五个等级</h2>
<pre><code>NHTSA：美国国家交通管理局
SAE：美国汽车工程师学会</code></pre>
<p><img src="/images/adas/2.png" width="80%"></p>
<p>在车企们万众一心向L3级自动驾驶冲刺的关口，以自动驾驶为最终目标的<code>ADAS系统</code>自然而然的火了。</p>
<p>目前世界上大部分的汽车，都处于<code>L1</code>级别，只有少数汽车，比如<code>特斯拉</code>，已经可以处于<code>L2</code>级别，并在不断完善当中。</p>
<p><code>ADAS</code>实现的是<code>第3级</code>，而<code>自动驾驶</code>实现的是<code>第4级</code>。想要从第3级发展到第4级，还需要更多的汽车上配有自动驾驶技术，而且得配合道路基础设施建设（道路上的摄像头，清晰的车道线），以及需要汽车互联，汽车手机互联等等，是个非常庞大的工程.</p>
<h2 id="adas">ADAS</h2>
<p>高级驾驶辅助系统<code>ADAS(Advanced Driver Assistance System)</code></p>
<blockquote>
<p>利用安装在车上的各式各样传感器（毫米波雷达、激光雷达、单），在汽车行驶过程中随时来感应周围的环境，收集数据，进行静态、动态物体的辨识、侦测与追踪，并结合导航仪地图数据，进行系统的运算与分析，从而预先让驾驶者察觉到可能发生的危险，有效增加汽车驾驶的<code>舒适性</code>和<code>安全性</code>。</p>
</blockquote>
<ul>
<li><p>ADAS核心是<code>环境感知</code></p></li>
<li><p>自动驾驶是<code>人工智能</code></p></li>
<li><p>ADAS可以视作自动驾驶汽车的<code>前提</code></p></li>
</ul>
<p><strong>虽然ADAS被很多人视作自动驾驶汽车的前提，但就其本质而言，ADAS是辅助驾驶，核心是环境感知，而自动驾驶是人工智能，两者体系有很大差别。</strong></p>
<hr />
<p>ADAS是最接近于自动驾驶的智能辅助系统。但是目前生活中比较常用的是<code>ADAS防碰撞预警</code>+<code>DSM防疲劳驾驶检测系统</code>。</p>
<h2 id="adas分类">ADAS分类</h2>
<p><img src="/images/adas/3.png" width="90%"></p>
<h3 id="adas前车碰撞预警fcw">ADAS——前车碰撞预警FCW</h3>
<p><img src="/images/adas/4.png" width="30%"></p>
<p>由安装在车头的雷达，侦测自车和前方车辆的距离及速度，初期会发出警告声来提醒驾驶人注意车距，若车距依然持续拉近，车辆便会先自动轻踩刹车，并轻拉安全带2-3次，警告驾驶人。</p>
<p>若系统判定追撞是没办法避免，启动自动紧急刹车（AEB）后，会同时立刻拉紧安全带固定驾驶人，降低意外发生后的伤害。</p>
<h3 id="adas盲点侦测系统">ADAS——盲点侦测系统</h3>
<p><img src="/images/adas/5.png" width="30%"></p>
<p>汽车驾驶人的盲点是指三面后视镜，左、右、内看不到的区域</p>
<p>盲点侦测系统就是运用雷达和传感器，来侦测车辆后方的盲点区，在盲点区侦测到车辆靠近时会向驾驶员提供警示，帮助驾驶人将意外的机率降至最低。</p>
<h3 id="adas停车辅助系统">ADAS——停车辅助系统</h3>
<p><img src="/images/adas/6.png" width="30%"></p>
<p>停车辅助系统又分为 2种，分别是主动式与被动式 -
主动式系统自动控制方向盘以帮助驾驶完成停车，当然油门、刹车与档位切换还是要车主自行操控。
-
被动式是以影像（摄影机）与影音（超音波）为感测单元所组成的，提供更多车身周围信息给车主掌握，减少碰撞机会。</p>
<h3 id="adas车道偏离警示系统ldw">ADAS——车道偏离警示系统LDW</h3>
<p><img src="/images/adas/7.png" width="30%"></p>
<p>由摄影机、传感器及控制器所组成，原理是运用在车身侧面或后视镜的摄影机，采样目前行驶车道的标识线，再通过图像处理取得当前汽车在车道的位子，这时只要汽车偏离车道，控制器就会发出警报信号。</p>
<p>从感测到发出警报，过程只需约0.5
秒的时间，以实时提醒、叫醒驾驶，避免意外的发生。</p>
<h3 id="adas适应性车灯系统">ADAS——适应性车灯系统</h3>
<p><img src="/images/adas/8.png" width="30%"></p>
<p>这套系统可依照不同的路况、环境、车速及天气状况，自动调整车灯的照明范围及角度，让车灯照射范围可以更深远下，又不会影像到其他用路人的视线，以提供驾驶人与对向来车更安全及舒适的照明。</p>
<p>从过去的AFS主动转向式头灯，到现在结合传感器的多颗LED智能型头灯，都是属于此系统的范畴。</p>
<h3 id="adas夜视系统">ADAS——夜视系统</h3>
<p><img src="/images/adas/9.png" width="30%"></p>
<p>可帮助驾驶可以在视线不明、看不清楚的夜晚或恶劣天气时，自动识别动物或大型异物，同时警告驾驶前方路况，以避免意外的发生。</p>
<p>辨别方式为以红外线来感知热量的不同，区分人、动物、车辆以及环境的差异，经过处理转变成图像，将原本看不清楚的物体清楚呈现在驾驶眼前，以降低行车风险。</p>
<h3 id="adas主动车距控制巡航系统-acc">ADAS——主动车距控制巡航系统
ACC</h3>
<p><img src="/images/adas/10.png" width="30%"></p>
<p>通过安装在车辆前部的车距传感器，持续扫描车辆前方道路来得知前车的车速与相对距离，行驶中会自动侦测车速，当与前车的距离越来越小时，会对应调整自身车速，与前方车辆保持安全距离，减少碰撞意外的发生，也就是所谓的高级版自动巡航系统。</p>
<p>目前许多车款上都已可看见此系统的踪影。</p>
<h3 id="adas驾驶人生理状态监视">ADAS——驾驶人生理状态监视</h3>
<p><img src="/images/adas/11.png" width="30%"></p>
<p>目前系统大多都是利用摄影机侦测驾驶者脸部，判断专注力程度、是否有打瞌睡的象征，还有系统更是利用驾驶人眼睛开闭频率情况，来辨别安全等级，提供适合的警告或是协助动作。</p>
<p>如果驾驶者的脸部表情变化减少，甚至出现闭眼的情况，车辆就会透过声响与灯号来警示车主注意，以减少意外事故发生。</p>
<h2 id="adas关键节点">ADAS关键节点</h2>
<ul>
<li><p>感知</p>
<p>目前ADAS主要采用的<code>传感器</code>有摄像头、雷达、激光和超声波等，可以探测光、热、压力或其它用于监测汽车状态的变量，通常位于车辆的前后保险杠、侧视镜、驾驶杆内部或者挡风玻璃上。</p>
<p>大部份ADAS采用<code>摄像头</code>+<code>雷达</code>的组合方式，以实现雷达测距与摄像头图像识别功能的互补。主动式与被动式红外夜视系统是两种主流的技术路线。<code>主动式</code>通过CCD接受物体反射的对应敏感频谱成像，而<code>被动式</code>的红外焦平面探测器接受物体的红外辐射成像。两者各有优势，将在较长时间内共存。</p>
<p>不同的系统需要使用不同类型的车用传感器，包含毫米波雷达、超声波雷达、红外雷达、激光雷达、CCD
CMOS影像传感器及轮速传感器等，来收集整车的工作状态及其参数变化情形，并将不断变化的机械运动变成电子参数(电压、电阻及电流)。</p>
<ul>
<li>举例来说，车道偏离警告系统使用CMOS影像传感器、夜视系统则使用红外线传感器、适应性定速控制通常使用雷达、停车辅助系统则会使用超声波等。</li>
</ul></li>
<li><p>判断</p>
<p>算法是ADAS系统可靠性、准确度的<code>决定性因素</code>，主要包括摄像头/雷达测距、行人识别、道路交通标志识别等，针对前装应用的可靠性要求高，需要进行大量的场景测试与标定。其中雷达标定的门槛最高。</p>
<p>电子控制单元(ECU)会在针对传感器所收集到的信息进行分析处理，然后再向控制的执行装置下达动作指令。</p></li>
<li><p>执行
包含油门、刹车、灯光、声响等系统都是属于执行器的范畴内，会依据ECU输出的讯号，来执行各种反应动作，让汽车安全行驶于道路上。</p>
<p>ADAS系统通过传感器获取数据，主芯片完成判断后，初级应用通过声音、图像、振动对驾驶者进行<code>警示</code>。</p>
<p>与电子控制功能结合后，逐渐进化到对车辆的<code>自动控制</code>。</p></li>
</ul>
<h2 id="adas产业链">ADAS产业链</h2>
<p><img src="/images/adas/12.png" width="60%"></p>
<p>在产业链中，<code>传感器技术</code>(摄像头与雷达)、<code>芯片</code>与<code>算法</code>是其中的关键。</p>
<ul>
<li><p>其中，摄像头核心部件CMOS感光芯片主要掌握在以索尼、三星为代表的日韩企业中。</p></li>
<li><p>雷达分为超声波雷达、毫米波雷达、激光雷达</p>
<ul>
<li>超声波雷达技术门槛较低，供应商较多；</li>
<li>激光雷达成本高昂，未商业化；</li>
<li>毫米波雷达成本在两者之间，技术门槛较高，主要掌握在ZF
TRW、博世等行业领先公司手中。</li>
</ul></li>
<li><p>而芯片、算法在ADAS系统中至关重要，行业集中度高，主要有Mobileeye、ADI等公司</p></li>
</ul>
<h2 id="视觉adas">视觉ADAS</h2>
<ul>
<li><p>原理</p>
<p>图像处理，将图片转换为<code>二维数据</code></p>
<p>模式识别，通过图像匹配进行识别，如车辆、行人、车道线、交通标志</p>
<p>利用物体的运动模式，或双目定位，估算目标物体与本车的相对距离和相对速度</p></li>
<li><p>优点</p>
<p>像素级别的颜色、偏移、距离等信息</p>
<p>成本低，信息量丰富、特征识别好</p></li>
<li><p>缺点</p>
<p>获取准确<code>三维信息</code>难度大</p>
<p>受环境光限制比较大，速度、距离分辨率差</p></li>
</ul>
<p>目前在行车辅助功能中，摄像头可用来单独实现很多功能，并且按照自动驾驶发展规律逐渐演进。</p>
<p><img src="/images/adas/13.png" width="60%"></p>
<p>这些功能更加强调对输入图像的处理，从拍摄的视频流中提取<code>有效目标运动信息</code>做进一步分析，给出预警信息或直接调动控制机构。相比视频输出类功能，强调<code>高速下的实时性</code>，这部分技术正处在发展上升期。</p>
<h2 id="视觉adas软硬需求">视觉ADAS软硬需求</h2>
<ul>
<li><p>摄像头模组</p>
<p>需要定制化开发，一般要满足宽动态、高感光、不一味追求高像素
ADAS的摄像头更讲究为行车时预留更多判断时间，需要看得更远。</p></li>
<li><p>核心算法芯片</p>
<p>如果在算法上叠加深度学习来帮助识别率提升，对硬件性能的要求只增不减，主要考虑的性能指标是运算速度、功耗、以及成本。</p></li>
<li><p>软件算法</p>
<p>源头是计算机视觉-&gt;深度学习的渗透</p>
<p>在感知方面，深度学习将会弯道超车传统视觉算法</p>
<p>无论哪种算法，数据都是用来训练测试的宝贵资源，而且不是单纯的越多越好，而是越有效越好（符合实际用车环境并保证多样化）。</p></li>
</ul>
<h2 id="小结">小结</h2>
<p>本次调研对<code>ADAS</code>有了一个系统的认知，感觉是<code>非常有潜力</code>以及<code>非常有挑战</code>的发展方向。</p>
<p>视觉ADAS相比于毫米波雷达识别效果更好，可以充分利用<code>视觉信息</code>进行车辆、路标等识别。</p>
<p>我们的重点在于利用<code>感知的信息</code>做决策，其中算法是ADAS系统可靠性、准确度的决定性因素，高效的部署会影响实时性。</p>
<p>接下来会对<code>FCW</code>及相关算法进行进一步调研</p>
<h2 id="参考博客">参考博客</h2>
<p><a
href="https://blog.csdn.net/u010607947/article/details/70847028?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0">一篇文章看懂ADAS</a></p>
<p><a
href="https://blog.csdn.net/zhanshen112/article/details/80374815?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-6">ADAS的八大系统</a></p>
<p><a
href="https://blog.csdn.net/sinat_32970179/article/details/99833646?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-13">特斯拉、吉利车上的那些ADAS系统你都会用吗？</a></p>
<p><a
href="https://blog.csdn.net/wanzew/article/details/82828579?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4">ADAS功能盘点及实现过程解析</a></p>
<p><a
href="https://blog.csdn.net/usstmiracle/article/details/95461183?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3">ADAS是什么？</a></p>
<p><a
href="https://blog.csdn.net/weixin_42229404/article/details/82623742?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-1">ADAS各功能模块及ADAS解决方案提供商详解</a></p>
<p><a
href="https://blog.csdn.net/Gary_Yan_cc/article/details/80717069?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158760398419195162559542%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=158760398419195162559542&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-12">车载摄像头在ADAS中的应用</a></p>
<p><a
href="https://blog.csdn.net/u011776903/article/details/72866442">ADAS视觉方案盘点上篇：摄像头、芯片和算法</a></p>
<h2 id="参考视频">参考视频</h2>
<p><a
href="https://haokan.baidu.com/v?vid=6343999001934023106&amp;pd=bjh&amp;fr=bjhauthor&amp;type=video">ADAS自动辅助驾驶系统为何如此受青睐？看看它到底有多强大！</a></p>
<p><a
href="https://haokan.baidu.com/v?vid=3780861684865301120&amp;pd=bjh&amp;fr=bjhauthor&amp;type=video">ADAS智能驾驶辅助系统</a></p>
<p><a
href="https://haokan.baidu.com/v?vid=8288616481555786115&amp;tab=recommend">WEY
VV6智能驾驶辅助系统深度体验</a></p>
<p><a
href="https://v.qq.com/x/page/m0373fiwrvp.html?">第一视角体验通用自动驾驶第二弹</a></p>
]]></content>
      <tags>
        <tag>ADAS</tag>
        <tag>tech， research</tag>
      </tags>
  </entry>
  <entry>
    <title>一文搞懂转置卷积的定义、计算和加速</title>
    <url>/2023/05/04/97_transposed-convolution/</url>
    <content><![CDATA[<p>最近做了一些转置卷积的相关部署工作，最开始搞的时候其实有点晕头转向的，总是试图在用卷积的计算方式反过来理解转置卷积，尤其是看到<code>padding</code>部分和<code>stride</code>部分，搞得我头更大了，心想怎么这么反人类？（后来发现是我理解的问题，其实一点也不反人类）</p>
<p>现在也算是了解了具体工作机制以及加速方式，在这里整理总结一下。欢迎留言、指正
:)</p>
<span id="more"></span>
<h1 id="转置卷积是什么">转置卷积是什么？</h1>
<h2 id="定义">定义</h2>
<p><code>转置卷积</code>有时候也被称为<code>反卷积</code>，我个人认为<code>反卷积</code>有很强的误导性，因为这并不是卷积的逆运算，还是叫<code>转置卷积</code>比较好。</p>
<p><code>转置卷积</code>在深度学习中表示为<code>卷积</code>的一个逆向过程，可以根据卷积核大小和输出的大小，恢复卷积前的<code>feature map</code>尺寸，而不是恢复原始值。</p>
<p>如果将卷积表示为y=Cx,转置卷积则是将的输入输出互换：x =
C<sup>T</sup>y</p>
<p>其中， C<sup>T</sup>表示矩阵转置。</p>
<p>详细定义这里就不仔细介绍了,上文里的各个参考文档里说的都很明白。</p>
<h2 id="需要注意">需要注意</h2>
<p>总结一下我认为的最重要的（最开始纠结了很久的）几个点：</p>
<ul>
<li><p>转置卷积不是恢复原始值，而是恢复原始尺寸（所以不要试图从卷积的逆运算角度考虑）</p></li>
<li><p><code>padding</code>方式和卷积的padding是不一样的，转置卷积的实际padding是<code>k-p-1</code></p></li>
<li><p><code>stride</code>在这里用途不是跳几个数，而是用于判断填充几个0</p></li>
<li><p>用公式法直接计算的话，首先对卷积核做中心对称操作(矩阵旋转180°)</p></li>
<li><p>不考虑性能的话，直接按照转置卷积定义写。反之，一定要优化，不然慢得很。</p></li>
</ul>
<p>The table below summarizes the two convolutions, standard and
transposed.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Conv Type</th>
<th style="text-align: center;">Operation</th>
<th style="text-align: center;">Zero Insertions</th>
<th style="text-align: center;">Padding</th>
<th style="text-align: center;">Stride</th>
<th style="text-align: center;">Output Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">Downsampling</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">s</td>
<td style="text-align: center;">(i+2p-k)/s+1</td>
</tr>
<tr class="even">
<td style="text-align: center;">Transposed</td>
<td style="text-align: center;">Upsampling</td>
<td style="text-align: center;">(s-1)</td>
<td style="text-align: center;">(k-p-1)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">(i-1)*s+k-2p</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意注意，有没有觉得转置卷积的<code>padding</code>和<code>stride</code>很反人类?这和我理解的完全不一样啊？其实不然，这里的<code>padding</code>和<code>stride</code>指的其实是转置卷积结果的<code>padding</code>和<code>stride</code>。即，逆向来看的情况下，卷积的<code>padding</code>和<code>stride</code>。</p>
</blockquote>
<blockquote>
<p>为了说明转置卷积如何<code>推理计算</code>，本文第二章节会使用表格所示的参数计算方式，直到第三章节才会从卷积的角度来看，到时候你会恍然大悟，<code>padding</code>和<code>stride</code>的含义，原来如此。</p>
</blockquote>
<h1 id="转置卷积的计算">转置卷积的计算</h1>
<h2 id="从最简单的开始">从最简单的开始</h2>
<p>conv_transpose有一种最直接的计算方式：首先对卷积核做中心对称操作(矩阵旋转180°)，并对输入feature
map进行插0，然后把旋转后的卷积核和插0后的feature map进行卷积操作</p>
<hr />
<p>现在假设输入的<code>feature map</code>是3x3大小，<code>kernel size</code>是3x3大小，<code>stride</code>为1，
<code>padding</code>为0，即：</p>
<pre><code>input_sz:     3
kernel_sz =   3
stride =      1
padding_sz =  0</code></pre>
<p>写一段torch代码计算一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">K = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">Y = torch.nn.functional.conv_transpose2d(X, K, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<p>得到输出结果： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensor([[[[  1.,   4.,  10.,  12.,   9.],</span><br><span class="line">          [  8.,  26.,  56.,  54.,  36.],</span><br><span class="line">          [ 30.,  84., 165., 144.,  90.],</span><br><span class="line">          [ 56., 134., 236., 186., 108.],</span><br><span class="line">          [ 49., 112., 190., 144.,  81.]]]])</span><br></pre></td></tr></table></figure> 计算过程：</p>
<ol type="1">
<li>对输入<code>X</code>进行处理，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding</li>
</ol>
<p>在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-0-1)=2</code>的padding。输入<code>X</code>则转化为</p>
<p><img src="/images/transposed_conv/x_s1p0.png" width="50%"></p>
<ol start="2" type="1">
<li>对卷积核<code>K</code>进行中心对称操作</li>
</ol>
<p>卷积核<code>K</code>则转化为</p>
<p><img src="/images/transposed_conv/k_reverse.png" width="30%"></p>
<ol start="3" type="1">
<li>进行卷积计算</li>
</ol>
<p><img src="/images/transposed_conv/s1p0.png" width="80%"></p>
<ol start="4" type="1">
<li>gif图解</li>
</ol>
<p><img src="/images/transposed_conv/transposed_conv_S1P0.gif" width="70%"></p>
<h2 id="考虑stride">考虑stride</h2>
<p>我个人建议不要用卷积的stride来理解转置卷积的stride，<code>stride</code>在这里用途不是跳几个数，而是用于判断填充几个0。</p>
<hr />
<p>现在假设输入的<code>feature map</code>是3x3大小，<code>kernel size</code>是3x3大小，<code>stride</code>为2，
<code>padding</code>为0，即：</p>
<pre><code>input_sz:     3
kernel_sz =   3
stride =      2
padding_sz =  0</code></pre>
<p>同样，写一段torch代码计算一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">K = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">Y = torch.nn.functional.conv_transpose2d(X, K, stride=<span class="number">2</span>, padding=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<p>得到输出结果： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensor([[[[  1.,   2.,   5.,   4.,   9.,   6.,   9.],</span><br><span class="line">          [  4.,   5.,  14.,  10.,  24.,  15.,  18.],</span><br><span class="line">          [ 11.,  16.,  40.,  26.,  60.,  36.,  45.],</span><br><span class="line">          [ 16.,  20.,  44.,  25.,  54.,  30.,  36.],</span><br><span class="line">          [ 35.,  46., 100.,  56., 120.,  66.,  81.],</span><br><span class="line">          [ 28.,  35.,  74.,  40.,  84.,  45.,  54.],</span><br><span class="line">          [ 49.,  56., 119.,  64., 135.,  72.,  81.]]]])</span><br></pre></td></tr></table></figure> 计算过程：</p>
<ol type="1">
<li>对输入<code>X</code>进行处理，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding</li>
</ol>
<p>在这个例子中，<code>s=2</code>，需插入1个0，进行<code>(k-p-1)=(3-0-1)=2</code>的padding。输入<code>X</code>则转化为</p>
<p><img src="/images/transposed_conv/x_s2p0.png" width="50%"></p>
<ol start="2" type="1">
<li>对卷积核K进行中心对称操作</li>
</ol>
<p>卷积核K则转化为</p>
<p><img src="/images/transposed_conv/k_reverse.png" width="30%"></p>
<ol start="3" type="1">
<li>进行卷积计算</li>
</ol>
<p><img src="/images/transposed_conv/s2p0.png" width="80%"></p>
<ol start="4" type="1">
<li>gif图解</li>
</ol>
<p><img src="/images/transposed_conv/transposed_conv_S2P0.gif" width="70%"></p>
<h2 id="考虑padding">考虑padding</h2>
<p>我最开始在<code>padding</code>这里疑惑了好一会儿，老是在从卷积的角度想转置卷积的<code>padding</code>。就很疑惑，怎么<code>padding</code>越大，计算结果的<code>feature map</code>越小呢？</p>
<p>后来暂时不想具体物理含义了，直接认为转置卷积的实际<code>padding</code>是<code>k-p-1</code>，万事大吉。（第三章节会解<code>padding</code>这个参数）</p>
<hr />
<blockquote>
<p>实际上，tensorflow的padding计算还是有点差异的，除了上面所说的计算，在计算padding的时候还有一个专门针对转置卷积的<code>offset</code>，这可能会导致
左右/上下 的padding数不一致。
为什么这么做呢？个人认为要从转置卷积的目的来看————还原原始<code>feature map</code>的尺寸。
本文暂不考虑这种情况，感兴趣的可以查看tensorflow源码。</p>
</blockquote>
<hr />
<p>现在假设输入的<code>feature map</code>是3x3大小，<code>kernel size</code>是3x3大小，<code>stride</code>为1，
<code>padding</code>为1，即：</p>
<pre><code>input_sz:     3
kernel_sz =   3
stride =      1
padding_sz =  1</code></pre>
<p>写一段torch代码计算一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">X = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">K = torch.Tensor([[[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]]])</span><br><span class="line">Y = torch.nn.functional.conv_transpose2d(X, K, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<p>得到输出结果： <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensor([[[[ 26.,  56.,  54.],</span><br><span class="line">          [ 84., 165., 144.],</span><br><span class="line">          [134., 236., 186.]]]])</span><br></pre></td></tr></table></figure> 计算过程：</p>
<ol type="1">
<li>对输入<code>X</code>进行处理，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding</li>
</ol>
<p>在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-1-1)=1</code>的padding。输入X则转化为</p>
<p><img src="/images/transposed_conv/x_s1p1.png" width="50%"></p>
<ol start="2" type="1">
<li>对卷积核<code>K</code>进行中心对称操作</li>
</ol>
<p>卷积核<code>K</code>则转化为</p>
<p><img src="/images/transposed_conv/k_reverse.png" width="30%"></p>
<ol start="3" type="1">
<li>进行卷积计算</li>
</ol>
<p><img src="/images/transposed_conv/s1p1.png" width="80%"></p>
<ol start="4" type="1">
<li>gif图解</li>
</ol>
<p><img src="/images/transposed_conv/transposed_conv_S1P1.gif" width="70%"></p>
<h2 id="考虑dilation">考虑dilation</h2>
<p>这里就不考虑了，和卷积一样的，很容易理解。</p>
<h1 id="转置卷积的理解">转置卷积的理解</h1>
<p>初次见到转置卷积的时候，我看到<a
href="https://github.com/vdumoulin/conv_arithmetic">conv_arithmetic</a>和<a
href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">What
is Transposed Convolutional
Layer</a>里面的动图计算是很开心的，毕竟这有助于我理解转置卷积是如何计算的。</p>
<p>但后来发现，了解计算过程看这些动图是比较好的，但是要是理解转置卷积，这些动图<strong>很容易误导</strong>。于是我写代码生成了一些动图，虽然有点丑，但是<code>padding</code>部分和<code>stride</code>部分立刻搞得清清楚楚明明白白了。</p>
<h2 id="stride1padding0">stride=1，padding=0</h2>
<p><img src="/images/transposed_conv/solution1_stride1_pad0.gif" width="70%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-0-1)=2</code>的padding。</li>
</ul>
<p>我们来换个角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride1_pad0.gif" width="55%"></p>
<ul>
<li><strong>从output的角度来看，stride=1，padding=0（你get到这个点了吗?!）</strong></li>
</ul>
<h2 id="stride1padding1">stride=1，padding=1</h2>
<p><img src="/images/transposed_conv/solution1_stride1_pad1.gif" width="60%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=1</code>，则无需插入0，只进行<code>(k-p-1)=(3-1-1)=1</code>的padding。</li>
</ul>
<p>同样，我们来换个角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride1_pad1.gif" width="50%"></p>
<ul>
<li><strong>从output的角度来看，stride=1，padding=1</strong></li>
</ul>
<p>相信你已经明白，我在最开始说的，<strong><code>padding</code>和<code>stride</code>指的其实是转置卷积结果的<code>padding</code>和<code>stride</code></strong>这句话了吧？</p>
<h2 id="stride2padding0">stride=2，padding=0</h2>
<p><img src="/images/transposed_conv/solution1_stride2_pad0.gif" width="70%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=2</code>，需插入1个0，然后进行<code>(k-p-1)=(3-0-1)=2</code>的padding。</li>
</ul>
<p>我们来换个角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride2_pad0.gif" width="50%"></p>
<ul>
<li><strong>从output的角度来看，stride=2，padding=0</strong></li>
</ul>
<p>尤其是在stride&gt;1的情况下，第二种的计算量是远小于第一种的，因为第一种有大量的无效0计算。</p>
<h2 id="stride2padding1">stride=2，padding=1</h2>
<p><img src="/images/transposed_conv/solution1_stride2_pad1.gif" width="70%"></p>
<ul>
<li>从input的角度来看，插入<code>(s-1)</code>的0，做<code>(k-p-1)</code>的padding。在这个例子中，<code>s=2</code>，需插入1个0，然后进行<code>(k-p-1)=(3-1-1)=1</code>的padding。</li>
</ul>
<p>output的角度看整个计算过程：</p>
<p><img src="/images/transposed_conv/solution2_stride2_pad1.gif" width="50%"></p>
<ul>
<li><strong>从output的角度来看，stride=2，padding=1</strong></li>
</ul>
<hr />
<p>至此，各种情况展示的差不多了，转置卷积的<code>padding</code>和<code>stride</code>的含义，相信你也理解了。大家还是习惯用卷积那套参数来定义转置卷积，甚至在很多推理框架中，转置卷积这个算子的input指的是计算结果，output指的是输入（这又是另一个曾让我困扰的地方了hhh）。</p>
<h1 id="转置卷积的加速">转置卷积的加速 🚀</h1>
<p>以最简单的stride=1，pad=0的情况为例：</p>
<h2 id="直接计算方式">直接计算方式</h2>
<p><img src="/images/transposed_conv/solution1_stride1_pad0.gif" width="50%"></p>
<ul>
<li><p>直接计算方式通过padding和补0操作，将 input 的 feature map
增大，导致整体计算量大幅度增大</p></li>
<li><p>其中有大量的0计算，其实是无效的。</p></li>
</ul>
<p>相应的伪代码为：</p>
<p><img src="/images/transposed_conv/solution1.png" width="40%"></p>
<h2 id="优化计算方式">优化计算方式</h2>
<p><img src="/images/transposed_conv/solution2_stride1_pad0.gif" width="50%"></p>
<p>也就是第三章节中提到的，从output的角度来看的计算方式，这种优化方式避免了大量的无效0计算，其实是已经大大减小<code>乘加计算量</code>的。因此可以从这个角度做加速，效果很明显的。</p>
<ul>
<li><p>从output的角度反过来寻找对应的input进行浮点乘加计算，这样便可以避免掉无效的0计算。</p></li>
<li><p>然而，在这6层for循环中，有大量的边界判断，这些if操作会严重影响计算并行度</p></li>
</ul>
<p>相应的伪代码为：</p>
<p><img src="/images/transposed_conv/solution2.png" width="40%"></p>
<h2 id="进一步优化计算方式">进一步优化计算方式</h2>
<p><img src="/images/transposed_conv/solution3_stride1_pad0.gif" width="50%"></p>
<p><code>gemm</code>计算</p>
<ul>
<li>相当于做了6层for循环中的所有浮点乘法计算，该部分密集型的计算可以使用并行计算进行大幅加速</li>
</ul>
<p><code>col2im</code>计算</p>
<ul>
<li>通过stride和padding计算累加规则，累加后还原出转置卷积的计算结果。</li>
</ul>
<p><code>col2im</code> 相应的伪代码为：</p>
<p><img src="/images/transposed_conv/solution3.png" width="40%"></p>
<h1 id="总结">总结</h1>
<p>转置卷积在诸如图像超分辨率、语音降噪等任务中还是很常见的，经过这一番的探索，现在也算是彻底理解转置卷积了。</p>
<p>关于转置卷积的加速，终极优化版本将转置卷积转换为
<code>gemm</code>+<code>col2im</code>，首先就是避免了大量的无效0计算，计算量通常可以减少为原来的18%～36%左右。</p>
<p>这样的坏处就是要开辟一块额外的内存用于存放gemm结果；好处就很明显了，将乘法和加法分离，对密集的乘法进行并行加速处理，无需边界判断，计算效率大大提升。其实精髓在于可以利用gemm的加速，如果再配合一块高速内存，转置卷积的计算速度提升个300%以上简直是轻轻松松啊。</p>
<h1 id="参考文档">参考文档</h1>
<p><a
href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">tf.keras.layers.Conv2DTranspose</a></p>
<p><a
href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">What
is Transposed Convolutional Layer?</a></p>
<p><a
href="https://blog.csdn.net/LoseInVain/article/details/81098502">一文搞懂反卷积，转置卷积</a></p>
<p><a
href="https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0">Up-sampling
with Transposed Convolution</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/115070523">转置卷积(Transpose
Convolution)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/283091528">conv_transpose
depth-wise优化技巧</a></p>
<p>图解转置卷积，我分别在<a
href="https://github.com/vdumoulin/conv_arithmetic">conv_arithmetic</a>和<a
href="https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11">What
is Transposed Convolutional Layer</a>里看到，感觉后者更容易理解。</p>
<p>本文用图参考了<a
href="https://github.com/aqeelanwar/conv_layers_animation">aqeelanwar</a>的代码，非常感谢。</p>
<p>图像压缩用了<a
href="https://www.iloveimg.com/zh-cn">iloveimg</a>，非常好用～</p>
]]></content>
      <tags>
        <tag>HPC</tag>
        <tag>tech</tag>
        <tag>engineering</tag>
        <tag>Transposed Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title>Telegram bot</title>
    <url>/2024/02/17/98_Telegram_bot/</url>
    <content><![CDATA[<p>最近学习一些telegram bot，记录一下</p>
<span id="more"></span>
<h1 id="installing">Installing</h1>
<p>安装 python-telegram-bot 库</p>
<p><code>pip install python-telegram-bot --upgrade</code></p>
<p>安装完毕 &gt; Successfully installed h11-0.14.0 httpcore-1.0.3
httpx-0.26.0 python-telegram-bot-20.8</p>
<h1 id="bot-申请">bot 申请</h1>
<p>私聊<a href="https://t.me/BotFather">BotFather</a>,</p>
<ul>
<li>/start</li>
<li>/newbot</li>
<li>Alright, a new bot. How are we going to call it? Please choose a
name for your bot.</li>
<li>IngressHomogeneous</li>
<li>Good. Now let's choose a username for your bot. It must end in
<code>bot</code>. Like this, for example: TetrisBot or tetris_bot.</li>
<li>IngressHomogeneousBot</li>
<li>Done! Congratulations on your new bot. You will find it at
t.me/IngressHomogeneousBot. You can now add a description, about section
and profile picture for your bot, see /help for a list of commands. By
the way, when you've finished creating your cool bot, ping our Bot
Support if you want a better username for it. Just make sure the bot is
fully operational before you do this.</li>
</ul>
<p>妥善保存最后的 API Token。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">You can control me by sending these commands:</span><br><span class="line"></span><br><span class="line">/newbot - create a new bot</span><br><span class="line">/mybots - edit your bots</span><br><span class="line"></span><br><span class="line">Edit Bots</span><br><span class="line">/setname - change a bot&#x27;s name</span><br><span class="line">/setdescription - change bot description</span><br><span class="line">/setabouttext - change bot about info</span><br><span class="line">/setuserpic - change bot profile photo</span><br><span class="line">/setcommands - change the list of commands</span><br><span class="line">/deletebot - delete a bot</span><br><span class="line"></span><br><span class="line">Bot Settings</span><br><span class="line">/token - generate authorization token</span><br><span class="line">/revoke - revoke bot access token</span><br><span class="line">/setinline - toggle inline mode</span><br><span class="line">/setinlinegeo - toggle inline location requests</span><br><span class="line">/setinlinefeedback - change inline feedback settings</span><br><span class="line">/setjoingroups - can your bot be added to groups?</span><br><span class="line">/setprivacy - toggle privacy mode in groups</span><br><span class="line"></span><br><span class="line">Web Apps</span><br><span class="line">/myapps - edit your web apps</span><br><span class="line">/newapp - create a new web app</span><br><span class="line">/listapps - get a list of your web apps</span><br><span class="line">/editapp - edit a web app</span><br><span class="line">/deleteapp - delete an existing web app</span><br><span class="line"></span><br><span class="line">Games</span><br><span class="line">/mygames - edit your games</span><br><span class="line">/newgame - create a new game</span><br><span class="line">/listgames - get a list of your games</span><br><span class="line">/editgame - edit a game</span><br><span class="line">/deletegame - delete an existing game</span><br></pre></td></tr></table></figure>
<h1 id="获取我的-id">获取我的 id</h1>
<p>每个 tg 用户都有一串唯一标识，即为 user_id，可以私聊 <a
href="https://t.me/kmuav2bot"><span class="citation"
data-cites="kmua">@kmua</span></a> 发送 /id 来获取它</p>
<p>跟着<a
href="https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions---Your-first-Bot">Your-first-Bot</a>这个来就好了，理解的话可以参考<a
href="https://krau.top/posts/tg-bot-dev-note-kmua">使用Python写一个Telegram
bot吧|Telegram bot教程</a>。</p>
<p>感觉整体比较好理解，接下来就是赋予其灵魂了～</p>
<hr />
<p>参考文章：</p>
<p><a
href="https://docs.python-telegram-bot.org/en/v20.8/">python-telegram-bot.org</a></p>
<p><a
href="https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions---Your-first-Bot">Your-first-Bot</a></p>
<p><a
href="https://krau.top/posts/tg-bot-dev-note-kmua">使用Python写一个Telegram
bot吧|Telegram bot教程</a></p>
]]></content>
      <tags>
        <tag>Telegram bot</tag>
        <tag>ingress</tag>
        <tag>Homogeneous</tag>
      </tags>
  </entry>
  <entry>
    <title>内存对齐的实现</title>
    <url>/2022/12/13/95_memory-align/</url>
    <content><![CDATA[<p>各个硬件平台对存储空间的处理上有很大的不同。一些平台对某些<code>特定类型的数据</code>只能从某些<code>特定地址</code>开始存取,而不是内存中任意地址都是可以读取的。</p>
<p>内存对齐不仅<strong>便于CPU快速访问</strong>，同时合理的利用字节对齐可以<strong>有效的节省存储空间</strong>。</p>
<p>花了一点点时间搞懂了内存对齐的实现与细节——</p>
<span id="more"></span>
<h2 id="什么是内存对齐">0.什么是内存对齐？</h2>
<p>计算机中内存空间是按照<code>byte</code>划分的，从理论上讲似乎对任何类型的变量的访问可以从任何地址开始，但实际情况是：在访问<code>特定类型变量</code>的时候通常在<code>特定的内存地址</code>访问，这就需要对这些数据在内存中存放的位置有限制，各种类型数据按照一定的规则在空间上排列，而不是顺序的一个接一个的排放，这就是对齐。</p>
<p>内存对齐是编译器的管辖范围。表现为：编译器为程序中的每个“数据单元”安排在适当的位置上。</p>
<h2 id="为什么要内存对齐">1.为什么要内存对齐？</h2>
<p>各个硬件平台对存储空间的处理上有很大的不同。一些平台对某些<code>特定类型的数据</code>只能从某些<code>特定地址</code>开始存取,而不是内存中任意地址都是可以读取的。</p>
<p><strong>比如有些架构的CPU在访问一个没有进行对齐的变量的时候会发生错误,那么在这种架构下编程必须保证字节对齐.</strong></p>
<p>其他平台可能没有这种情况，但是最常见的是如果不按照适合其平台要求对数据存放进行对齐，会在存取效率上带来损失。</p>
<p>内存对齐不仅<strong>便于CPU快速访问</strong>，同时合理的利用字节对齐可以<strong>有效的节省存储空间</strong>。</p>
<p>参考链接：https://juejin.cn/post/6870162226032934926</p>
<h2 id="我的分析实践">2.我的分析实践</h2>
<p><img src="/images/memory_align/MemoryAlign1.png" width="90%"></p>
<p><img src="/images/memory_align/MemoryAlign2.png" width="90%"></p>
]]></content>
      <tags>
        <tag>tech</tag>
        <tag>memory align</tag>
      </tags>
  </entry>
  <entry>
    <title>Homogeneous_ShangHai</title>
    <url>/2023/08/04/99_Homogeneous-ShangHai/</url>
    <content><![CDATA[<h1 id="背景">0、背景</h1>
<h2 id="前年2021">0.0 前年（2021）</h2>
<p>我在北京的校园里，设计并制作了一个六重竹笋，<a
href="https://mp.weixin.qq.com/s/O6tBVdg4Saa4KtITM-UNng">《六重蓝笋成长记（上）》</a>和<a
href="https://mp.weixin.qq.com/s/6t3HY1REaKn-AUS1TxTOdQ">《六重蓝笋成长记（下）》</a>，开始体会到竹笋的乐趣，并立志于做更高重的笋。</p>
<h2 id="去年2022">0.1 去年（2022）</h2>
<p>我在上海的某个特殊时期，闲得实在是无聊，制作了 <a
href="https://mp.weixin.qq.com/s/cNCfhqoNB2wIC-5lAzIR3w">视频：从入门到夺笋</a>，以缓解不能出去玩的郁闷，并立志于做更高重的七重(七色彩虹)竹笋。</p>
<span id="more"></span>
<h2 id="今年2023">0.2 今年（2023）</h2>
<p>随着我做高重蓝笋的念头越来越强烈，便开始着手准备这件事情。</p>
<p>非常重要的问题来了——做几重？</p>
<p><img src="/images/ingress/homogeneous_table.png" width="60%"></p>
<ul>
<li><p>我上一次做单人六重，耗时 5 个小时，行程 31.37 公里。</p></li>
<li><p><code>@ALHDLIOX</code> 上一次做<a
href="https://mp.weixin.qq.com/s/mqPZ0Rjx0QR3yCGgSz1Hxg">单人七重</a>，耗时
35 小时，行程 184.69 公里</p></li>
</ul>
<p>经过鲁莽的思考，我觉得九这个数字很好，代表数之极。做九重！</p>
<p>正所谓：</p>
<blockquote>
<p><strong>九重开，灵之斿，垂惠恩，鸿祜休。</strong></p>
</blockquote>
<p>不对，做九重的 AP 只够我转生半次的，我要十重！</p>
<h1 id="获取数据">1、获取数据</h1>
<h2 id="三年前的数据">1.1 三年前的数据</h2>
<p>我想起 <code>@Nanoape</code> 曾在 <a
href="https://mp.weixin.qq.com/s/ewEHaY8gKo2o31ApP5T7GQ">来刷新新的个人
Dronenet 飞行记录吧</a> 一文中提到他拿到了全球所有 Portal 的数据。共
1.31G，Portal 总数 11,475,124，数据统计截止到 2020.06.12。</p>
<p>于是我下载了portal数据<a
href="https://mega.nz/file/PsYnkCoD#ujlASFyomu1ZT8WZXi1_57ogH3pneGRXT2EC156u-lg">Mega</a>(这个网盘真良心啊，不限速，不强制登陆)</p>
<p>然而三年过去了，这一数据已经过时，比起得不到最优解，我更担心的是有的
portal 移动了位置，致使整个计划失败。</p>
<h2 id="获取新数据">1.2 获取新数据</h2>
<p>我本想通过某个脚本来抓取上海区域的所有 portal
的信息，但是抓到了这个信息：</p>
<blockquote>
<p>{'message': 'Please, dont do so. Contact <span class="citation"
data-cites="Lanched">@Lanched</span> insted.'}</p>
</blockquote>
<p>谢天谢地，这似乎制止了我违反<a
href="https://nianticlabs.com/terms">ToS</a>的行为。</p>
<p>于是我联系了
<code>@Lanched</code>，在他的热心帮助下，我顺利拿到了一份整个上海portal的信息。</p>
<blockquote>
<p>数据截止到 2023-7-30 日，区域范围是北纬 30.662225-31.879785，东经
120.860389-121.969865，基本包含整个上海区域，一共 30,179 个potal。</p>
</blockquote>
<p>在与他的交流过程中得知，全球 portal 的数量已经从2020年的 11,475,124
个，2023年上涨到了 18,284,480 个，增长为原来的
159%。看这个数据很可观，依稀还是能看到它茁壮成长的影子的（拜托拜托，再撑十年吧）。</p>
<p>最后，我向他表示感谢，并表示 以后有需要我的地方我会尽全力帮忙，
得到的回复却是:</p>
<blockquote>
<p><code>I dont think, you can help me in ingress - anyway i cant play it anymore.</code></p>
</blockquote>
<p>我看了他的名字是
<code>Alex Zverev</code>，猜测到他的国籍，顿时心里五味杂陈，但愿以后游戏可以更纯粹一些。</p>
<h1 id="分析数据">2、分析数据</h1>
<p>写了一些小脚本，对拿到的数据进行了分析，结果如下：</p>
<p><img src="/images/ingress/shanghai_portals.gif" width="80%"></p>
<ul>
<li>上海的经纬度是东经120°52′-122°12′，北纬30°40′-31°53′之间。</li>
<li>数据截止到2023-7-30日，区域范围是北纬30.662225-31.879785，东经120.860389-121.969865，基本包含整个上海区域</li>
<li>共30179个potal。</li>
</ul>
<p><img src="/images/ingress/part_of_shanghai.png" width="60%"></p>
<p>我喜欢密集的区域！于是进一步将经纬度限制在比较密集的
北纬30.988-31.430，东经121.249-121.718，一共22431个portal:</p>
<p><img src="/images/ingress/part_of_shanghai_gif.gif" width="80%"></p>
<pre><code>上边长 =  44472m, 下边长 =  44679m
左边长 =  48716m, 右边长 =  48716m</code></pre>
<h1 id="计算竹笋的解">3、计算竹笋的解</h1>
<h2 id="思路一求助大厉害">3.1 思路一：求助大厉害</h2>
<p>又一次翻出了 <code>@NanoApe</code> 的<a
href="https://github.com/Konano/Ingress-Field-Design">大厉害</a>，又一次感叹高三的时候能写出这代码实在是太太太强了！</p>
<p>稍微修改了一下代码，使其能够兼容读取非json形式的portal信息。这时我突然有一个大胆的想法——如果不对竹笋的计算重数进行限制，将整个上海的
potals 送进代码，看一看整个魔都能容纳几重笋？</p>
<p>对一些代码进行修改，比如从<code>栈</code>上分配内存改为从<code>堆</code>上分配内存，增大对<code>portal</code>和<code>field</code>数量的限制等，最终发现这个方案不太可行——</p>
<p>以筛选出的 22431 个portal为例，这可以形成 <span
class="math inline">\(C^3_{22431} = 1880773814095\)</span> 个
field，如果对每一个 field
都进行保存计算，这比ChatGPT的参数量都多一个量级，直接劝退。</p>
<h2 id="思路二分而治之">3.2 思路二：分而治之</h2>
<p>采用<code>分治思想(Divide and Conquer)</code>，先选出若干小区域，对每个小区域使用大厉害进行竹笋计算，然后在所有计算结果中寻求高重竹笋的解。</p>
<p><img src="/images/ingress/divided_img.gif" width="80%"></p>
<p>如上，每一小块区域包含了1000个portal，通过对每个小区域计算竹笋的解，然后对所有的解进行拼接，拼出一个大竹笋。</p>
<p>然后就是兴致勃勃的准备进行暴力求解，辛苦电脑跑几天，然而代码跑起来突然意识到这条路其实是行不通的。</p>
<p>大致原因就是，竹笋确实是能获得很多解，但是没法拼大竹笋啊。于是也放弃了。</p>
<p>（不过这个代码在后面发挥了很大的作用）</p>
<h2 id="思路三不求优解但求有解">3.3 思路三：不求优解，但求有解</h2>
<p>灵感来源于以前聊天时 <code>@hydronephrosis</code> 的撒豆子说法：</p>
<blockquote>
<p>我喜欢这样理解field的形成：类似于学概率的时候往桌布上撒豆子的模式，在一块三角形的布上撒一颗豆子(po)，连接所在区域的三个顶点，把布分成三个小区域；然后再撒下一颗，连接所在小区域的三个顶点，以此类推，直到所有po都囊括，形成完美多重。根据撒豆子顺序的不同，同样的一组po可以形成成千上万种不同的完美多重，每一个完美多重也可以通过这种形式解读成唯一的一系列po的顺序。这种多样性正是完美多重的魅力所在。如果每一次不止撒一颗，而是根据现有区域的数量来撒相同数量的一把豆子，使得刚好每个区域都落下一颗，就是竹笋了。如此竟与你看field的三维视角有异曲同工之妙。</p>
</blockquote>
<blockquote>
<p>每撒一颗豆子都是多加三个field和三条link，也可以解释为什么完美多重的结构改变不影响AP和field的数量。</p>
</blockquote>
<p>以及<code>@HesalTJ</code>的补充：</p>
<blockquote>
<p>在游戏内实现的话就是，利用在Field之下连Link的新规则，在形成第一重之后依次抵达下一级的内点射3out
3out 3out…
实际上执行起来会绕很多圈，但是它的美丽在于，如果用Intel截了仅Field的图一帧一帧地做成录像，那么你会看到的是一个填充色由浅逐渐逐渐变深的三角形，每一帧都有一个小三角形的深度+1重，每次提升了重数之后新的小三角形面积变小数量×3：
从一重+1帧变为二重、+3帧变为三重、+9帧变为四重、+27帧变为五重…</p>
</blockquote>
<p>可惜的是，按照这种思路写代码比较难拿到最优解，做笋的时候会多走很多路。不过，不求优解，但求有解嘛:)</p>
<p>而且这种解法是我最喜欢的竹笋形状，我心目中的完美竹笋——</p>
<p><strong>高重的顶点位于次高重field的重心位置，这样的竹笋看起来更加规则，非常完美！</strong></p>
<blockquote>
<p>备注：三条中线相交的点叫做重心。</p>
</blockquote>
<p>针对这种思路，po场的分布均匀程度更重要，于是进一步将经纬度限制在分布比较密集且均匀的
北纬31.18-31.29，东经121.40-121.54，一共 8704 个 portal.</p>
<pre><code>上边长 =  13296m, 下边长 =  13312m
左边长 =  12226m, 右边长 =  12226m</code></pre>
<p>于是，我得到了上海的一个<strong>八重竹笋</strong>的解：</p>
<p><img src="/images/ingress/8_layer_homogeneous.gif" width="80%"></p>
<p>当然，这是一个假的。它甚至连一个6重竹笋都不是。</p>
<p>于是换一个思路，不再寻找三角形的重心位置，而是能够
<strong>将po的数量三等分的位置</strong>。这里就不展示了，因为跑出来个更不靠谱的结果。</p>
<p><strong>想来想去，还是得依靠 Nano 的大厉害啊！</strong></p>
<p>其实，<strong>思路二：分而治之</strong>
的方法其实是可行的，只不过我的那个方法实在是蠢到家了。</p>
<p>而且，要想计算高重竹笋，分而治之大概是最好的方法。</p>
<h1 id="搁置">4、搁置</h1>
<p>暂时没有得到8重解，这件事情就<strong>搁置</strong>了两个月。</p>
<p>11月的一天早上，<span class="citation"
data-cites="Hesal">@Hesal</span> 发给了我一个兑换码，兑换出来 800 个
la，我瞬间狂喜，因为这可以通过运动胶囊合成竹笋党最爱的
SBUL，也就在此刻，上海大竹笋计划又一次被我记起。</p>
<p>然后，这件事情又<strong>搁置</strong>了近两个月。</p>
<p>12月底，在 <code>@NanoApe</code> 的频道看到了这么一条消息——</p>
<blockquote>
<p>六年前的 2017 年，26 名北京 Res Agents
在金融街完成了全球最快六重蓝竹笋，12 月 24 日凌晨 02 点 17 分到凌晨 02
点 58 分，耗时 39 分钟。</p>
</blockquote>
<p><img src="/images/ingress/homogeneous_L6_2017.gif" width="80%"></p>
<p>想一想，26位玩家，在凌晨，齐心协力种出竹笋，真是一件浪漫的事情啊！</p>
<p>然后，这件事情又<strong>搁置</strong>了近两个月。</p>
<p>2024年2月底，我终于用光了那 800 个
la，上海大竹笋计划又一次被我记起！</p>
<h1 id="可行性分析">5、可行性分析</h1>
<h2 id="场内连线是否能实现真竹笋">5.1 场内连线是否能实现真竹笋？</h2>
<p>大约在2022年5月份的时候，已经允许 field 内 500米 的 link
了，那时候我产生过类似的疑问，于是做过相关实验。</p>
<p>结论是：在field内往顶点po射，是能够起到顶点po往里面射一样的效果的（即层数增加一致），所以不需要担心这种规则下会做一个假竹笋。</p>
<p>以前和平周活动可以允许长度不超过 8 公里的 Link 在 Field
下面建立，这非常利好7重以上的竹笋。 但是仍然需要注意一点：</p>
<ul>
<li>7重笋要保证封口时最近的 65 - 40 - 2 = 23 条 link 小于 8 公里</li>
<li>8重笋要保证封口时最近的 129 - 40 - 2 = 78 条 link 小于 8 公里</li>
<li>9重笋要保证封口时最近的 257 - 40 - 2 = 215 条 link 小于 8 公里</li>
</ul>
<h2 id="上海能否出现8重竹笋">5.2 上海能否出现8重竹笋？</h2>
<p>目前8重应该是可以的，尤其按照最近半年来上海portal肉眼可见的增长速度，这个难度是降低的。</p>
<p>但是按照现在双方特工们的活跃程度以及极个别的不和谐因素，出现高重竹笋的困难度还是非常高的。</p>
<h2 id="玩家够多吗">5.3 玩家够多吗？</h2>
<p>上海这边做竹笋尤其需要速战速决，这就必须要多人合作共同完成。</p>
<p>比如8重竹笋，要想在几个小时内完成至少要动用10名以上有竹笋经验的特工，</p>
<p>好在上海这边活跃玩家有很多，但是有竹笋经验的玩家比较少一些。尤其这两年，感觉已经很少听到竹笋的大新闻了，竹笋教快要没落了哇。</p>
<h1 id="起航">6、起航</h1>
<p>遍地开花</p>
<hr />
<p>至于上海高重竹笋计划，大概会在某个月黑风高夜顺利完成吧。</p>
]]></content>
      <tags>
        <tag>ingress</tag>
        <tag>Homogeneous</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2018/09/10/0_hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>ComfyUI</title>
    <url>/2024/10/31/106_ComfyUI/</url>
    <content><![CDATA[<p>开始学习 ComfyUI，学学文生图相关的知识</p>
<span id="more"></span>
<h1 id="教程">教程</h1>
<p><a
href="https://comfyanonymous.github.io/ComfyUI_tutorial_vn/">ComfyUI
官方教程</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion
Model</a></p>
<p><a
href="https://blog.csdn.net/weixin_37863729/article/details/141216314?ops_request_misc=%257B%2522request%255Fid%2522%253A%25228A8AF038-A51B-412A-B9CC-2615FB38F564%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=8A8AF038-A51B-412A-B9CC-2615FB38F564&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-141216314-null-null.142%5Ev100%5Epc_search_result_base6&amp;utm_term=comfyui&amp;spm=1018.2226.3001.4187">从零开始教你玩转ComfyUI-保姆级部署教程-手把手带你ComfyUI工作流搭建</a></p>
<p>ComfyUI 是节点式的Stable Diffusion用户界面。</p>
<ul>
<li>节点式就是每个功能被拆解成独立的模块，模块之间由线相连，用于传递信息。这种形式呈现出清晰的工作流，并且允许根据需求来自由搭建工作流。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/miniforge3/bin/activate</span><br><span class="line">conda create --name ComfyUI</span><br><span class="line">conda init bash</span><br><span class="line">conda activate ComfyUI</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/comfyanonymous/ComfyUI</span><br><span class="line"><span class="built_in">cd</span> ComfyUI</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>由于PyTorch 通常支持 Python 3.8、3.9、3.10（部分支持 3.11）。
报错：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ERROR: Could not find a version that satisfies the requirement torch (from versions: none)</span><br><span class="line">ERROR: No matching distribution found for torch</span><br></pre></td></tr></table></figure>
<p>于是安装3.10</p>
<p><code>conda install python=3.10</code></p>
<p>下载模型
<code>wget -P ~/Projects/ComfyUI/models/checkpoints https://hf-mirror.com/LarryAIDraw/v1-5-pruned-emaonly/resolve/main/v1-5-pruned-emaonly.ckpt</code></p>
<ul>
<li>os.environ['CUDA_VISIBLE_DEVICES']
<ul>
<li>设置了一个环境变量，用于限制哪些 GPU
设备对程序可见。这个变量在程序启动时确定，并且通常在设置后会影响如
TensorFlow 和 PyTorch 等深度学习框架，使它们只看到特定的 GPU。</li>
<li>必须在任何与 torch 或 CUDA 相关的模块加载之前设置。如果 torch
已经初始化了 CUDA 设备，然后再设置这个环境变量，那么它将不会生效。</li>
</ul></li>
</ul>
<p>启动： <code>python main.py</code></p>
<h1 id="qa">Q&amp;A</h1>
<h2 id="stable-diffusion-和-pixel-diffusion-的区别">Stable Diffusion 和
Pixel Diffusion 的区别？</h2>
<p>Stable Diffusion 和 Pixel Diffusion 都是基于扩散模型（Diffusion
Models）的图像生成方法，但它们有一些关键的区别，特别是在
<strong>处理图像的方式</strong> 和 <strong>训练策略</strong> 上。</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>特性</th>
<th>Stable Diffusion</th>
<th>Pixel Diffusion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>噪声添加</td>
<td>先在潜在空间添加噪声，后再通过反向去噪生成图像。</td>
<td>直接在图像的像素空间上添加噪声并去噪。</td>
</tr>
<tr class="even">
<td>潜在空间</td>
<td>使用VAE将图像转换为潜在空间，在潜在空间中进行扩散操作。</td>
<td>不使用潜在空间，直接在像素空间中进行操作。</td>
</tr>
<tr class="odd">
<td>生成效率</td>
<td>更高效，因为操作在潜在空间进行，减少了计算复杂度。</td>
<td>可能较低效，因为需要处理原始图像像素，计算需求更大。</td>
</tr>
<tr class="even">
<td>计算资源需求</td>
<td>需要较少的计算资源，适合生成较高分辨率的图像。</td>
<td>需要更多的计算资源，尤其在生成高分辨率图像时。</td>
</tr>
<tr class="odd">
<td>应用领域</td>
<td>艺术创作、文本到图像生成、游戏开发等。</td>
<td>主要用于更精细的像素级图像生成任务。</td>
</tr>
<tr class="even">
<td>生成质量</td>
<td>生成质量高，能够生成非常细致的图像，特别适合生成艺术风格图像。</td>
<td>可能在一些细节处理上表现更好，但计算开销较大。</td>
</tr>
</tbody>
</table>
<p>简而言之，Stable Diffusion 更侧重于在潜在空间中进行高效的图像生成，而
Pixel Diffusion
则注重更精细的像素级处理，适合那些需要直接操作图像像素的应用。</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>ComfyUI</tag>
        <tag>Diffusion</tag>
        <tag>Tech</tag>
      </tags>
  </entry>
  <entry>
    <title>TinyML and Efficient Deep Learning Computing</title>
    <url>/2024/11/04/107_TinyML%20and%20Efficient%20Deep%20Learning%20Computing/</url>
    <content><![CDATA[<p>在<a
href="https://www.zhihu.com/question/668077638/answer/4043039987">知乎</a>看到这么一句话：</p>
<pre><code>跟着好好上韩松的课程，把 lab 都自己认真做一遍，特别是 AWQ 那套算法和推理的框架 （quantizaiton），只要能读懂整套代码，就能自己回答你问的问题了～</code></pre>
<p>感觉自己其实没有一个系统的学习，于是跟着<a
href="https://hanlab.mit.edu/courses/2024-fall-65940">韩松</a>学习一遍，在此记录一下。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>TinyML</tag>
        <tag>Tech</tag>
      </tags>
  </entry>
  <entry>
    <title>Efficient Streaming Language Models with Attention Sinks</title>
    <url>/2024/11/04/108_streaming-llm/</url>
    <content><![CDATA[<p>Deploy LLMs for infinite-length inputs without sacrificing efficiency
and performance.</p>
<span id="more"></span>
<p><a
href="https://github.com/mit-han-lab/streaming-llm">Git地址</a></p>
<h1 id="背景">背景</h1>
<h2 id="两大挑战">两大挑战</h2>
<p>在长时间交互的流式应用中，如多轮对话这样的连续聊天，会有两大挑战：</p>
<ul>
<li>在解码阶段，缓存先前标记的键和值状态 (KV)
会消耗大量内存。容易OOM（out of memory）。</li>
<li>popular LLM 无法推广到比训练序列长度更长的文本。模型性能中断。</li>
</ul>
<h2 id="内存占用与困惑度">内存占用与困惑度</h2>
<p>在<a
href="https://github.com/tomaarsen/attention_sinks">attention_sinks</a>中指出了这种现象：</p>
<p><img src="/images/108_streaming-llm/Llama-2-7b-hf.png" width="70%"></p>
<ul>
<li>简单的 transformer （橙色）
<ul>
<li>内存占用随着输入长度线性增长，容易 OOM。</li>
<li>输入长度如果超过训练时的上下文长度，困惑度就会增加。图中在4k长度以后，模型回答质量会很差。（其中这里的4k长度是训练窗口）</li>
</ul></li>
<li>使用 window attention （绿色）
<ul>
<li>使用有限的、恒定的内存</li>
<li>当句子长度超过窗口长度时，困惑度爆炸。因为这时候句子前面的token的 KV
cache 会被驱逐。而最前面的几个token非常重要，称之为 attention
sink，也就是本文的重点。</li>
</ul></li>
<li>StreamingLLM （蓝色）
<ul>
<li>使用有限的、恒定的内存</li>
<li>困惑度一直很低，一直到10k长度（实测到 4M 也ok）</li>
</ul></li>
</ul>
<h2 id="kv-cache">KV Cache</h2>
<p>在decoding阶段，要想进行attention计算，需要之前token的
<strong>K</strong>eys 和 <strong>V</strong>alues。以及需要当前的
<strong>Q</strong>uery token。</p>
<p><img src="/images/108_streaming-llm/kvcache.png" width="70%"></p>
<p>不必在每个 token 生成步骤中重新计算所有先前 token 的 Key 和 Value
向量，而是可以只对当前 token 执行增量计算，并重新使用 KV
缓存中先前计算的 Key/Value 向量。当前 token 的 Key/Value 向量也会附加到
KV 缓存中，以供下一个 token 生成步骤使用。</p>
<p><img src="/images/108_streaming-llm/kv-cache-optimization.png" width="70%"></p>
<p>通过重用以前计算的 Self-Attention
键值对来节省计算资源，这种方式称之为 KV cache。</p>
<p><a
href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html">内容来源</a></p>
<h2 id="kv-cache-的内存占用">KV Cache 的内存占用</h2>
<p>在长文本输入的情况下， KV chche 的占用是非常大的。</p>
<p><img src="/images/108_streaming-llm/large-kvcache.png" width="90%"></p>
<p>所以，防止内存不足必然是要使用 window
attention，这样计算复杂度就变成了 O(TL).</p>
<blockquote>
<p>The language model, pre-trained on texts of length L, predicts the
Tth token (T ≫ L)</p>
</blockquote>
<h2 id="window-attention">Window Attention</h2>
<p><img src="/images/108_streaming-llm/window.png" width="90%"></p>
<p>这种 window attention 的方法 cache 了最近 L 个 token
的KV，这样内存占用就很少了。问题是，一旦起始 token 的 KV cache
被逐出后，困惑度就爆炸了。</p>
<h1 id="有趣的现象">有趣的现象</h1>
<p>为什么一旦起始 token 的 KV cache
被逐出后，困惑度就爆炸？<strong>因为第一个 token 非常重要</strong></p>
<p><img src="/images/108_streaming-llm/attention-sink.png" width="90%"></p>
<p>在第二层后，其他很多层的随机采样对第一个 token 有非常大的
attention，所以第一个token有很大的 attention
score，即使这个token的语义没什么意义。</p>
<p>为什么会这样？这和 softmax 的原理有关系：</p>
<p><img src="/images/108_streaming-llm/softmax.png" width="90%"></p>
<p>softmax必须是概率的总和为1，即使某些 token 不是那么的重要。</p>
<p>这就导致第一个 token 就很特别：由于是自回归的方式，所有后续的 token
都会关注第一个 token。</p>
<p>如果有些东西不是那么相关，就会把所有的 attention scores 转移到第一个
token。这就是所谓的 Attention sinks。</p>
<p>也就是说，模型过度关注初始的几个 token 是因为没有指定的 sink token 来
offload 过多的注意力分数。因此，该模型无意中使用全局可见的token（主要是
initial tokens）作为Attention sinks。</p>
<p><strong>是位置重要还是语义重要？</strong></p>
<p>实验证明，位置重要，前4个token的位置很重要。</p>
<h1 id="解决方式">解决方式</h1>
<p><img src="/images/108_streaming-llm/d_streaming_llm.png" width="90%"></p>
<p>StreamingLLM 保留了 attention sink 也就是前几个 initial tokens，
这样就能进行稳定的 attention computation, 这几个attention sink 会与
最近的一些 tokens 共同组成一个 window。</p>
<p>就像图中，包含两部分</p>
<ul>
<li><ol type="1">
<li>Attention sinks (four initial tokens) stabilize the attention
computation;</li>
</ol></li>
<li><ol start="2" type="1">
<li>Rolling KV Cache retains the most recent tokens, crucial for
language modeling.</li>
</ol></li>
</ul>
<p>假设 KV cache 的大小是8，</p>
<ul>
<li>Generating Token 7
<ul>
<li>所有的都在KV Cache</li>
</ul></li>
<li>Generating Token 8
<ul>
<li>保留前四个Attention sinks，跳过4，使用 rolling window
的方式处理剩余的tokens</li>
<li>尤其需要注意这时候的 position encoding，这时候要使用 cache 里面的
position，而不是原始文本的
position。所以并不是[0,1,2,3,5,6,7,8]，而是[0,1,2,3,4,5,6,7]</li>
</ul></li>
</ul>
<p>所以这种方式无需模型微调即可解决 window attention的困惑度问题。</p>
<h2 id="streamingllm-与-paged-attention-集成">StreamingLLM 与 Paged
attention 集成</h2>
<p>将第一页固定在 KV Cache 中，永远不 evicte KV Cache 的第一页。</p>
<p>--&gt;会带来一点额外的开销。比如一个page有16个tokens，而你只需要固定4个tokens</p>
<h1 id="总结">总结</h1>
<p><img src="/images/108_streaming-llm/illustration.png" width="90%"></p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 25%" />
<col style="width: 31%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;">计算复杂度</th>
<th style="text-align: center;">困惑度</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dense Attention</td>
<td style="text-align: left;">高 O(T^2)</td>
<td style="text-align: center;">差</td>
<td
style="text-align: left;">输入长度如果超过训练时的上下文长度，困惑度就会爆炸。</td>
</tr>
<tr class="even">
<td>Window Attention</td>
<td style="text-align: left;">低 O(TL)</td>
<td style="text-align: center;">差</td>
<td
style="text-align: left;">当句子长度超过窗口长度时，困惑度爆炸。</td>
</tr>
<tr class="odd">
<td>Sliding Window Re-computation</td>
<td style="text-align: left;">高 O(TL^2)</td>
<td style="text-align: center;">优</td>
<td style="text-align: left;">计算量大</td>
</tr>
<tr class="even">
<td>StreamingLLM</td>
<td style="text-align: left;">低 O(TL)</td>
<td style="text-align: center;">优</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h1 id="代码研究">代码研究</h1>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install torch torchvision torchaudio</span><br><span class="line">pip install transformers==4.33.0 accelerate datasets evaluate wandb scikit-learn scipy sentencepiece</span><br><span class="line"></span><br><span class="line">python setup.py develop</span><br></pre></td></tr></table></figure>
<p>Run Streaming Llama Chatbot</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python examples/run_streaming_llama.py  --enable_streaming</span><br></pre></td></tr></table></figure>
<p>最开始没有注定安装<code>transformers==4.33.0</code>，遇到报错：</p>
<p>研究一下代码逻辑：</p>
<p>使用了默认的 model_name_or_path， lmsys/vicuna-13b-v1.3 下载到了
<code>~/.cache/huggingface/transformers</code></p>
<p>data_root</p>
<p>start_size是4</p>
<p>recent_size是2000</p>
<p>在main函数里，load函数获取到 model 和 tokenizer。</p>
<blockquote>
<p>Tokenizer 是 NLP
中的一个重要组件，负责将文本分解为更小的可处理单元。不同的分词策略适用于不同的语言和任务，选择合适的分词器对模型性能有很大影响。</p>
</blockquote>
<p>由于使能了 enable_streaming</p>
<pre><code>kv_cache = enable_streaming_llm(
    model, start_size=args.start_size, recent_size=args.recent_size
)</code></pre>
<p><a
href="https://github.com/vllm-project/vllm/issues/1253">StreamingLLM
support?</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Tech</tag>
        <tag>KV Cache</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM Compressor</title>
    <url>/2024/11/08/109_llm_compressor/</url>
    <content><![CDATA[<p>llmcompressor is an easy-to-use library for optimizing models for
deployment with vllm.</p>
<p><a
href="https://github.com/vllm-project/llm-compressor">Github</a></p>
<p><a
href="https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/">Blog</a></p>
<p>LLM Compressor
是一款优化大型语言模型（LLM）的工具，通过先进的压缩技术实现快速推理，尤其在vLLM框架内。该工具旨在将分散的压缩库统一封装，简化应用并提升性能。</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Tech</tag>
        <tag>Compressor</tag>
      </tags>
  </entry>
  <entry>
    <title>C++Primer学习</title>
    <url>/2024/11/13/110_C++Primer/</url>
    <content><![CDATA[<p>学习C++</p>
<span id="more"></span>
<h1 id="起源">起源</h1>
<p>1、之前看一个NV公司的人分享经验，就是后悔没有好好学习C++；</p>
<p>2、昨日，丁大佬指导我要把C++学好。</p>
<p>想想自己一直在吃c语言的底子，确实没有发展上限了，这二件事情的碰撞，让我燃起学习C++的热情！</p>
<h1 id="理由">理由</h1>
<p>（引用 孟岩推荐序 2013 年 8 月）</p>
<p>C++11
标准公布之后，C++社群出现了久违的热情，有人甚至叫出“C++的复兴”。指望
C++回到 20 世纪 90 年代中期那样的地位显然是昧于大势的奢望，但是
C++经历了这么多年的打磨与起伏，其在工业界的地位已经非常稳固，在很多领域里已经是不可取代也没必要被取代的统治者。新标准的出现能够大大提升
C++开发的效率和质量，因此赢得欢呼也是情理之中。在这种氛围之下，编译器实现的速度也令人惊喜。短短两年时间，从开源的
GCC、LLVM 到专有的 VisualC++和 Intel
C++，对于新标准的追踪之快，覆盖之全，与当年 C++98
标准颁布之后迟迟不能落地的窘境相比，可谓对比强烈。当年是热情的开发者反复敦促厂商实现完整标准而不得，为此沮丧无奈，那种心情，至今记忆犹新。时过境迁，今天是编译器实现远远冲在前面，开发者倒是大大地落在了后面。</p>
<p>时至今日，能够基本了解 C++11 标准的程序员恐怕不多，而能够以新的
C++风格开发实践的人更是凤毛麟角。因此，今天的
C++开发者面临的一个重要任务就是快速掌握新的 C++风格和工具。</p>
<p>而说到教授“正宗的”C++11 编程风格，《C++Primer（第 5
版）》如同它之前的版本一样，扮演着法定教科书的角色。</p>
<p>一种优秀的编程语言，一定要对于计算这件事情实现一个完整和自洽的抽象。十几年来编程语言领域的竞争，除却实现质量之外，基本上是在比拼抽象的设计。而
C
语言之所以四十年长盛不衰，根本在于它对于现代计算机提供了一个最底层的高级抽象：凡是比它低的抽象都过于简陋，凡是比它高的抽象都可以用
C
语言构造出来。C++成功的根本原因，恰恰是因为它虽然试图提供一些高级的抽象机制，但是其根基与
C 在同一层面。正因为如此，每当你需要走下去直接与硬件对话时，C++成为 C
之外唯一有效率的选择。我的一个朋友在进行了多年的大型系统软件开发之后，不无感慨地说，C++最大的力量不在于其抽象，恰恰在于其不抽象。</p>
<p>话虽然如此，但是 C++之所以脱离 C
而存在，毕竟还是因为其强大的抽象能力。BjarneStroustrup
曾经总结说，C++同时支持 4 种不同的编程风格：C
风格、基于对象、面向对象和泛型。事实上，把微软的 COM
也算进来的话，还可以加上一种“基于组件”的风格。这么多的风格共存于一种语言，就是其强大抽象机制的证明。但是，在
C++11
以前，C++的抽象可以说存在若干缺陷，其中最严重的是缺少自动内存管理和对象级别的消息发送机制。今天看来，C++98
只能说是特定历史条件造成的半成品，无论是从语言机制，还是标准库完备程度来说，可以说都存在明显的、不容忽略的缺陷。其直接后果，就是优雅性的缺失和效率的降低。我本人在十年前曾经与当时中国
C++社群中不少杰出的人物交流探讨，试图从 C++98
中剪裁出一个小巧、优雅的、自成一体的子集，希望至少在日常编程中，能够在这个子集之内可以写出与当时的
Java 和
C#同样干净明晰的代码。为此我们尝试了各种古怪的模板技巧，并且到处寻找有启发的代码和经验来构造这个语言子集，结果并不理想，甚至可以说是令人非常失望。后来我在我的博客中发表过好几篇文章，探讨所谓的
C++风格问题，其实就是说，C++不支持简洁明快的面向对象风格，大家还不如回到基于对象甚至
C 语言的风格，最多加点模板，省一点代码量。非要面向对象的话，就必须依赖像
Qt 或者 MFC 那样的基础设施才可以。</p>
<p>C++11 出来之后，增强的语言机制和大为完善的标准库，为
C++语言的编程风格带来了革命性的变化。如果能够纯熟地运用 C++11
的新特征、新机制，那么就能够形成一种简洁优雅的
C++编程风格，以比从前更高的效率、更好的质量进行软件开发。对于这种新的风格，我认为“直觉、自然”是最佳的描述。也就是说，解决任何问题不必拘泥于什么笼盖一切的编程思想，也不再沉溺于各种古怪的模板技巧中无法自拔，而是能够根据那个问题本身采用最自然、最符合直觉的方式。C++有自己的一套思维方式，比如容器、算法、作为概念抽象的对象等，很大程度上这套思维方式确实是合乎直觉的。只有到了
C++11
这一代，C++语言的高级抽象才基本完备，这样一种风格才可能真正落实。因此可以说
C++11 对于 C++98 而言，不是一次简单的升级，而是一次本质的跃升。</p>
<p>学习新的 C++风格，并不是轻而易举的事情。即便对于以前已经精通
C++的人来说，熟练掌握 rvaluereference、move 语义，了解
unique_ptr、shared_ptr 和 weak_ptr 的完整用法，明智地使用 function/bind
和 lambda 机制，学习 C++Concurrency
的新技术，都绝非一朝一夕之功。对于那些初学者来说，这件事情更不简单。</p>
<p>本书无论对于初学者还是提高者，都是最经典的教科全书。一直以来，它的特点就是完整而详细，基本上关于语言本身的问题，都可以在这本书里得到解决。而本书的另一个重要优点，就是其完全基于新的编程风格编写，所有的例子和讲解都遵循
C++11
标准所体现出来的思路和风格进行，如果能够踏下心来认真学习和练习，那么就能“一次到位”地掌握
C++11，尽管可能会比较慢。有经验的
C++开发者阅读这本书当然不用从头到尾，选择自己关心的内容学习 C++11
的新特性就可以，是快速升级自身能力的捷径。</p>
<p>差不多十年前，我提出一个观点，每一个具体的技术领域，只需要读四五本书就够了。以前的
C++是个例外，因为语言设计有缺陷，所以要读很多书才知道如何绕过缺陷。现在的
C++11
完全可以了，大家读四五本书就可以达到合格的水平，这恰恰是语言进步的体现。</p>
<p>本书是这四五本中的一本，而且是“教程+参考书”，扛梁之作，初学者的不二法门。另一本是《C++标准程序库（第
2 版）》，对于 C++熟手来说更为快捷。ScottMeyers 的 Effective
C++永远是学习 C++者必读的，只不过这本书的第 4
版不知道什么时候出来。AnthonyWilliams 的 C++ Concurrency inAction
是学习用标准
C++开发并发程序的最佳选择。国内的作品，我则高度推荐陈硕的《Linux
多线程服务端编程》。这本书的名字赶跑了不少潜在的读者，所以我要特别说明一下。这本书是
C++开发的高水平作品，与其说是教你怎么用
C++写服务端开发，不如说是教你如何以服务端开发为例子提升
C++开发水平。前面几本书都是谈标准 C++自己的事情，碰到像 iostream
这样失败的标准组件也不得不硬着头皮介绍。而这本书是接地气的实践结晶，告诉你面对具体问题时应怎样权衡，C++里什么好用，什么不好用，为什么，等等。</p>
<p>今天的 C++学习者是非常幸运的，可以在 C++11
这个基础上大步向前，不必再因为那些语言的缺陷和过度的技巧而烦恼。大家静下心来认真读几本书，可以打下很好的基础。</p>
<h1 id="记录">记录</h1>
<h2 id="注释">注释</h2>
<blockquote>
<p>Although the compiler ignores comments, readers of our code do not.
Programmers tend to believe comments even when other parts of the system
documentation are out of date. An incorrect comment is worse than no
comment at all because it may mislead the reader. When you change your
code, be sure to update the comments, too!</p>
</blockquote>
<p>虽然编译器会忽略注释，但我们代码的读者不会。即使系统文档的其他部分已经过时，程序员也倾向于相信注释。不正确的注释比没有注释更糟糕
因为它可能会误导读者。当您更改代码时，请务必更新注释！</p>
<blockquote>
<p>Comment pairs generally are used for multiline explanations, whereas
double-slash comments tend to be used for half-line and single-line
remarks</p>
</blockquote>
<p>注释对通常用于多行解释，而双斜线注释往往用于半行和单行注释</p>
<h2 id="debug">debug</h2>
<blockquote>
<p>It is a good practice to <strong>correct errors in the sequence they
are reported</strong>. Often a single error can have a cascading effect
and cause a compiler to report more errors than actually are
present.</p>
</blockquote>
<blockquote>
<p>It is also a good idea to <strong>recompile the code after each
fix—or after making</strong> at most a small number of obvious fixes.
This cycle is known as <code>edit-compile-debug</code>.</p>
</blockquote>
<p>按照错误报告的顺序来纠正错误是一种很好的做法。通常，一个错误会产生连锁反应，导致编译器报告的错误比实际存在的错误更多。每次修复后重新编译代码也是一个好主意——或者在进行少数明显的修复后重新编译代码。这个循环称为<code>编辑-编译-调试</code>。</p>
<h2 id="class">Class</h2>
<blockquote>
<p>In C++ we define our own data structures by defining a class. A class
defines a type along with a collection of operations that are related to
that type. The class mechanism is one of the most important features in
C++. In fact, a primary focus of the design of C++ is to make it
possible to <strong>define class types that behave as naturally as the
built-in types</strong>.</p>
</blockquote>
<p>在 C++
中，我们通过定义类来定义自己的数据结构。一个类定义了一种类型，以及与该类型相关的操作集合。类机制是
C++ 中最重要的特性之一。事实上，C++
设计的主要重点是<strong>使定义行为与内置类型一样自然的类类型成为可能</strong>。</p>
<p>Perhaps the most important feature in C++ is the class, which lets
programmers define their own types. In C++ such types are sometimes
called “class types” to distinguish them from the types that are built
into the language.</p>
<p>Some languages let programmers define types that specify only what
data make up the type. Others, like C++, allow programmers to define
types that include operations as well as data.</p>
<p><strong>A major design goal of C++ is to let programmers define their
own types that are as easy to use as the built-in types.</strong> The
Standard C++ library uses these features to implement a rich library of
class types and associated functions.</p>
<hr />
<p>我的疑问：C++里面的类和结构体有什么区别？</p>
<p>在 C++ 中，<strong>类（class）</strong> 和
<strong>结构体（struct）</strong>
的基本功能几乎是相同的，它们都可以用来定义数据成员和成员函数，具有相似的语法和行为。然而，二者之间的主要区别在于默认的访问权限以及在代码设计上的一些常见约定：</p>
<h3 id="默认访问权限">1. <strong>默认访问权限</strong></h3>
<ul>
<li><strong>类（class）</strong>：默认情况下，类的成员是
<strong>私有（private）</strong>
的。即，所有未指定访问权限的成员在类中都是私有的。</li>
<li><strong>结构体（struct）</strong>：结构体的成员默认是
<strong>公有（public）</strong>
的。也就是说，如果不显式定义访问修饰符，所有成员在结构体中都是公有的。</li>
</ul>
<p><strong>示例</strong>： <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line">    <span class="type">int</span> x; <span class="comment">// 默认是 private</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">MyStruct</span> &#123;</span><br><span class="line">    <span class="type">int</span> x; <span class="comment">// 默认是 public</span></span><br><span class="line">    <span class="type">int</span> y;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="代码风格和用途上的习惯">2.
<strong>代码风格和用途上的习惯</strong></h3>
<ul>
<li><strong>类</strong>：在 C++
中，类通常用于表示带有封装性和私有成员的复杂数据类型，适合用来定义对象和数据抽象。</li>
<li><strong>结构体</strong>：虽然 C++
的结构体具备和类相同的功能，但通常用于表示简单的数据结构，主要用于存储数据。结构体在
C++ 中常用于兼容 C
语言的代码，也更倾向于用于无复杂行为的简单数据封装。</li>
</ul>
<h3 id="继承时的默认访问控制">3.
<strong>继承时的默认访问控制</strong></h3>
<ul>
<li><strong>类继承</strong>：当一个类从另一个类继承时，如果没有指定继承方式，则默认是
<strong>私有继承</strong>。</li>
<li><strong>结构体继承</strong>：当结构体继承另一个结构体或类时，默认是
<strong>公有继承</strong>。</li>
</ul>
<p><strong>示例</strong>： <figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BaseClass</span> &#123;&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">BaseStruct</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DerivedClass</span> : BaseClass &#123;&#125;; <span class="comment">// 默认 private 继承</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">DerivedStruct</span> : BaseStruct &#123;&#125;; <span class="comment">// 默认 public 继承</span></span><br></pre></td></tr></table></figure></p>
<h3 id="兼容性">4. <strong>兼容性</strong></h3>
<ul>
<li><strong>结构体</strong>：更接近 C 语言中的 struct，适合与 C
语言代码兼容。</li>
<li><strong>类</strong>：C++ 中独有，包含更多的面向对象特性。</li>
</ul>
<h3 id="代码可读性和设计意图">5.
<strong>代码可读性和设计意图</strong></h3>
<ul>
<li>使用 <strong>class</strong>
表示对象的设计更符合面向对象编程的设计习惯，意图更加明确。</li>
<li>使用 <strong>struct</strong>
则更适合简单的数据集合，传达出“主要用于数据存储”的意图。</li>
</ul>
<h3 id="总结">总结</h3>
<p>尽管类和结构体在 C++
中功能相似，但类更常用于封装复杂行为和数据，而结构体更偏向简单数据存储。它们之间的区别主要在于访问控制上的默认值以及编程习惯。</p>
<hr />
<blockquote>
<p>In general, the class author determines all the operations that can
be used on objects of the class type.</p>
</blockquote>
<h2 id="include">include</h2>
<p>Headers from the standard library are enclosed in angle brackets
(&lt; &gt;).</p>
<p>Those that are not part of the library are enclosed in double quotes
(" ").</p>
<ul>
<li>&lt;&gt;：用于包含标准库和第三方库的头文件，编译器直接从系统路径中查找。</li>
<li>""：用于包含用户定义的头文件，编译器优先在当前目录查找，找不到时再去系统路径查找。</li>
</ul>
<h2 id="constexpr">constexpr</h2>
<blockquote>
<p>Variables declared as constexpr are implicitly const and must be
initialized by constant expressions:</p>
</blockquote>
<p>声明为 constexpr 的变量一定是一个常量，并且必须用常量表达式初始化</p>
<ul>
<li>常量类型：const 常量的值可以在运行时确定，constexpr
要求值在编译时确定。</li>
<li>应用范围：constexpr 可以用于函数，以便让编译器在编译时计算结果，而
const 不具备此功能。</li>
<li>性能：constexpr 能提升编译时计算的性能，而 const
可能会产生运行时开销。</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Tech</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>triton</title>
    <url>/2024/11/14/112_triton/</url>
    <content><![CDATA[<p>其实最近心思一直在把之前的推理框架的工作能够沉淀一下发一篇论文，今天丁大佬教育我不要闭门造车，不要局限于推理框架，多看看别人的工程学习学习,学学triton、TVM、mlc-llm这些东西。很受用。</p>
<p>听人劝，吃饱饭。开始学习 <a
href="https://github.com/triton-lang/triton">triton</a></p>
<span id="more"></span>
<p><a href="https://openai.com/index/triton/">官网</a> <a
href="https://zhuanlan.zhihu.com/p/719703418">翻译</a></p>
<p>Triton 是一种类似 Python 的开源编程语言，它使没有 CUDA
经验的研究人员能够编写高效的 GPU
代码——大多数时候与专家能够编写的代码相当。</p>
<p>其他框架too verbose, lack flexibility or generate code noticeably
slower than Triton‘s hand-tuned baselines.</p>
<ul>
<li>目前已有的 DSL 在灵活性和（对相同算法）速度上明显慢于像
cuBLAS、cuDNN 或 TensorRT 这样的库中可用的最佳手写计算内核。</li>
<li>已有的 DSL 如 polyhedral machinery (Tiramisu/Tensor
Comprehensions)、scheduling languages (Halide、TVM)
等在效率上还有提升空间。</li>
</ul>
<p>The architecture of modern GPUs can be roughly divided into three
major components—DRAM, SRAM and ALUs—each of which must be considered
when optimizing CUDA code:</p>
<ul>
<li><p>Memory transfers from DRAM must be coalesced into large
transactions to leverage the large bus width of modern memory
interfaces.</p></li>
<li><p>Data must be manually stashed to SRAM prior to being re-used, and
managed so as to minimize shared memory bank conflicts upon
retrieval.</p></li>
<li><p>Computations must be partitioned and scheduled carefully, both
across and within Streaming Multiprocessors (SMs), so as to promote
instruction/thread-level parallelism and leverage special-purpose ALUs
(e.g., tensor cores).</p></li>
</ul>
<p>即</p>
<ul>
<li>合并访问 (Memory Coalescing)</li>
<li>避免 Bank Conflict (Avoiding Shared Memory Bank Conflicts)</li>
<li>提升计算效率 (Maximizing Computational Efficiency)</li>
</ul>
<p><img src="/images/112/gpu-architecture.svg" width="80%"></p>
<p>Triton 简化了 CUDA
编程的底层优化，通过自动化的方式减少开发者的负担，但同时保留了一些高层次的算法决策，例如
SM
任务调度和平铺策略，供开发者灵活调整。这种设计既提高了编程效率，又确保了对复杂算法的适用性。</p>
<p>Triton 通过对 blocks（尺寸为 2
的幂的小数组）的操作来公开实例内并行性，而不是单指令、多线程 (SIMT)
执行模型。通过这样做，Triton 有效地抽象了与 CUDA
线程块内的并发性相关的所有问题（例如，内存合并、共享内存同步/冲突、张量核心调度）。</p>
<p>在标准 CUDA 实现中，Softmax
归一化需要显式管理线程之间的同步（如多线程并行计算同一行的指数值和总和）。Triton
则通过将计算任务划分为每个 kernel
处理一行，避免了线程同步的复杂性，大大降低了代码实现难度，同时保持高性能。</p>
<p><strong>The bottom line here is not that Triton is inherently better,
but that it simplifies the development of specialized kernels that can
be much faster than those found in general-purpose
libraries.</strong></p>
<p>The good performance of Triton comes from a modular system
architecture centered around Triton-IR, an LLVM-based intermediate
representation in which multi-dimensional blocks of values are
first-class citizens.</p>
<p>总之， Triton
的核心理念是基于分块的编程范式可以促进神经网络的高性能计算核心的构建。CUDA
编写属于传统的 “单程序，多数据” GPU
执行模型，在线程的细粒度上进行编程，Triton
是在分块的细粒度上进行编程。</p>
<p>跟着教程走：https://triton-lang.org/main/getting-started/tutorials/index.html</p>
<p><a
href="https://blog.csdn.net/kebijuelun/article/details/136343258">OpenAI
Triton 入门教程</a></p>
<p>triton
在循环中是逐块进行计算的。这种方法的一个关键优势是，它导致了块结构的迭代空间，相较于现有的DSL，为程序员在实现稀疏操作时提供了更多的灵活性，同时允许编译器为数据局部性和并行性进行积极的优化。</p>
<h3 id="triton.jit">triton.jit</h3>
<p>triton.jit 是 Triton 框架中的一个装饰器，用于将 Python 函数
JIT（即时）编译为高性能的 GPU 内核。</p>
<ol type="1">
<li><p>JIT 编译：</p>
<ul>
<li>使用 Triton 编译器将被装饰的函数编译为 GPU 内核代码。</li>
<li>编译后的内核在 GPU 上运行，利用 GPU 的并行计算能力实现高性能。</li>
</ul></li>
<li><p>隐式转换：</p>
<ul>
<li>当调用一个使用 triton.jit 编译的函数时，传入的参数如果具有
.data_ptr() 方法（如 PyTorch 张量）和 .dtype
属性，会自动被转换为指针。</li>
<li>这使得与 PyTorch 等框架的张量交互更加方便。</li>
</ul></li>
<li><p>限制环境：</p>
<ul>
<li>JIT 编译后的函数在 GPU 上运行，不能直接访问普通的 Python
对象或标准库。</li>
<li>仅能使用以下资源：</li>
<li>Python 的基本类型（如 int、float）。</li>
<li>Triton 包中的内置函数。</li>
<li>函数的参数。</li>
<li>其他被 JIT 编译的函数。</li>
</ul></li>
</ol>
<h3 id="triton.autotune">triton.autotune</h3>
<p>triton.autotune 是一个装饰器，用于为 triton.jit 编译的函数添加
自动调优
功能。它通过评估多个配置（configs），选择最优的内核配置以获得最佳性能。</p>
<p>核心功能</p>
<ol type="1">
<li>自动配置选择：
<ul>
<li>提供多个候选配置（configs），triton.autotune
会自动评估这些配置的性能。</li>
<li>适用于 GPU 上的高性能计算，能够动态调整块大小（如
BLOCK_SIZE）、线程数等关键参数。</li>
</ul></li>
<li>动态适应输入：
<ul>
<li>通过 key 参数指定，当输入参数发生变化时，重新评估配置的性能。</li>
</ul></li>
<li>安全优化：
<ul>
<li>通过 reset_to_zero 和 restore_value
确保多次内核调用不会造成副作用。</li>
</ul></li>
<li>性能输出：
<ul>
<li>如果设置环境变量 TRITON_PRINT_AUTOTUNING=1，Triton
会打印调优过程的日志，包括时间消耗和最佳配置。</li>
</ul></li>
</ol>
<h3 id="triton.heuristics">triton.heuristics</h3>
<p>triton.heuristics 是 Triton 提供的装饰器，用于动态计算 meta-parameter
的值。这对于以下情况非常有用：</p>
<ol type="1">
<li><p>自动调优代价高昂：在一些情况下，使用 triton.autotune
进行全面的自动调优可能会非常耗时。</p></li>
<li><p>自动调优不可用：某些情况下，输入特性使得自动调优不适用。</p></li>
<li><p>动态计算 meta-parameter：</p>
<ul>
<li>根据输入参数动态推导内核中的 meta-parameter。</li>
<li>避免硬编码或繁琐的手动配置。</li>
<li>例如：根据张量大小动态设置块大小 BLOCK_SIZE。</li>
</ul></li>
<li><p>替代部分自动调优：</p>
<ul>
<li>相比 triton.autotune 的多次配置测试，triton.heuristics
通过计算直接生成最优或次优的 meta-parameter 值。</li>
</ul></li>
</ol>
<p>主要看一下 <code>Matrix Multiplication</code> 这个代码:</p>
<p>实现一个高性能的 FP16 矩阵乘法内核，其性能可以与 NVIDIA 的 cuBLAS 或
AMD 的 rocBLAS
相媲美。这些库是由硬件厂商提供的优化矩阵操作库，但它们通常是闭源的，不易定制。通过本教程，用户可以学习如何自行实现高性能矩阵乘法，同时能够轻松扩展或调整以适应特殊需求，例如深度学习中的自定义操作。</p>
<h2 id="关键学习点">关键学习点</h2>
<ol type="1">
<li>块级矩阵乘法（Block-level Matrix Multiplications）：
<ul>
<li>通过将矩阵分成小块，逐块进行操作，以提升性能和内存利用率。</li>
<li>这种分块操作可以显著提升 GPU 的计算效率。</li>
</ul></li>
<li>多维指针运算（Multi-dimensional Pointer Arithmetic）：
<ul>
<li>学习如何通过指针运算访问矩阵中的子块（block）。</li>
<li>对于行优先（row-major）存储的矩阵，需要通过复杂的指针偏移计算每个块的位置。</li>
</ul></li>
<li>程序重排序以提高 L2 缓存命中率（Program Re-ordering for Improved L2
Cache Hit Rate）：
<ul>
<li>通过改变程序块的执行顺序（如按列优先或分组的方式执行），可以提高 L2
缓存的命中率。</li>
<li>这种优化减少了从全局内存中加载数据的次数，从而显著提高性能。</li>
</ul></li>
<li>自动性能调优（Automatic Performance Tuning）：
<ul>
<li>利用 Triton
提供的自动调优工具，为特定的硬件配置选择最佳参数（如块大小、并行线程数等）。</li>
<li>自动调优使得代码可以在不同的硬件平台上实现接近最优的性能。</li>
</ul></li>
</ol>
<p><a
href="https://triton-lang.org/main/python-api/triton.language.html">triton.language</a>里面能用的函数是一些基础款，也够用了，几乎没有学习成本</p>
<p>Triton的优势： 可以直接控制 SRAM(onchip, shared memory) 和 HBM(global
memory) GPU memory</p>
<p>学着学着不对劲，还是先学LLVM吧。</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>HPC</tag>
        <tag>Tech</tag>
      </tags>
  </entry>
  <entry>
    <title>113_tensorflow实现量化softmax算子</title>
    <url>/2024/12/02/114_tensorflow%E5%AE%9E%E7%8E%B0%E9%87%8F%E5%8C%96softmax%E7%AE%97%E5%AD%90/</url>
    <content><![CDATA[<p>研究了一下 tensorflow 实现 int8 量化的 softmax 算子</p>
<span id="more"></span>
<p>以这个算子为例： <img src=/images/114/softmax.png width=50% />
<img src=/images/114/node.png width=50% /></p>
<p>首先要计算一些参数</p>
<p>在 <code>CalculateSoftmaxParams</code> 函数中，</p>
<p>先确保了2个参数： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">TF_LITE_ENSURE_EQ(context, output-&gt;params.zero_point, <span class="number">-128</span>);</span><br><span class="line">TF_LITE_ENSURE(context, output-&gt;params.scale == <span class="number">1.f</span> / <span class="number">256</span>);</span><br></pre></td></tr></table></figure> 看来量化的softmax算子的输出 tensor 的
<code>scale</code> 和 <code>zero point</code> 是固定的。</p>
<p>然后是函数 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">tflite::PreprocessSoftmaxScaling(</span><br><span class="line">    static_cast&lt;<span class="type">double</span>&gt;(params-&gt;beta),</span><br><span class="line">    static_cast&lt;<span class="type">double</span>&gt;(input-&gt;params.scale), kScaledDiffIntegerBits,</span><br><span class="line">    &amp;op_data-&gt;input_multiplier, &amp;input_left_shift);</span><br></pre></td></tr></table></figure></p>
<p>由于使用的是 Double-rounding(文末附了Single-Rounding 和
Double-Rounding的区别)</p>
<p><code>const double max_real_multiplier = (1LL &lt;&lt; 31) - 1.0;</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">const double input_beta_real_multiplier =</span><br><span class="line">    std::min&lt;double&gt;(beta * input_scale * (1 &lt;&lt; (31 - input_integer_bits)),</span><br><span class="line">                    max_real_multiplier);</span><br></pre></td></tr></table></figure>
<p>其中，</p>
<ul>
<li>beta: 1.000000
<ul>
<li>Used even when beta defaults to 1.0.</li>
<li>即使 beta 被设置为默认值 1.0，它不会被忽略，而是仍然参与计算。</li>
</ul></li>
<li>input_scale: 0.091732</li>
<li>input_integer_bits: 5
<ul>
<li>static const int kScaledDiffIntegerBits = 5;</li>
<li>这是设置的固定值</li>
</ul></li>
<li>max_real_multiplier: 2147483647.000000
<ul>
<li>使用的是 Double-rounding</li>
<li>const double max_real_multiplier = (1LL &lt;&lt; 31) - 1.0;</li>
</ul></li>
</ul>
<p>得到 <code>input_beta_real_multiplier: 6156025.000000</code></p>
<p>然后是函数 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">QuantizeMultiplierGreaterThanOne(input_beta_real_multiplier,</span><br><span class="line">                                 quantized_multiplier, left_shift);</span><br></pre></td></tr></table></figure></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (double_multiplier == <span class="number">0.</span>) &#123;</span><br><span class="line">  *quantized_multiplier = <span class="number">0</span>;</span><br><span class="line">  *shift = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">const</span> <span class="type">double</span> q = <span class="built_in">std</span>::<span class="built_in">frexp</span>(double_multiplier, shift);</span><br><span class="line"><span class="keyword">auto</span> q_fixed = static_cast&lt;<span class="type">int64_t</span>&gt;(TfLiteRound(q * (<span class="number">1LL</span> &lt;&lt; <span class="number">31</span>)));</span><br><span class="line">  <span class="keyword">if</span> (q_fixed == (<span class="number">1LL</span> &lt;&lt; <span class="number">31</span>)) &#123;</span><br><span class="line">  q_fixed /= <span class="number">2</span>;</span><br><span class="line">  ++*shift;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (*shift &lt; <span class="number">-31</span>) &#123;</span><br><span class="line">  *shift = <span class="number">0</span>;</span><br><span class="line">  q_fixed = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">*quantized_multiplier = static_cast&lt;<span class="type">int32_t</span>&gt;(q_fixed);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>计算结果是</p>
<ul>
<li>quantized_multiplier = 1575942400</li>
<li>left_shift = 23</li>
</ul>
<p>然后是函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">op_data-&gt;diff_min =</span><br><span class="line">    -1.0 * tflite::CalculateInputRadius(kScaledDiffIntegerBits,</span><br><span class="line">                                        op_data-&gt;input_left_shift);</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">double</span> max_input_rescaled =</span><br><span class="line">    <span class="number">1.0</span> * ((<span class="number">1</span> &lt;&lt; input_integer_bits) - <span class="number">1</span>) *</span><br><span class="line">    (<span class="number">1LL</span> &lt;&lt; (total_signed_bits - input_integer_bits)) /</span><br><span class="line">    (<span class="number">1LL</span> &lt;&lt; input_left_shift);</span><br><span class="line"><span class="comment">// Tighten bound using floor.  Suppose that we could use the exact value.</span></span><br><span class="line"><span class="comment">// After scaling the difference, the result would be at the maximum.  Thus we</span></span><br><span class="line"><span class="comment">// must ensure that our value has lower magnitude.</span></span><br><span class="line"><span class="keyword">return</span> static_cast&lt;<span class="type">int</span>&gt;(<span class="built_in">std</span>::<span class="built_in">floor</span>(max_input_rescaled));</span><br></pre></td></tr></table></figure>
<p>通过 floor
操作对边界进行收紧，使计算结果不会超过允许的最大值或最小值。
假设可以使用精确的浮点数值进行计算。如果直接使用精确值进行放缩（scaling），计算结果可能正好达到边界值（如最大值
127）。但是实际计算中，任何额外的误差都可能导致结果超出边界。因此，为了避免超出范围，我们需要保证数值的绝对值低于或等于边界值。使用
floor 是一种方法，可以确保数值朝着更小的方向调整，从而满足边界要求。</p>
<p>通过向下取整，确保计算结果不会超出规定的范围，从而避免数值的溢出问题。</p>
<p>input_integer_bits = 5 input_left_shift = 23 total_signed_bits =
31</p>
<p>op_data-&gt;diff_min = -248 op_data-&gt;input_left_shift = 23
op_data-&gt;input_multiplier = 1575942400</p>
<p>在 TensorFlow 的量化运算中，double-rounding 和 single-rounding
是处理整数运算中舍入过程的两种策略。它们在计算中间结果和最终结果时的处理方式不同，对精度和性能有一定影响。</p>
<ol type="1">
<li>Single-Rounding (单次舍入)</li>
</ol>
<p>单次舍入是在完成量化运算后，直接对结果进行一次舍入操作。 • 过程： •
直接将运算的中间结果（通常是乘法后的积）除以一个缩放因子，并进行一次舍入操作。
•
用于快速实现，但在一些边界情况（比如舍入时丢失低位信息）下可能会有轻微的误差。</p>
<ol start="2" type="1">
<li>Double-Rounding (双次舍入)</li>
</ol>
<p>双次舍入是一个更精确的舍入策略，通过两次独立的舍入操作，减少了舍入误差。
• 过程： 1.
首先对乘法结果进行一次初步舍入，保留更多的有效位（通常在中间的高精度计算中使用）。
2. 再根据目标量化范围对初步舍入后的结果进行二次舍入。</p>
<ul>
<li>Single-Rounding：
<ul>
<li>通常在硬件限制较多（如嵌入式设备）的 TensorFlow Lite Micro
中实现。</li>
<li>适合性能优先的场景。</li>
</ul></li>
<li>Double-Rounding：
<ul>
<li>用于更高精度要求的 TensorFlow Lite 和 TensorFlow
图层之间的累积计算。</li>
<li>对浮点到定点转换中的精度保留尤为重要。</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>softmax</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac M1 安装 Tensorflow</title>
    <url>/2024/11/22/113_MacM1_tensorflow/</url>
    <content><![CDATA[<p>近期在Mac M1上使用tensorflow，发现不能用了。报错：</p>
<p><code>The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.</code></p>
<span id="more"></span>
<ul>
<li><p>尝试使用conda重新安装，报错如上；</p></li>
<li><p>docker搭建x86环境安装，报错非法指令；</p></li>
</ul>
<p><a
href="https://tensorflow.google.cn/install/source?hl=zh-cn">官网</a>说是从1.6版本以后，预编译二进制文件将使用AVX指令，导致有的CPU无法执行。</p>
<p>很烦人，以至于我都开始搜电脑主机准备搭建一个Ubuntu了。好在后来找到了解决方式：</p>
<h1 id="删除现有conda">删除现有conda</h1>
<h2 id="检查-conda-安装路径">1. 检查 Conda 安装路径</h2>
<p>通常，Conda 安装在以下路径之一：~/anaconda3 或 ~/miniconda3</p>
<p>如果不确定路径，可以运行以下命令来确认安装位置：</p>
<p><code>which conda</code> 或 <code>conda info</code></p>
<p>在输出中查找 base environment 的路径。</p>
<h2 id="删除-conda-文件">2. 删除 Conda 文件</h2>
<h3 id="删除安装目录">1. 删除安装目录：</h3>
<p><code>rm -rf ~/anaconda3</code> <code>rm -rf ~/miniconda3</code></p>
<p>根据实际路径选择其中一个。</p>
<h3 id="清理环境变量">2. 清理环境变量：</h3>
<p>打开终端，编辑 ~/.zshrc（Mac 默认终端）或 ~/.bashrc 文件：</p>
<p><code>nano ~/.zshrc</code></p>
<p>删除与 Conda 相关的内容（例如 PATH 环境变量中的 anaconda3/bin 或
miniconda3/bin）。</p>
<h3 id="刷新环境变量">3. 刷新环境变量：</h3>
<p><code>source ~/.zshrc</code></p>
<h2 id="删除-conda-配置文件">3. 删除 Conda 配置文件</h2>
<p>无论在哪个平台，Conda 的配置文件通常位于：</p>
<p><code>~/.condarc</code></p>
<p>删除该文件：</p>
<p><code>rm ~/.condarc</code></p>
<h2 id="检查是否完全删除">4. 检查是否完全删除</h2>
<p>运行以下命令，确认 Conda 已被删除：</p>
<p><code>conda --version</code></p>
<p>如果显示 command not found，则已完全删除。</p>
<h1 id="安装miniforge">安装Miniforge</h1>
<p>在 Mac M1/M2 等 Apple Silicon 上安装 TensorFlow
需要一些特殊的步骤，因为默认的 TensorFlow 版本可能不支持 ARM 架构。</p>
<h2 id="步骤-1-安装-miniforge推荐">步骤 1: 安装 Miniforge（推荐）</h2>
<p>Miniforge 是 Conda 的轻量级版本，可以自动适配 Apple Silicon
架构。</p>
<p>1.1 下载 Miniforge</p>
<p>在终端中运行以下命令以下载并安装 Miniforge：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh</span><br><span class="line">bash Miniforge3-MacOSX-arm64.sh</span><br></pre></td></tr></table></figure>
<p>按照提示完成安装。</p>
<p>1.2 激活环境</p>
<p>运行以下命令以激活 Conda 环境：</p>
<p><code>source ~/miniforge3/bin/activate</code></p>
<h2 id="步骤-2-创建新环境并安装-tensorflow">步骤 2: 创建新环境并安装
TensorFlow</h2>
<h3 id="创建一个新的环境">1. 创建一个新的环境：</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n tensorflow_env python=3.10 -y</span><br><span class="line">conda activate tensorflow_env</span><br></pre></td></tr></table></figure>
<h3 id="安装-tensorflow-for-macos">2. 安装 TensorFlow for macOS:</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install -c apple tensorflow-deps</span><br><span class="line">python -m pip install tensorflow-macos</span><br><span class="line">python -m pip install tensorflow-metal</span><br></pre></td></tr></table></figure>
<p>说明： • tensorflow-macos 是为 macOS 专门优化的 TensorFlow 版本。 •
tensorflow-metal 是用于加速 M1/M2 上 GPU 的插件。</p>
<h2 id="步骤-3-测试安装">步骤 3: 测试安装</h2>
<p>运行以下 Python 脚本以测试安装是否成功：</p>
<p><code>import tensorflow as tf</code></p>
<h1 id="检查-tensorflow-是否可用">检查 TensorFlow 是否可用</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;TensorFlow Version:&quot;</span>, tf.__version__)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is GPU available:&quot;</span>, tf.config.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>如果输出类似以下内容，则安装成功：</p>
<blockquote>
<p>TensorFlow Version: 2.x.x</p>
</blockquote>
<blockquote>
<p>Is GPU available: [PhysicalDevice(name='/physical_device:GPU:0',
device_type='GPU')]</p>
</blockquote>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>树莓派Zero实验</title>
    <url>/2024/11/14/111_%E6%A0%91%E8%8E%93%E6%B4%BE%E5%AE%9E%E9%AA%8C/</url>
    <content><![CDATA[<p>做一些在树莓派<code>zero w</code>
上面的实验，这里记录一下整体流程。</p>
<span id="more"></span>
<h1 id="准备工作">准备工作</h1>
<p>在 <a href="https://www.raspberrypi.com/software/">官网</a> 下载
<code>Raspberry Pi Imager</code> 安装 <code>Raspberry Pi OS</code>。</p>
<p>下载
<code>operating system image</code>，考虑到其较低的硬件配置（1GHz
单核处理器和 512MB 内存），选择使用了 <strong>Raspberry Pi OS
(Legacy)</strong>，这是 A stable legacy version of Raspberry Pi OS
Bullseye，具体信息是：</p>
<pre><code>  Raspberry Pi OS (Legacy) Lite
  Release date: October 22nd 2024
  System: 32-bit
  Kernel version: 6.1
  Debian version: 11 (bullseye)
  Size: 366MB
  Show SHA256 file integrity hash:
  45dd65d579ec2b106a1e3181032144406eab61df892fcd2da8d83382fa4f7e51</code></pre>
<p>烧录过程参照<a
href="https://www.raspberrypi.com/documentation/computers/getting-started.html">官网教程</a>一步一步走就好了。</p>
<p>然而没能连上网（猜测在烧录工具里设置的 wifi
密码保存是乱码），于是在Boot目录中创建名为
<code>wpa_supplicant.conf</code> 文件。</p>
<p>将下面内容直接粘贴进去</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">country=CN</span><br><span class="line">ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev</span><br><span class="line">update_config=1</span><br><span class="line"></span><br><span class="line">network=&#123;</span><br><span class="line">   ssid=&quot;power_cube&quot;</span><br><span class="line">   psk=&quot;22222222&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用 <code>Angry IP Scanner</code>
软件扫描了一下，确定了树莓派连接wifi后的IP，顺利开始 ssh
连接开发之旅。</p>
<h1 id="树莓派zero-w">树莓派zero w</h1>
<p>https://www.raspberrypi.com/products/raspberry-pi-zero-w/</p>
<p>树莓派 Zero 是树莓派系列中小尺寸高性价比的卡片式开发主板。树莓派 Zero
采用的是 ARM11 内核的 BCM2835 处理器，经济适用。树莓派 Zero W 是在 Zero
原版的基础上板载无线网卡，支持蓝牙和 WiFi 功能，Zero WH 是在 Zero W
的基础上加焊了 40PIN 排针，方便用户的调试和使用。</p>
<p>主要功能特性有：</p>
<pre><code>  802.11 b/g/n wireless LAN
  Bluetooth 4.1
  Bluetooth Low Energy (BLE)
  1GHz, single-core CPU
  512MB RAM
  Mini HDMI® port and micro USB On-The-Go (OTG) port
  Micro USB power
  HAT-compatible 40-pin header
  Composite video and reset headers
  CSI camera connector</code></pre>
<h2 id="架构信息">架构信息</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pi@raspberrypi:~ $ uname -m</span><br><span class="line">armv6l</span><br></pre></td></tr></table></figure>
<ul>
<li>armv6l：表示 ARMv6 架构（通常是树莓派 Zero、Zero W）。</li>
<li>armv7l：表示 ARMv7 架构（树莓派 2）。</li>
<li>aarch64 或 armv8：表示 64 位的 ARMv8 架构（树莓派 3、树莓派
4，如果运行的是 64 位操作系统）。</li>
</ul>
<h2 id="cpu-信息">CPU 信息</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pi@raspberrypi:~ $ cat /proc/cpuinfo</span><br><span class="line">processor	: 0</span><br><span class="line">model name	: ARMv6-compatible processor rev 7 (v6l)</span><br><span class="line">BogoMIPS	: 697.95</span><br><span class="line">Features	: half thumb fastmult vfp edsp java tls </span><br><span class="line">CPU implementer	: 0x41</span><br><span class="line">CPU architecture: 7</span><br><span class="line">CPU variant	: 0x0</span><br><span class="line">CPU part	: 0xb76</span><br><span class="line">CPU revision	: 7</span><br><span class="line"></span><br><span class="line">Hardware	: BCM2835</span><br><span class="line">Revision	: 9000c1</span><br><span class="line">Serial		: 000000008caea6be</span><br><span class="line">Model		: Raspberry Pi Zero W Rev 1.1</span><br></pre></td></tr></table></figure>
<ul>
<li>half：半精度浮点运算支持（16 位浮点数）。</li>
<li>thumb：支持 Thumb 16 位压缩指令集，节省代码空间。</li>
<li>fastmult：支持快速整数乘法运算，提升性能。</li>
<li>vfp：支持向量浮点运算硬件，加速浮点计算。</li>
<li>edsp：增强的 DSP 指令集，用于加速数字信号处理。</li>
<li>java：支持 Jazelle 指令集扩展，用于加速 Java 字节码执行。</li>
<li>tls：支持线程本地存储，用于多线程编程。</li>
</ul>
<p>这些特性使得 ARMv6 处理器在树莓派 Zero W
上能够兼具浮点计算、数字信号处理和节省内存空间的能力，适合嵌入式和轻量级的多任务应用。</p>
<h2 id="主频">主频</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pi@raspberrypi:~ $ vcgencmd measure_clock arm</span><br><span class="line">frequency(48)=700000000</span><br></pre></td></tr></table></figure>
<p>当前是700MHz，实验了一下，当CPU推理模型时,会变成
<code>1 GHz</code>，如<a
href="https://forums.raspberrypi.com/viewtopic.php?t=127934">文档</a>所说：
&gt; It runs at 700MHz when idle and 1000MHz when busy (the core is
above 50% utilised).</p>
<h2 id="温度">温度</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vcgencmd measure_temp</span><br></pre></td></tr></table></figure>
<p>如果温度接近或超过 80°C，说明需要更好的散热。</p>
<h1 id="交叉编译">交叉编译</h1>
<p>树莓派 Zero W 使用 BCM2835 芯片，架构为 ARMv6，因此需要 ARMv6
兼容的交叉编译工具链。</p>
<p>我实验了官网的<a
href="https://github.com/raspberrypi/tools">交叉编译链接</a></p>
<ul>
<li><p><strong>路径</strong>：<code>tools/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin/</code></p></li>
<li><p><code>arm-linux-gnueabihf-gcc</code>（ARMv6 兼容的 GCC
编译器）</p></li>
<li><p><code>arm-linux-gnueabihf-g++</code>（ARMv6 兼容的 G++
编译器）</p></li>
</ul>
<p>配置了树莓派的交叉编译链，编译报错：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.../gcc-linaro-arm-linux-gnueabihf-raspbian-x64/bin/arm-linux-gnueabihf-gcc: cannot execute binary file</span><br></pre></td></tr></table></figure>
<p>这是因为arm-linux-gnueabihf-gcc 是针对 Linux 系统的二进制文件，macOS
的架构不支持直接运行此工具链。</p>
<p>于是在mac上装一个docker</p>
<pre><code>  docker version
  Client:
  Version:           27.3.1
  API version:       1.47
  Go version:        go1.22.7
  Git commit:        ce12230
  Built:             Fri Sep 20 11:38:18 2024
  OS/Arch:           darwin/arm64
  Context:           default</code></pre>
<p>启动一个 Linux 容器，并挂载项目目录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --platform linux/amd64 -it -v /Users/zhaodongyu/Projects:/Projects --name aidget ubuntu:20.04 /bin/bash</span><br><span class="line">docker start aidget</span><br><span class="line">docker <span class="built_in">exec</span> -it aidget /bin/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这会在 Docker 容器中将您的项目目录挂载到 /Projects。</p>
<p>注意，一定要加 <code>--platform linux/amd64</code>! 我是 MAC M1
电脑，这个选项可以强制 Docker 使用 x86-64 模拟环境。不然会报错
<code>rosetta error: failed to open elf at /lib64/ld-linux-x86-64.so.2</code>。（被这个问题恶心了很久）</p>
<p>经过实验，官方的交叉编译链编译出的可执行程序在树莓派
<code>zero w</code> 上运行是有问题的。</p>
<p>参考<a
href="https://forums.raspberrypi.com/viewtopic.php?t=336678">官方交流文档</a>在<a
href="https://github.com/tttapa/docker-arm-cross-toolchain?tab=readme-ov-file">这里</a>
下载<a
href="https://github.com/tttapa/docker-arm-cross-toolchain/releases/latest/download/x-tools-armv6-rpi-linux-gnueabihf-gcc12.tar.xz">1.0.0
交叉编译链</a>。</p>
<ul>
<li>Target triplet:armv6-rpi-linux-gnueabihf</li>
<li>GCC 12.4</li>
</ul>
<blockquote>
<p>ABI BREAK: the armv6-rpi-linux-gnueabihf toolchain is now built using
-march=armv6 instead of -march=armv6kz to provide compatibility with the
ARMv6 version of Raspberry Pi OS. This may affect the ABI, so you are
advised to recompile all code and dependencies after upgrading. (See
tttapa/RPi-Cross-Cpp-Development#4 (comment) for more information.)</p>
</blockquote>
<p>下载后解压：</p>
<p><code>tar -xf x-tools-aarch64-rpi3-linux-gnu-armv6-rpi-linux-gnueabihf-gcc12.tar.xz</code></p>
<p>在容器中安装相关工具：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt install make</span><br><span class="line">apt install -y binutils</span><br><span class="line"></span><br><span class="line"><span class="comment"># apt install -y gcc-arm-linux-gnueabihf</span></span><br></pre></td></tr></table></figure>
<p>这些标志的含义如下：</p>
<ul>
<li>-march=armv6：指定 ARMv6 架构，适用于树莓派 Zero 和 Pi 1。</li>
<li>-mfloat-abi=hard：使用硬件浮点 ABI。</li>
<li>-mfpu=vfp：指定使用 VFP（Vector Floating Point）浮点单元。</li>
<li>-marm：强制使用 ARM 指令集，而非 Thumb 指令集。</li>
</ul>
<h1 id="软件信息">软件信息</h1>
<p>树莓派的gcc版本(bullseye)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pi@raspberrypi:~ $ gcc --version</span><br><span class="line">gcc (Raspbian 10.2.1-6+rpi1) 10.2.1 20210110</span><br><span class="line">Copyright (C) 2020 Free Software Foundation, Inc.</span><br><span class="line">This is free software; see the source for copying conditions.  There is NO</span><br><span class="line">warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</span><br></pre></td></tr></table></figure>
<h1 id="加速">加速？</h1>
<p>处理器性能：BCM2835 使用的是 1GHz 的单核 ARM11 处理器，架构为
ARMv6，比现代 ARM 处理器（ARMv7、ARMv8
等）落后，缺少现代处理器中的许多优化指令集（如 NEON SIMD 指令）。</p>
<p>由于 CMSIS-NN 主要优化的是 ARM Cortex-M 系列处理器，对 ARMv6
并没有专门的加速支持。然而，CMSIS-NN
的优化策略（如量化、卷积优化、循环展开和指令调度）可以为 ARMv6
上的神经网络推理提供启发。</p>
<h1 id="section">2024-11-17</h1>
<p><a
href="https://github.com/mit-han-lab/tinyengine">tinyengine</a>是个很好的推理框架，我想跟它
PK 推理数据，经过一番移植遇到各种各样的问题，包括且不限于 <a
href="https://github.com/mit-han-lab/tinyengine/issues/43">get_kernel_buffer
undefined</a>, arm cortex-M7/M4
cores，针对性做了一些加速算子，移植起来确实需要些精力，给他改得算子性能差了的话又胜之不武，于是放弃。如果能够
PK，这一定是一个强有力的对手！</p>
<p>我发现它使用tf_convertor.parseOperatorInfo()解析模型要比我用flatbuffer解析简洁很多，我也要优化一下我的转换脚本。</p>
<p>tf_convertor.parseOperatorInfo() 是 TensorFlow 提供的 Python
API，简化了对 TFLite 模型的操作和解析，提供高级封装，方便直接读取 TFLite
模型中的算子信息。</p>
<p>直接解析 FlatBuffer，TFLite 模型的文件格式是 FlatBuffer
二进制文件，直接解析可以获取所有模型信息，包括 TensorFlow
工具链不支持的部分。</p>
<p>对比总结</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>特性</th>
<th>tf_convertor.parseOperatorInfo()</th>
<th>解析 FlatBuffer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>开发难度</td>
<td>简单，高度封装</td>
<td>高，需解析 FlatBuffer 结构</td>
</tr>
<tr class="even">
<td>灵活性</td>
<td>较低，依赖 TensorFlow 提供的功能</td>
<td>高，可访问底层和自定义信息</td>
</tr>
<tr class="odd">
<td>性能</td>
<td>较低，依赖 TensorFlow 工具链运行</td>
<td>高，直接操作二进制数据</td>
</tr>
<tr class="even">
<td>依赖</td>
<td>TensorFlow 工具链</td>
<td>FlatBuffer 库，无需 TensorFlow</td>
</tr>
<tr class="odd">
<td>适用场景</td>
<td>快速开发、标准模型解析</td>
<td>需要深度分析或轻量化项目</td>
</tr>
</tbody>
</table>
<p>既然项目是要让大家使用，使用 tf_convertor.parseOperatorInfo()
快速获取基本信息确实是更好的选择，决定改成这种方式，如果这种方式搞不定，再换成解析
FlatBuffer。</p>
<p>本来想用tinyengine的几个tflite作为实验模型，既然也用不了这个框架，那还是改用tflite的一些公开模型吧。所以PK的主力对象又换成了tflite
for micro（没什么挑战）</p>
<h1 id="搭建tflm">搭建TFLM</h1>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd Projects/RaspberryPi/tflm</span><br><span class="line">git clone https://github.com/tensorflow/tflite-micro.git</span><br></pre></td></tr></table></figure>
<blockquote>
<p>commit b23864944c0bc6c2603408754fd2b9b3dbaa862d</p>
</blockquote>
<p>在<code>tflite-micro/tensorflow/lite/micro/tools/make/targets/</code>目录下新建<code>rpi_zero_w_makefile.inc</code></p>
<p>内容为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TARGET_TOOLCHAIN_PREFIX := ~/x-tools/armv6-rpi-linux-gnueabihf/bin/armv6-rpi-linux-gnueabihf-</span><br><span class="line"></span><br><span class="line">CXXFLAGS += $(FLAGS_GCC)</span><br><span class="line">CCFLAGS += $(FLAGS_GCC)</span><br><span class="line"></span><br><span class="line">PLATFORM_FLAGS = \</span><br><span class="line">  -DTF_LITE_MCU_DEBUG_LOG \</span><br><span class="line">  -mfloat-abi=hard \</span><br><span class="line">  -mfpu=vfp \</span><br><span class="line">  -funsigned-char \</span><br><span class="line">  -mlittle-endian \</span><br><span class="line">  -Wno-type-limits \</span><br><span class="line">  -Wno-unused-private-field \</span><br><span class="line">  -fomit-frame-pointer \</span><br><span class="line">  -MD \</span><br><span class="line">  -march=armv6 \</span><br><span class="line">  -marm</span><br><span class="line"></span><br><span class="line">ifneq ($(PIC),)</span><br><span class="line">  PLATFORM_FLAGS += -fpic</span><br><span class="line">endif</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Common + C/C++ flags</span></span><br><span class="line">CXXFLAGS += $(PLATFORM_FLAGS)</span><br><span class="line">CCFLAGS += $(PLATFORM_FLAGS)</span><br><span class="line"></span><br><span class="line">CC := $(TARGET_TOOLCHAIN_PREFIX)gcc</span><br><span class="line">CXX := $(TARGET_TOOLCHAIN_PREFIX)g++</span><br><span class="line">AR := $(TARGET_TOOLCHAIN_PREFIX)ar</span><br></pre></td></tr></table></figure>
<p>编译静态库：</p>
<p><code>make -f tensorflow/lite/micro/tools/make/Makefile TARGET=rpi_zero_w microlite</code></p>
<p>得到静态库：</p>
<p><code>x-tools/armv6-rpi-linux-gnueabihf/bin/armv6-rpi-linux-gnueabihf-ar: creating gen/rpi_zero_w_x86_64_default_gcc/lib/libtensorflow-microlite.a</code></p>
<p>为了公平竞争，编译<code>BUILD_TYPE := release</code>版本,编译时没什么问题，运行时会遇到报错:</p>
<blockquote>
<p>undefined reference to `MicroPrintf(char const*, ...)'</p>
</blockquote>
<p>include/tensorflow/lite/micro/micro_mutable_op_resolver.h里面关于MicroPrintf的代码注释掉即可编译。</p>
<h1 id="数据分析">数据分析</h1>
<h2 id="lenet-5-模型">lenet-5 模型</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pi@raspberrypi:~/lenet5 $ size empty </span><br><span class="line">   text	   data	    bss	    dec	    hex	filename</span><br><span class="line">   2191	    340	      4	   2535	    9e7	empty</span><br><span class="line">pi@raspberrypi:~/lenet5 $ size tfml_default </span><br><span class="line">   text	   data	    bss	    dec	    hex	filename</span><br><span class="line"> 117353	 252116	     16	 369485	  5a34d	tfml_default</span><br><span class="line">pi@raspberrypi:~/lenet5 $ size tfml_release </span><br><span class="line">   text	   data	    bss	    dec	    hex	filename</span><br><span class="line">  85925	 252100	     12	 338037	  52875	tfml_release</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th>filename</th>
<th style="text-align: right;">text</th>
<th style="text-align: right;">data</th>
<th style="text-align: right;">bss</th>
<th style="text-align: right;">dec</th>
<th style="text-align: right;">invoke time</th>
<th>heap(B)</th>
<th>stack(B)</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>empty</td>
<td style="text-align: right;">2191</td>
<td style="text-align: right;">340</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">2535</td>
<td style="text-align: right;"></td>
<td>27368</td>
<td>2292</td>
<td></td>
</tr>
<tr class="even">
<td>tfml_default</td>
<td style="text-align: right;">117353</td>
<td style="text-align: right;">252116</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">369485</td>
<td style="text-align: right;"></td>
<td>67880</td>
<td>42464</td>
<td></td>
</tr>
<tr class="odd">
<td>tfml_release</td>
<td style="text-align: right;">85925</td>
<td style="text-align: right;">252100</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">338037</td>
<td style="text-align: right;"></td>
<td>67880</td>
<td>42464</td>
<td></td>
</tr>
<tr class="even">
<td>aidget</td>
<td style="text-align: right;">x</td>
<td style="text-align: right;">x</td>
<td style="text-align: right;">x</td>
<td style="text-align: right;">x</td>
<td style="text-align: right;">x</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>tfml_default</td>
<td style="text-align: right;">115162</td>
<td style="text-align: right;">251776</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">18.37 ms</td>
<td>40512</td>
<td>40172</td>
<td>-empty</td>
</tr>
<tr class="even">
<td>tfml_release</td>
<td style="text-align: right;">83734</td>
<td style="text-align: right;">251416</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">17.11 ms</td>
<td>40512</td>
<td>40172</td>
<td>-empty</td>
</tr>
</tbody>
</table>
<ul>
<li>text
<ul>
<li>表示代码段（.text）的大小。</li>
<li>包含程序的可执行指令。</li>
<li>较大时，说明程序包含更多功能或逻辑。</li>
</ul></li>
<li>data
<ul>
<li>表示已初始化全局和静态变量的大小。</li>
<li>包含变量的初始值。</li>
</ul></li>
<li>bss
<ul>
<li>表示未初始化或默认初始化为零的全局和静态变量的大小。</li>
<li>仅分配内存，不包含实际数据。</li>
</ul></li>
<li>dec
<ul>
<li>总大小（text + data + bss），以字节为单位。</li>
</ul></li>
<li>hex
<ul>
<li>dec 的十六进制表示。</li>
</ul></li>
</ul>
<h3 id="速度测试">速度测试</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pi@raspberrypi:~/lenet5 $ ./tfml_release </span><br><span class="line">Time taken: 17.111680108 seconds</span><br><span class="line">output = 0.100160</span><br><span class="line">pi@raspberrypi:~/lenet5 $ ./tfml_default </span><br><span class="line">Time taken: 18.373274514 seconds</span><br><span class="line">output = 0.100160</span><br></pre></td></tr></table></figure>
<h3 id="heap测试">heap测试</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">pi@raspberrypi:~/lenet5 $ valgrind --tool=massif --stacks=yes --massif-out-file=massif_output_empty.out ./empty </span><br><span class="line"></span><br><span class="line">pi@raspberrypi:~/lenet5 $ valgrind --tool=massif --stacks=yes --massif-out-file=massif_output_tfml_default.out ./tfml_default_1</span><br><span class="line"></span><br><span class="line">pi@raspberrypi:~/lenet5 $ valgrind --tool=massif --stacks=yes --massif-out-file=massif_output_tfml_release.out ./tfml_release_1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pi@raspberrypi:~/lenet5 $ ms_print massif_output_empty.out </span><br><span class="line">pi@raspberrypi:~/lenet5 $ ms_print massif_output_tfml_default.out </span><br><span class="line">pi@raspberrypi:~/lenet5 $ ms_print massif_output_tfml_release.out </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Node CONV_2D (number 0) invoke time: 11.220000</span><br><span class="line">Node TANH (number 1) invoke time: 1.351000</span><br><span class="line">Node MUL (number 2) invoke time: 0.506000</span><br><span class="line">Node ADD (number 3) invoke time: 0.387000</span><br><span class="line">Node MAX_POOL_2D (number 4) invoke time: 0.294000</span><br><span class="line">Node CONV_2D (number 5) invoke time: 5.136000</span><br><span class="line">Node TANH (number 6) invoke time: 0.469000</span><br><span class="line">Node MUL (number 7) invoke time: 0.178000</span><br><span class="line">Node ADD (number 8) invoke time: 0.128000</span><br><span class="line">Node MAX_POOL_2D (number 9) invoke time: 0.100000</span><br><span class="line">Node RESHAPE (number 10) invoke time: 0.013000</span><br><span class="line">Node FULLY_CONNECTED (number 11) invoke time: 1.234000</span><br><span class="line">Node TANH (number 12) invoke time: 0.049000</span><br><span class="line">Node FULLY_CONNECTED (number 13) invoke time: 0.272000</span><br><span class="line">Node TANH (number 14) invoke time: 0.038000</span><br><span class="line">Node FULLY_CONNECTED (number 15) invoke time: 0.038000</span><br><span class="line">Node SOFTMAX (number 16) invoke time: 0.013000</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr class="header">
<th>index</th>
<th>operator</th>
<th>tflm</th>
<th>aidget</th>
<th>aidget</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>CONV_2D</td>
<td>11.22</td>
<td>14.502</td>
<td>6.038</td>
</tr>
<tr class="even">
<td>1</td>
<td>TANH</td>
<td>1.351</td>
<td>2.725</td>
<td>0.334</td>
</tr>
<tr class="odd">
<td>2</td>
<td>MUL</td>
<td>0.506</td>
<td>0.269</td>
<td>0.193</td>
</tr>
<tr class="even">
<td>3</td>
<td>ADD</td>
<td>0.387</td>
<td>2.725</td>
<td>0.176</td>
</tr>
<tr class="odd">
<td>4</td>
<td>MAX_POOL_2D</td>
<td>0.294</td>
<td>1.114</td>
<td>0.496</td>
</tr>
<tr class="even">
<td>5</td>
<td>CONV_2D</td>
<td>5.136</td>
<td>11.357</td>
<td>5.152</td>
</tr>
<tr class="odd">
<td>6</td>
<td>TANH</td>
<td>0.469</td>
<td>0.857</td>
<td>0.119</td>
</tr>
<tr class="even">
<td>7</td>
<td>MUL</td>
<td>0.178</td>
<td>0.094</td>
<td>0.066</td>
</tr>
<tr class="odd">
<td>8</td>
<td>ADD</td>
<td>0.128</td>
<td>0.092</td>
<td>0.059</td>
</tr>
<tr class="even">
<td>9</td>
<td>MAX_POOL_2D</td>
<td>0.100</td>
<td>0.155</td>
<td>0.161</td>
</tr>
<tr class="odd">
<td>10</td>
<td>RESHAPE</td>
<td>0.013</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>11</td>
<td>FULLY_CONNECTED</td>
<td>1.234</td>
<td>3.805</td>
<td>1.163</td>
</tr>
<tr class="odd">
<td>12</td>
<td>TANH</td>
<td>0.049</td>
<td>0.075</td>
<td>0.013</td>
</tr>
<tr class="even">
<td>13</td>
<td>FULLY_CONNECTED</td>
<td>0.272</td>
<td>0.385</td>
<td>0.251</td>
</tr>
<tr class="odd">
<td>14</td>
<td>TANH</td>
<td>0.038</td>
<td>0.059</td>
<td>0.011</td>
</tr>
<tr class="even">
<td>15</td>
<td>FULLY_CONNECTED</td>
<td>0.038</td>
<td>0.036</td>
<td>0.027</td>
</tr>
<tr class="odd">
<td>16</td>
<td>SOFTMAX</td>
<td>0.013</td>
<td>0.010</td>
<td>0.006</td>
</tr>
</tbody>
</table>
<p>https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md</p>
<p>https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md</p>
<p><a href="https://arxiv.org/abs/1704.04861">MobileNets</a> are small,
low-latency, low-power models parameterized to meet the resource
constraints of a variety of use cases. They can be built upon for
classification, detection, embeddings and segmentation similar to how
other popular large scale models, such as Inception, are used.
MobileNets can be run efficiently on mobile devices with <a
href="https://www.tensorflow.org/lite">TensorFlow Lite</a>.</p>
<p><a href="https://arxiv.org/abs/1801.04381">MobileNetV2</a> and <a
href="https://arxiv.org/abs/1905.02244">MobilenetV3</a> networks.</p>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 22%" />
<col style="width: 26%" />
<col style="width: 22%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Quantized</th>
<th style="text-align: center;">Million MACs</th>
<th style="text-align: center;">Million Parameters</th>
<th style="text-align: center;">Top-1 Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><a
href="http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224.tgz">float_v1_1.0_224</a></td>
<td style="text-align: center;"><a
href="http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz">uint8</a></td>
<td style="text-align: center;">569</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a
href="https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz">float_v2_1.0_224</a></td>
<td style="text-align: center;"><a
href="https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz">uint8</a></td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a
href="https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large_224_1.0_float.tgz">float_v3_large_224</a></td>
<td style="text-align: center;"><a
href="https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large_224_1.0_uint8.tgz">uint8</a></td>
<td style="text-align: center;">217</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">75.2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a
href="https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_float.tgz">float_v3_small_224</a></td>
<td style="text-align: center;"><a
href="https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_uint8.tgz">uint8</a></td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">67.5</td>
</tr>
</tbody>
</table>
<p>https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/models/micro_speech_quantized.tflite</p>
<p>lenet5</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Tech</tag>
        <tag>Raspberry</tag>
      </tags>
  </entry>
  <entry>
    <title>115_Video_Generative</title>
    <url>/2025/01/07/115_Video_Generative/</url>
    <content><![CDATA[<p>学习研究文生视频</p>
<p>Effective scaling strategy
将计算资源需求减少最多5倍，同时实现预期的模型性能。</p>
<p>13 billion parameters 130亿参数 pre-training it on internet-scale
images and videos. 在互联网规模的图像和视频上进行了预训练。</p>
<p>在视频生成的四个关键方面表现出色： - 视觉质量 visual quality -
运动动态 motion dynamics - 视频与文本对齐 video-text alignment -
语义场景切换 semantic scene cut</p>
<p>第3部分介绍了我们的数据预处理技术，包括数据过滤和重新标注模型。
第4部分详细介绍了鸿源视频所有组件的架构，以及我们的训练和推理策略。
第5部分讨论了加速模型训练和推理的方法，从而支持130亿参数的大型模型开发。
第6部分评估了我们的文本到视频基础模型的性能，并将其与开源和闭源的顶尖视频生成模型进行比较。</p>
<p>image-video joint training strategy 图像-视频联合训练策略</p>
<p>对于形状为 (T + 1) × 3 × H × W 的视频，我们的3D VAE将其压缩为形状为
(T/ct + 1) × C × (H/cs) × (W/cs) 的潜在特征。</p>
<p>This compression significantly reduces the number of tokens for the
subsequent diffusion transformer model, allowing us to train videos at
the original resolution and frame rate.</p>
<p>中文:
这种压缩显著减少了后续扩散Transformer模型的Token数量，使我们能够在原始分辨率和帧率下训练视频。</p>
<p>在单个GPU上编码和解码高分辨率长视频可能会导致内存不足（OOM）错误。为了解决这个问题，我们使用了时空切片策略spatial-temporal
tiling strategy，将输入视频在空间和时间维度上分割为重叠的切片。
每个切片分别进行编码/解码，然后将输出拼接在一起。对于重叠区域，我们使用线性组合
linear combination 进行融合。
这种切片策略使我们能够在单个GPU上编码/解码任意分辨率和时长的视频。</p>
<p>推理过程中直接使用切片策略可能会由于训练和推理的不一致性而导致明显的伪影。
为了解决这个问题，我们引入了一个额外的微调阶段，在训练过程中随机启用或禁用切片策略。</p>
<p>对于文本分支，我们首先使用先进的大语言模型（LLM）将文本编码为捕捉细粒度语义信息的嵌入序列。
同时，我们使用CLIP模型提取包含全局信息的池化文本表示。这种表示随后在维度上进行扩展，并在输入模型前添加到时间步
timestep embedding 嵌入中。 CLIP model:
一种预训练模型，可生成文本和图像的嵌入表示。</p>
<p>为了有效整合文本和视觉信息，我们采用了与[47]中视频生成类似的“双流到单流Dual-stream
to Single-stream”混合模型设计策略。
在双流阶段，视频和文本Token通过多个Transformer块独立处理，使每种模态能够学习各自适当的调节机制，而不受干扰。
在单流阶段，我们将视频和文本Token拼接后输入后续Transformer块，以实现有效的多模态信息融合。
这种设计捕捉了视觉和语义信息之间的复杂交互，提升了整体模型性能。</p>
<p>在其生成模型中使用了基于扩散的骨干网络，建立在扩散概率模型的最新进展之上。
骨干网络在潜在空间中运行，处理由3DVAE编码器提取的Token序列。
核心网络结构采用基于Transformer的U-Net设计，集成了时空注意力层和前馈模块。</p>
<p>对于文本分支，我们采用了一种双层方法，将大规模预训练语言模型（LLM）与CLIP模型结合起来。
第一层使用LLM提取细粒度的文本特征，捕捉详细的语义信息。
接下来是CLIP模型，将嵌入池化为一个全局文本表示，用于总结输入文本的整体语境。
两种模型的组合嵌入随后作为输入，传递给视频生成骨干网络。
通过整合两层特征，文本编码器有效平衡了细粒度细节和全局语境，实现了精确的文本与视频对齐。</p>
<p><code>dist.init_process_group("nccl")</code></p>
<p>初始化分布式训练过程，设置进程间的通信方式，特别是使用
<strong>NCCL</strong>（NVIDIA Collective Communications
Library）作为通信后端。</p>
<p>是 PyTorch
分布式训练中的一个重要步骤，用于初始化分布式环境，使得多个进程可以通过通信框架互相协调工作。</p>
<p>NCCL 是一个高效的分布式通信库，特别优化了在 NVIDIA GPU
上进行数据传输和集合操作（如
<code>all-reduce</code>，<code>all-gather</code>）的性能。它能够利用 GPU
内部的高速互联（如
NVLink、PCIe）进行通信，提供低延迟、高带宽的数据交换。</p>
<p>工作原理 -
<strong>多机训练</strong>：如果有多个机器，每个机器上可能有多个
GPU，这时 <code>init_process_group("nccl")</code>
会通过网络将各机器上的进程连接起来。 -
<strong>单机多卡训练</strong>：如果只在一台机器上使用多个
GPU，这个初始化过程会使得不同 GPU 之间可以高效地共享数据。 -
<strong>通信模式</strong>：在 NCCL 后端的帮助下，PyTorch 可以进行高效的
<code>all-reduce</code>（将多个 GPU
上的梯度合并），<code>broadcast</code>（将一个 GPU 的数据广播到其他
GPU）等操作。</p>
<p>ring_degree ulysses_degree
https://github.com/xdit-project/xDiT/tree/main</p>
<p>𝒑: Number of pixels;<br />
𝒉𝒔: Model hidden size;<br />
𝑳: Number of model layers;<br />
𝑷: Total model parameters;<br />
𝑵: Number of parallel devices;<br />
𝑴: Number of patch splits;<br />
𝑸𝑶: Query and Output parameter count;<br />
𝑲𝑽: KV Activation parameter count;<br />
𝑨 = 𝑸 = 𝑶 = 𝑲 = 𝑽: Equal parameters for Attention, Query, Output, Key,
and Value;</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 6%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 21%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">attn-KV</th>
<th style="text-align: center;">communication cost</th>
<th style="text-align: center;">param memory</th>
<th style="text-align: center;">activations memory</th>
<th style="text-align: center;">extra buff memory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Tensor Parallel</td>
<td style="text-align: center;">fresh</td>
<td style="text-align: center;"><span class="math inline">\(4O(p \times
hs)L\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\frac{1}{N}P\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}QO\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}KV\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">DistriFusion*</td>
<td style="text-align: center;">stale</td>
<td style="text-align: center;"><span class="math inline">\(2O(p \times
hs)L\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(P\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}QO\)</span></td>
<td style="text-align: center;"><span class="math inline">\(2AL =
(KV)L\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Ring Sequence Parallel*</td>
<td style="text-align: center;">fresh</td>
<td style="text-align: center;"><span class="math inline">\(2O(p \times
hs)L\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(P\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}QO\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}KV\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Ulysses Sequence Parallel</td>
<td style="text-align: center;">fresh</td>
<td style="text-align: center;"><span
class="math inline">\(\frac{4}{N}O(p \times hs)L\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(P\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}QO\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{N}A
= \frac{1}{N}KV\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">PipeFusion*</td>
<td style="text-align: center;">stale-</td>
<td style="text-align: center;"><span class="math inline">\(2O(p \times
hs)\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\frac{1}{N}P\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{2}{M}A
= \frac{1}{M}QO\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\frac{2L}{N}A = \frac{1}{N}(KV)L\)</span></td>
</tr>
</tbody>
</table>
<p>看 communication cost 的话， Ulysses Sequence Parallel 更小一些，</p>
<p>Ring Sequence Parallel 和 Ulysses Sequence Parallel
是分布式计算中用于并行化序列模型（如
Transformer）的两种策略。它们的主要区别在于如何分割和处理数据流，以及它们在模型中的实现方式。</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>特性</th>
<th>Ring Sequence Parallel</th>
<th>Ulysses Sequence Parallel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>数据分割方式</td>
<td>小块，按时间步或序列依赖分割</td>
<td>较大块，按长序列分割</td>
</tr>
<tr class="even">
<td>设备间依赖性</td>
<td>强依赖，顺序传递</td>
<td>较弱依赖，尽量独立处理</td>
</tr>
<tr class="odd">
<td>通信模式</td>
<td>环形通信，设备间按序依赖</td>
<td>较少通信，设备间松耦合</td>
</tr>
<tr class="even">
<td>适用任务</td>
<td>时间步序列任务（RNN-like）</td>
<td>长序列模型（Transformer-like）</td>
</tr>
<tr class="odd">
<td>计算效率</td>
<td>较低，因需等待前一设备完成</td>
<td>较高，因设备独立并行工作</td>
</tr>
<tr class="even">
<td>实现复杂度</td>
<td>较低，容易实现</td>
<td>较高，需复杂的分割与同步策略</td>
</tr>
</tbody>
</table>
<p>选择的考量 - 如果模型需要严格的时间步依赖（如
RNN）且通信开销可以接受，Ring Sequence Parallel 是一个适合的选择。 -
如果模型是基于长序列（如 GPT 或 BERT
的长文档处理），并且需要最大化并行效率，Ulysses Sequence Parallel
更为理想。</p>
<p>args = Namespace(model='HYVideo-T/2-cfgdistill', latent_channels=16,
precision='bf16', rope_theta=256, vae='884-16c-hy',
vae_precision='fp16', vae_tiling=True, text_encoder='llm',
text_encoder_precision='fp16', text_states_dim=4096, text_len=256,
tokenizer='llm', prompt_template='dit-llm-encode',
prompt_template_video='dit-llm-encode-video', hidden_state_skip_layer=2,
apply_final_norm=False, text_encoder_2='clipL',
text_encoder_precision_2='fp16', text_states_dim_2=768,
tokenizer_2='clipL', text_len_2=77, denoise_type='flow', flow_shift=7.0,
flow_reverse=True, flow_solver='euler',
use_linear_quadratic_schedule=False, linear_schedule_end=25,
model_base='ckpts',
dit_weight='ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt',
model_resolution='540p', load_key='module', use_cpu_offload=True,
batch_size=1, infer_steps=50, disable_autocast=False,
save_path='./results', save_path_suffix='', name_suffix='',
num_videos=1, video_size=[960, 544], video_length=129, prompt='A cat
walks on the grass, realistic style.', seed_type='auto', seed=42,
neg_prompt=None, cfg_scale=1.0, embedded_cfg_scale=6.0, use_fp8=True,
reproduce=False, ulysses_degree=1, ring_degree=1)</p>
<p>VAE学习 https://www.gwylab.com/note-vae.html</p>
<p>llava-llama-3-8b-v1_1-hf is a LLaVA model fine-tuned from
meta-llama/Meta-Llama-3-8B-Instruct and CLIP-ViT-Large-patch14-336 with
ShareGPT4V-PT and InternVL-SFT by XTuner.</p>
<p>是的，llava-llama-3-8b-v1_1-hf 是一个典型的
MLLM（多模态大语言模型）的实例。</p>
<p>为什么是 MLLM？</p>
<p>这个模型的架构和功能体现了多模态大语言模型的特点： 1. 多模态能力： •
它结合了文本（LLaMA 模型）和视觉信息（CLIP
模型），能够处理图像和文本两种模态的数据。 • 通过 CLIP
的视觉编码器（CLIP-ViT-Large-patch14-336）来提取图像特征，文本部分由
Meta-Llama-3-8B-Instruct 处理。 2. 大语言模型（LLM）能力： • 使用 Meta
的 Llama 3
模型，这是一种强大的文本生成和理解语言模型，经过指令微调（Instruct
Tuning），适合对话和任务式问答。 3. 多模态数据的联合微调： • 使用数据集
ShareGPT4V-PT 和 InternVL-SFT，对多模态输入（图像+文本）进行强化训练。 •
ShareGPT4V-PT：包含 ChatGPT 和 GPT-4 Vision 提供的多模态对话数据。 •
InternVL-SFT：一种视觉-语言领域的任务微调数据集。 4. 模型设计： • 结合
LLaVA（Large Language and Vision Assistant）
框架，用于多模态问答（Multimodal
Q&amp;A），可以同时处理图像和文本输入，生成多模态输出。</p>
<p>LLaVA 是 MLLM 的典型实现</p>
<p>LLaVA（Large Language and Vision Assistant）
系列模型本身就是多模态大语言模型（MLLM）的重要代表之一，结合了强大的视觉编码器（如
CLIP）和语言模型（如 LLaMA）的能力，能够在以下多模态任务中表现优异： •
图文问答（Visual Question Answering, VQA）： •
输入图片和问题，回答与图片相关的问题。 • 图像描述生成（Image
Captioning）： • 输入图片，生成详细的文字描述。 • 图文对话（Multimodal
Dialogue）： • 在对话中理解并回应图像内容与文本问题的混合输入。</p>
<p>llava-llama-3-8b-v1_1-hf 就是这个方向的具体实现之一。</p>
<p>总结</p>
<p>llava-llama-3-8b-v1_1-hf
是一个典型的多模态大语言模型（MLLM），它结合了强大的文本生成能力和视觉理解能力，是
LLaVA 框架的一个应用实例。</p>
<p>如果你正在探索 MLLM
或者具体想知道它的实现原理、训练过程或应用场景，可以进一步探讨！</p>
<p><a href="https://www.youtube.com/watch?v=MzxPBzLSJh4">Teacache
Caching:提速200%，质量无损秘诀！ | Speed Boost 200%, No Quality Loss
Secrets!</a></p>
<p><a
href="https://github.com/welltop-cn/ComfyUI-TeaCache">ComfyUI-TeaCache</a></p>
<p>工作流：<a
href="https://github.com/amao2001/ganloss-latent-space">ganloss-latent-space</a></p>
<p><a href="https://github.com/ali-vilab/TeaCache">TeaCache技术</a> <a
href="https://arxiv.org/pdf/2411.19108">paper</a></p>
<p><img src="/images/115/Teacache.png" width="80%"></p>
<p>Uniform Cache: 均匀的缓存技术 - 毫无重点 - 没有优化的技术</p>
<p>TeaCache: 非均匀 多次重用 输出取决于输入，输入变化很小 -&gt;
输出变化可以接受，重用缓存，提高缓存效率</p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Tech</tag>
        <tag>T2V</tag>
      </tags>
  </entry>
  <entry>
    <title>116_learning_records</title>
    <url>/2025/02/07/116_learning_records/</url>
    <content><![CDATA[<p>扎实基础，系统学习。</p>
<span id="more"></span>
<h3 id="how-to-learn-deep-learning-framework"><a
href="https://github.com/BBuf/how-to-learn-deep-learning-framework?tab=readme-ov-file">how-to-learn-deep-learning-framework</a></h3>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/486360176">一文读懂 PyTorch
显存管理机制</a></li>
<li><a
href="https://zhuanlan.zhihu.com/p/527143823">PyTorch显存分配原理——以BERT，GPT为例</a></li>
</ul>
<h3 id="how-to-optim-algorithm-in-cuda"><a
href="https://github.com/BBuf/how-to-optim-algorithm-in-cuda?tab=readme-ov-file">how-to-optim-algorithm-in-cuda</a></h3>
<p>接下来要花时间学习这个
[gpu-mode/lectures][https://github.com/gpu-mode/lectures]</p>
<h2 id="deepseek">Deepseek</h2>
<p><a href="https://arxiv.org/pdf/2412.19437v1">技术报告</a> -
Multi-head Latent Attention (MLA) for efficient inference</p>
<pre><code>[Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.](https://arxiv.org/abs/2405.04434)</code></pre>
<ul>
<li><p>DeepSeekMoE for cost-effective training</p>
<p><a href="https://arxiv.org/abs/2401.06066">DeepSeekMoE: Towards
Ultimate Expert Specialization in Mixture-of-Experts Language
Models</a></p></li>
</ul>
<h3 id="moe">MOE</h3>
<p><img src="/images/116/MOE.png" width="80%"></p>
<p><a href="https://zhuanlan.zhihu.com/p/19117825360">MoE
大模型负载均衡</a></p>
<blockquote>
<p>从 GShard 到 DeepSeek-V3，我们不难发现负载均衡已经成为 MoE
模型能否取得成功的关键因素之一。 - GShard 提出了 top-2 gating
和容量限制的雏形； - Switch 用 top-1 gating
证明了简单路由也能支撑大规模； - GLaM 强调能效；DeepSpeed-MoE
则兼顾了训练和推理； - ST-MoE 用 z-loss 解决稳定性； - Mixtral
强调路由的时间局部性； - OpenMoE 暴露了末端 token 掉队等问题； - JetMoE
尝试 dropless； - DeepSeekMoE 做了细粒度拆分和共享专家； -
最后，DeepSeek-V3 又带来了更“轻量级”的偏置调节策略。</p>
</blockquote>
<blockquote>
<p>主要启示：负载均衡永远在动态平衡——过度干预会损害模型本身的学习目标，完全无视则会出现专家闲置或拥堵。往后我们大概率会看到更多
HPC 技巧与更灵活的 gating 机制，以及更多针对推理部署的优化。</p>
</blockquote>
<p>从<a
href="https://m.okjike.com/originalPosts/67aaf2fe2d8ef3d9a00dd07e">这里</a>跳到小宇宙听了三小时的
<a
href="https://www.xiaoyuzhoufm.com/episode/67a1b697247d51713c868367">DeepSeek论文的逐句讲解</a>
-
接近3个小时的高密度输出，非常能杀脑细胞，但杀完之后分泌出来的内啡肽，也含量爆炸。</p>
<p><a
href="https://zhuanlan.zhihu.com/p/12302566679">大模型推理张量并行的4种模式</a></p>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Tech</tag>
      </tags>
  </entry>
  <entry>
    <title>117_FlashMLA</title>
    <url>/2025/02/24/117-FlashMLA/</url>
    <content><![CDATA[<p>今天Deepseek开源 <a
href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a>，之前看过很多MLA相关知识了，很好的学习
Cuda 加速的机会，实践学习记录一下。</p>
<span id="more"></span>
<h1 id="实验平台">实验平台</h1>
<p>如FlashMLA所讲： &gt; FlashMLA is an efficient MLA decoding kernel
for Hopper GPUs, optimized for variable-length sequences serving.</p>
<p>实验平台选择 Hopper 结构的 GPU。</p>
<details>
<summary>
实验平台信息
</summary>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Mon Feb 24 13:56:23 2025       </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.4     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  NVIDIA H20                     On  | 00000000:16:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0              74W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   1  NVIDIA H20                     On  | 00000000:17:00.0 Off |                    0 |</span><br><span class="line">| N/A   29C    P0              75W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   2  NVIDIA H20                     On  | 00000000:40:00.0 Off |                    0 |</span><br><span class="line">| N/A   28C    P0              74W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   3  NVIDIA H20                     On  | 00000000:41:00.0 Off |                    0 |</span><br><span class="line">| N/A   28C    P0              74W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   4  NVIDIA H20                     On  | 00000000:96:00.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0              73W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   5  NVIDIA H20                     On  | 00000000:97:00.0 Off |                    0 |</span><br><span class="line">| N/A   28C    P0              72W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   6  NVIDIA H20                     On  | 00000000:C0:00.0 Off |                    0 |</span><br><span class="line">| N/A   26C    P0              72W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">|   7  NVIDIA H20                     On  | 00000000:C1:00.0 Off |                    0 |</span><br><span class="line">| N/A   29C    P0              73W / 500W |      0MiB / 97871MiB |      0%      Default |</span><br><span class="line">|                                         |                      |             Disabled |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                                         </span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<h1 id="安装">安装</h1>
<p><code>git clone https://github.com/deepseek-ai/FlashMLA.git</code></p>
<p><code>git submodule update --init csrc/cutlass/</code>(GPU平台“网不好”所以先
update 一下)</p>
<details>
<summary>
安装过程log
</summary>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python setup.py install</span></span><br><span class="line">running install</span><br><span class="line">/usr/local/lib/python3<span class="number">.10</span>/dist-packages/setuptools/_distutils/cmd.py:<span class="number">79</span>: SetuptoolsDeprecationWarning: setup.py install <span class="keyword">is</span> deprecated.</span><br><span class="line">!!</span><br><span class="line"></span><br><span class="line">        ********************************************************************************</span><br><span class="line">        Please avoid running ``setup.py`` directly.</span><br><span class="line">        Instead, use pypa/build, pypa/installer <span class="keyword">or</span> other</span><br><span class="line">        standards-based tools.</span><br><span class="line"></span><br><span class="line">        See https://blog.ganssle.io/articles/<span class="number">2021</span>/<span class="number">10</span>/setup-py-deprecated.html <span class="keyword">for</span> details.</span><br><span class="line">        ********************************************************************************</span><br><span class="line"></span><br><span class="line">!!</span><br><span class="line">  self.initialize_options()</span><br><span class="line">/usr/local/lib/python3<span class="number">.10</span>/dist-packages/setuptools/_distutils/cmd.py:<span class="number">79</span>: EasyInstallDeprecationWarning: easy_install command <span class="keyword">is</span> deprecated.</span><br><span class="line">!!</span><br><span class="line"></span><br><span class="line">        ********************************************************************************</span><br><span class="line">        Please avoid running ``setup.py`` <span class="keyword">and</span> ``easy_install``.</span><br><span class="line">        Instead, use pypa/build, pypa/installer <span class="keyword">or</span> other</span><br><span class="line">        standards-based tools.</span><br><span class="line"></span><br><span class="line">        See https://github.com/pypa/setuptools/issues/<span class="number">917</span> <span class="keyword">for</span> details.</span><br><span class="line">        ********************************************************************************</span><br><span class="line"></span><br><span class="line">!!</span><br><span class="line">  self.initialize_options()</span><br><span class="line">running bdist_egg</span><br><span class="line">running egg_info</span><br><span class="line">creating flash_mla.egg-info</span><br><span class="line">writing flash_mla.egg-info/PKG-INFO</span><br><span class="line">writing dependency_links to flash_mla.egg-info/dependency_links.txt</span><br><span class="line">writing top-level names to flash_mla.egg-info/top_level.txt</span><br><span class="line">writing manifest file <span class="string">&#x27;flash_mla.egg-info/SOURCES.txt&#x27;</span></span><br><span class="line">adding license file <span class="string">&#x27;LICENSE&#x27;</span></span><br><span class="line">writing manifest file <span class="string">&#x27;flash_mla.egg-info/SOURCES.txt&#x27;</span></span><br><span class="line">installing library code to build/bdist.linux-x86_64/egg</span><br><span class="line">running install_lib</span><br><span class="line">running build_py</span><br><span class="line">creating build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla</span><br><span class="line">copying flash_mla/__init__.py -&gt; build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla</span><br><span class="line">copying flash_mla/flash_mla_interface.py -&gt; build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla</span><br><span class="line">running build_ext</span><br><span class="line">/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/utils/cpp_extension.py:<span class="number">426</span>: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined <span class="keyword">for</span> CUDA version <span class="number">12.4</span></span><br><span class="line">  warnings.warn(<span class="string">f&#x27;There are no <span class="subst">&#123;compiler_name&#125;</span> version bounds defined for CUDA version <span class="subst">&#123;cuda_str_version&#125;</span>&#x27;</span>)</span><br><span class="line">building <span class="string">&#x27;flash_mla_cuda&#x27;</span> extension</span><br><span class="line">creating /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc</span><br><span class="line">Emitting ninja build file /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/build.ninja...</span><br><span class="line">Compiling objects...</span><br><span class="line">Allowing ninja to <span class="built_in">set</span> a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">[<span class="number">1</span>/<span class="number">2</span>] c++ -MMD -MF /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc/flash_api.o.d -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=<span class="built_in">format</span>-security -g -fwrapv -O2 -fPIC -I/data/private/zhaodongyu/FlashMLA/csrc -I/data/private/zhaodongyu/FlashMLA/csrc/cutlass/include -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include/TH -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3<span class="number">.10</span> -c -c /data/private/zhaodongyu/FlashMLA/csrc/flash_api.cpp -o /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc/flash_api.o -O3 -std=c++<span class="number">17</span> -DNDEBUG -Wno-deprecated-declarations -DTORCH_API_INCLUDE_EXTENSION_H <span class="string">&#x27;-DPYBIND11_COMPILER_TYPE=&quot;_gcc&quot;&#x27;</span> <span class="string">&#x27;-DPYBIND11_STDLIB=&quot;_libstdcpp&quot;&#x27;</span> <span class="string">&#x27;-DPYBIND11_BUILD_ABI=&quot;_cxxabi1011&quot;&#x27;</span> -DTORCH_EXTENSION_NAME=flash_mla_cuda -D_GLIBCXX_USE_CXX11_ABI=<span class="number">0</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">2</span>] /usr/local/cuda/<span class="built_in">bin</span>/nvcc --generate-dependencies-<span class="keyword">with</span>-<span class="built_in">compile</span> --dependency-output /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc/flash_fwd_mla_bf16_sm90.o.d -I/data/private/zhaodongyu/FlashMLA/csrc -I/data/private/zhaodongyu/FlashMLA/csrc/cutlass/include -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include/TH -I/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3<span class="number">.10</span> -c -c /data/private/zhaodongyu/FlashMLA/csrc/flash_fwd_mla_bf16_sm90.cu -o /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc/flash_fwd_mla_bf16_sm90.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options <span class="string">&#x27;&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;-fPIC&#x27;</span><span class="string">&quot;&#x27;&quot;</span><span class="string">&#x27;&#x27;</span> -O3 -std=c++<span class="number">17</span> -DNDEBUG -D_USE_MATH_DEFINES -Wno-deprecated-declarations -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-<span class="keyword">lambda</span> --use_fast_math --ptxas-options=-v,--register-usage-level=<span class="number">10</span> -gencode arch=compute_90a,code=sm_90a --threads <span class="number">32</span> -DTORCH_API_INCLUDE_EXTENSION_H <span class="string">&#x27;-DPYBIND11_COMPILER_TYPE=&quot;_gcc&quot;&#x27;</span> <span class="string">&#x27;-DPYBIND11_STDLIB=&quot;_libstdcpp&quot;&#x27;</span> <span class="string">&#x27;-DPYBIND11_BUILD_ABI=&quot;_cxxabi1011&quot;&#x27;</span> -DTORCH_EXTENSION_NAME=flash_mla_cuda -D_GLIBCXX_USE_CXX11_ABI=<span class="number">0</span></span><br><span class="line">ptxas info    : <span class="number">192</span> <span class="built_in">bytes</span> gmem</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi160EEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi160EEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">24</span> <span class="built_in">bytes</span> stack frame, <span class="number">0</span> <span class="built_in">bytes</span> spill stores, <span class="number">0</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">48</span> registers, <span class="number">24</span> <span class="built_in">bytes</span> cumulative stack size, <span class="number">640</span> <span class="built_in">bytes</span> smem</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi128EEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi128EEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">24</span> <span class="built_in">bytes</span> stack frame, <span class="number">0</span> <span class="built_in">bytes</span> spill stores, <span class="number">0</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">48</span> registers, <span class="number">24</span> <span class="built_in">bytes</span> cumulative stack size, <span class="number">512</span> <span class="built_in">bytes</span> smem</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi96EEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi96EEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">24</span> <span class="built_in">bytes</span> stack frame, <span class="number">0</span> <span class="built_in">bytes</span> spill stores, <span class="number">0</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">48</span> registers, <span class="number">24</span> <span class="built_in">bytes</span> cumulative stack size, <span class="number">384</span> <span class="built_in">bytes</span> smem</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi64EEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi64EEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">24</span> <span class="built_in">bytes</span> stack frame, <span class="number">0</span> <span class="built_in">bytes</span> spill stores, <span class="number">0</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">48</span> registers, <span class="number">24</span> <span class="built_in">bytes</span> cumulative stack size, <span class="number">256</span> <span class="built_in">bytes</span> smem</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi32EEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash36flash_fwd_splitkv_mla_combine_kernelIN7cutlass10bfloat16_tEflLi512ELi32EEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">24</span> <span class="built_in">bytes</span> stack frame, <span class="number">0</span> <span class="built_in">bytes</span> spill stores, <span class="number">0</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">48</span> registers, <span class="number">24</span> <span class="built_in">bytes</span> cumulative stack size, <span class="number">128</span> <span class="built_in">bytes</span> smem</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash28flash_fwd_splitkv_mla_kernelI27Flash_fwd_kernel_traits_mlaILi576ELi64ELi64ELi8EN7cutlass10bfloat16_tELi512EELb0ENS_16SharedStorageMLAIS4_EEEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash28flash_fwd_splitkv_mla_kernelI27Flash_fwd_kernel_traits_mlaILi576ELi64ELi64ELi8EN7cutlass10bfloat16_tELi512EELb0ENS_16SharedStorageMLAIS4_EEEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">104</span> <span class="built_in">bytes</span> stack frame, <span class="number">112</span> <span class="built_in">bytes</span> spill stores, <span class="number">124</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">255</span> registers, <span class="number">104</span> <span class="built_in">bytes</span> cumulative stack size</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_ZN5flash28flash_fwd_splitkv_mla_kernelI27Flash_fwd_kernel_traits_mlaILi576ELi64ELi64ELi8EN7cutlass10bfloat16_tELi512EELb1ENS_16SharedStorageMLAIS4_EEEEv20Flash_fwd_mla_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _ZN5flash28flash_fwd_splitkv_mla_kernelI27Flash_fwd_kernel_traits_mlaILi576ELi64ELi64ELi8EN7cutlass10bfloat16_tELi512EELb1ENS_16SharedStorageMLAIS4_EEEEv20Flash_fwd_mla_params</span><br><span class="line">    <span class="number">104</span> <span class="built_in">bytes</span> stack frame, <span class="number">116</span> <span class="built_in">bytes</span> spill stores, <span class="number">136</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">255</span> registers, <span class="number">104</span> <span class="built_in">bytes</span> cumulative stack size</span><br><span class="line">ptxas info    : Compiling entry function <span class="string">&#x27;_Z23get_mla_metadata_kernel19Mla_metadata_params&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_90a&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _Z23get_mla_metadata_kernel19Mla_metadata_params</span><br><span class="line">    <span class="number">24</span> <span class="built_in">bytes</span> stack frame, <span class="number">0</span> <span class="built_in">bytes</span> spill stores, <span class="number">0</span> <span class="built_in">bytes</span> spill loads</span><br><span class="line">ptxas info    : Used <span class="number">32</span> registers, <span class="number">24</span> <span class="built_in">bytes</span> cumulative stack size, <span class="number">32768</span> <span class="built_in">bytes</span> smem</span><br><span class="line">x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=<span class="built_in">format</span>-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc/flash_api.o /data/private/zhaodongyu/FlashMLA/build/temp.linux-x86_64-cpython-<span class="number">310</span>/csrc/flash_fwd_mla_bf16_sm90.o -L/usr/local/lib/python3<span class="number">.10</span>/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla_cuda.cpython-<span class="number">310</span>-x86_64-linux-gnu.so</span><br><span class="line">creating build/bdist.linux-x86_64/egg</span><br><span class="line">creating build/bdist.linux-x86_64/egg/flash_mla</span><br><span class="line">copying build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla/__init__.py -&gt; build/bdist.linux-x86_64/egg/flash_mla</span><br><span class="line">copying build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla/flash_mla_interface.py -&gt; build/bdist.linux-x86_64/egg/flash_mla</span><br><span class="line">copying build/lib.linux-x86_64-cpython-<span class="number">310</span>/flash_mla_cuda.cpython-<span class="number">310</span>-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg</span><br><span class="line">byte-compiling build/bdist.linux-x86_64/egg/flash_mla/__init__.py to __init__.cpython-<span class="number">310.</span>pyc</span><br><span class="line">byte-compiling build/bdist.linux-x86_64/egg/flash_mla/flash_mla_interface.py to flash_mla_interface.cpython-<span class="number">310.</span>pyc</span><br><span class="line">creating stub loader <span class="keyword">for</span> flash_mla_cuda.cpython-<span class="number">310</span>-x86_64-linux-gnu.so</span><br><span class="line">byte-compiling build/bdist.linux-x86_64/egg/flash_mla_cuda.py to flash_mla_cuda.cpython-<span class="number">310.</span>pyc</span><br><span class="line">creating build/bdist.linux-x86_64/egg/EGG-INFO</span><br><span class="line">copying flash_mla.egg-info/PKG-INFO -&gt; build/bdist.linux-x86_64/egg/EGG-INFO</span><br><span class="line">copying flash_mla.egg-info/SOURCES.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO</span><br><span class="line">copying flash_mla.egg-info/dependency_links.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO</span><br><span class="line">copying flash_mla.egg-info/top_level.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO</span><br><span class="line">writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt</span><br><span class="line">zip_safe flag <span class="keyword">not</span> <span class="built_in">set</span>; analyzing archive contents...</span><br><span class="line">__pycache__.flash_mla_cuda.cpython-<span class="number">310</span>: module references __file__</span><br><span class="line">creating dist</span><br><span class="line">creating <span class="string">&#x27;dist/flash_mla-1.0.0+bcb90f2-py3.10-linux-x86_64.egg&#x27;</span> <span class="keyword">and</span> adding <span class="string">&#x27;build/bdist.linux-x86_64/egg&#x27;</span> to it</span><br><span class="line">removing <span class="string">&#x27;build/bdist.linux-x86_64/egg&#x27;</span> (<span class="keyword">and</span> everything under it)</span><br><span class="line">Processing flash_mla-<span class="number">1.0</span><span class="number">.0</span>+bcb90f2-py3<span class="number">.10</span>-linux-x86_64.egg</span><br><span class="line">creating /usr/local/lib/python3<span class="number">.10</span>/dist-packages/flash_mla-<span class="number">1.0</span><span class="number">.0</span>+bcb90f2-py3<span class="number">.10</span>-linux-x86_64.egg</span><br><span class="line">Extracting flash_mla-<span class="number">1.0</span><span class="number">.0</span>+bcb90f2-py3<span class="number">.10</span>-linux-x86_64.egg to /usr/local/lib/python3<span class="number">.10</span>/dist-packages</span><br><span class="line">Adding flash-mla <span class="number">1.0</span><span class="number">.0</span>+bcb90f2 to easy-install.pth file</span><br><span class="line"></span><br><span class="line">Installed /usr/local/lib/python3<span class="number">.10</span>/dist-packages/flash_mla-<span class="number">1.0</span><span class="number">.0</span>+bcb90f2-py3<span class="number">.10</span>-linux-x86_64.egg</span><br><span class="line">Processing dependencies <span class="keyword">for</span> flash-mla==<span class="number">1.0</span><span class="number">.0</span>+bcb90f2</span><br><span class="line">Finished processing dependencies <span class="keyword">for</span> flash-mla==<span class="number">1.0</span><span class="number">.0</span>+bcb90f2</span><br><span class="line">&lt;/details&gt;</span><br></pre></td></tr></table></figure>
<h1 id="测试结果">测试结果</h1>
<details>
<summary>
测试结果log
</summary>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># python tests/test_flash_mla.py</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 36, n_repeat: 144</span><br><span class="line">0.609 ms, 30 TFLOPS, 998 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 35, n_repeat: 140</span><br><span class="line">0.634 ms, 29 TFLOPS, 965 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 36, n_repeat: 145</span><br><span class="line">0.610 ms, 60 TFLOPS, 1004 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 35, n_repeat: 140</span><br><span class="line">0.634 ms, 60 TFLOPS, 1001 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 36, n_repeat: 145</span><br><span class="line">0.613 ms, 60 TFLOPS, 1000 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 34, n_repeat: 138</span><br><span class="line">0.645 ms, 59 TFLOPS, 998 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 35, n_repeat: 143</span><br><span class="line">0.618 ms, 118 TFLOPS, 1005 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 35, n_repeat: 143</span><br><span class="line">0.622 ms, 117 TFLOPS, 995 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 35, n_repeat: 143</span><br><span class="line">0.621 ms, 118 TFLOPS, 1002 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 35, n_repeat: 141</span><br><span class="line">0.631 ms, 113 TFLOPS, 965 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 19, n_repeat: 79</span><br><span class="line">1.186 ms, 123 TFLOPS, 539 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 18, n_repeat: 74</span><br><span class="line">1.261 ms, 121 TFLOPS, 528 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 19, n_repeat: 79</span><br><span class="line">1.191 ms, 123 TFLOPS, 537 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=4096, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 19, n_repeat: 76</span><br><span class="line">1.235 ms, 120 TFLOPS, 523 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 10, n_repeat: 40</span><br><span class="line">2.394 ms, 122 TFLOPS, 282 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=4096, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 9, n_repeat: 38</span><br><span class="line">2.516 ms, 120 TFLOPS, 276 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 19, n_repeat: 79</span><br><span class="line">1.175 ms, 31 TFLOPS, 1032 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 19, n_repeat: 78</span><br><span class="line">1.198 ms, 30 TFLOPS, 1012 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 20, n_repeat: 80</span><br><span class="line">1.173 ms, 62 TFLOPS, 1037 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=16, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 18, n_repeat: 74</span><br><span class="line">1.271 ms, 61 TFLOPS, 1024 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 19, n_repeat: 79</span><br><span class="line">1.178 ms, 62 TFLOPS, 1033 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 20, n_repeat: 81</span><br><span class="line">1.147 ms, 61 TFLOPS, 1012 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 19, n_repeat: 79</span><br><span class="line">1.183 ms, 123 TFLOPS, 1036 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=32, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 20, n_repeat: 81</span><br><span class="line">1.151 ms, 121 TFLOPS, 1017 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 19, n_repeat: 79</span><br><span class="line">1.190 ms, 123 TFLOPS, 1030 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 20, n_repeat: 80</span><br><span class="line">1.162 ms, 120 TFLOPS, 1011 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 10, n_repeat: 41</span><br><span class="line">2.305 ms, 127 TFLOPS, 539 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=64, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 10, n_repeat: 40</span><br><span class="line">2.408 ms, 125 TFLOPS, 532 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 10, n_repeat: 41</span><br><span class="line">2.314 ms, 126 TFLOPS, 537 GB/s</span><br><span class="line">b=128, s_q=1, mean_sk=8192, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 9, n_repeat: 38</span><br><span class="line">2.508 ms, 125 TFLOPS, 529 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=False</span><br><span class="line">n_warmup: 5, n_repeat: 20</span><br><span class="line">4.698 ms, 124 TFLOPS, 272 GB/s</span><br><span class="line">b=128, s_q=2, mean_sk=8192, h_q=128, h_kv=1, d=576, dv=512, causal=True, varlen=True</span><br><span class="line">n_warmup: 5, n_repeat: 21</span><br><span class="line">4.648 ms, 123 TFLOPS, 270 GB/s</span><br><span class="line">&lt;/details&gt;</span><br></pre></td></tr></table></figure>
<h1 id="学习分析">学习分析</h1>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Tech</tag>
      </tags>
  </entry>
</search>
